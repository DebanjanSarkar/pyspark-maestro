{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e5b666-5029-4549-9d67-c945d231342c",
   "metadata": {},
   "source": [
    "<h1><center>Connecting PySpark with SQL SERVER</center></h1>\n",
    "<hr><hr><hr>",
    "Required connector and driver used: Apache Spark connector: SQL Server & Azure SQL <br>",
    "MS Doc Link: https://learn.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver16 <br>",
    "Reference Video: https://www.youtube.com/watch?v=YPe_jmV3pzk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fc4b86-ea48-4050-bcf8-5c75650b9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290a55e5-12d4-427d-8829-0ae217715053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d2069-8a35-4385-8d79-17ea86b7dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004-sql_server_pyspark_connection\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "notebook_name = ipynbname.name()\n",
    "\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102d17d4-7653-441d-825d-69f7338ec25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>004-sql_server_pyspark_connection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x178f0158bb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName( notebook_name )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f97db-9136-4a72-ae4d-9d2ebf8126f7",
   "metadata": {},
   "source": [
    "- For connection to the SQL Server, any port needs to be exposed for the SQL Server, from the 'SQL Server (2022) Configuration Manager' and Firewall. By default, for most of the SQL Server installations in local machine, the port `1433` is exposed.\n",
    "- We need to get the `SERVER_NAME` (Server IP Address/Hostname) of the SQL Server we want to connect.\n",
    "- The SQL Server path is then obtained by : `\"jdbc:sqlserver://{SQL_SERVER_NAME}:{SQL_SERVER_EXPOSED_PORT}\"`\n",
    "- Using this sql server path, and the database name(which we want to connect to), the connection URL is formed as `{SQL_SERVER_PATH};databaseName={DATABASE_NAME};`. This URL needs to be passed as `url` option while reading or writing data to sql server.\n",
    "- The format to be used while reading/writing data to SQL Server: `jdbc`\n",
    "- Also, `.option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")` must be passed as option, so as to specify the driver that is to be used for fetching/ingesting data to SQL server.\n",
    "- The other options to specify are `dbtable`, `user` and `password`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6100166f-99e0-47ba-bf58-3ea919e1139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_NAME = \"jdbc:sqlserver://10.0.2.5:1433\"\n",
    "SQL_SERVER_NAME = \"DEBANJAN\"\n",
    "SQL_SERVER_EXPOSED_PORT = \"1433\"\n",
    "\n",
    "SQL_SERVER_PATH = f\"jdbc:sqlserver://{SQL_SERVER_NAME}:{SQL_SERVER_EXPOSED_PORT}\"\n",
    "\n",
    "DATABASE_NAME = \"prac\"\n",
    "CONNECTION_URL = f\"{SQL_SERVER_PATH};databaseName={DATABASE_NAME};\"\n",
    "\n",
    "TABLE_NAME = \"crj_orders\"\n",
    "\n",
    "# Create an \".env\" file, and it, set environment variables named \"SQL_SERVER_USERNAME\" and \"SQL_SERVER_PASSWORD\", with the coorrect username and password, of the SQL to which we wannt to get connected.\n",
    "\n",
    "USERNAME = os.getenv(\"SQL_SERVER_USERNAME\")\n",
    "PASSWORD = os.getenv(\"SQL_SERVER_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd5170-eb5d-46ad-a559-5c9317406b4c",
   "metadata": {},
   "source": [
    "### Reading from SQL Server:\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae658c3-0683-4071-9267-a3481cd7fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_server_df = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", CONNECTION_URL)\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .option(\"dbtable\", TABLE_NAME)\n",
    "    .option(\"user\", USERNAME)\n",
    "    .option(\"password\", PASSWORD)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1010a3-2827-4fa7-90c7-7206d5e849d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|cust_id|       product|  price|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|      2|        Laptop|  35000|\n",
      "|  2|      3|        Scooty|  80000|\n",
      "|  3|      1|         Phone|  15000|\n",
      "|  4|      3|        Laptop|  45000|\n",
      "|  5|      4|           Car|1000000|\n",
      "|  6|      3|Dressing Table|  15000|\n",
      "|  7|      1|        IPhone|  69000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_server_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ac3a06-8480-4955-ab23-ba498e9b12bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+\n",
      "|cust_id|   fname|   lname|\n",
      "+-------+--------+--------+\n",
      "|      1|  Projna| Kabiraj|\n",
      "|      2|Debanjan|  Sarkar|\n",
      "|      3|  Nitika|  Sarkar|\n",
      "|      4|    Atul|   Kumar|\n",
      "|      5|   Tuhin|  Sarkar|\n",
      "|      7|  Sagnik|Bairagya|\n",
      "+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", CONNECTION_URL)\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .option(\"dbtable\", \"crj_customers\")\n",
    "    .option(\"user\", USERNAME)\n",
    "    .option(\"password\", PASSWORD)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "customers_df = customers_df.withColumnRenamed(\"id\", \"cust_id\")\n",
    "\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ea4753-38b6-4dc9-9d98-013b79b1655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|cust_id|       product|  price|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|      2|        Laptop|  35000|\n",
      "|  2|      3|        Scooty|  80000|\n",
      "|  3|      1|         Phone|  15000|\n",
      "|  4|      3|        Laptop|  45000|\n",
      "|  5|      4|           Car|1000000|\n",
      "|  6|      3|Dressing Table|  15000|\n",
      "|  7|      1|        IPhone|  69000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", CONNECTION_URL)\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .option(\"dbtable\", \"crj_orders\")\n",
    "    .option(\"user\", USERNAME)\n",
    "    .option(\"password\", PASSWORD)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7f3278c-91e0-4b0c-8f0e-243fdae7148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------+--------+--------+\n",
      "|  id|       product|  price|   fname|   lname|\n",
      "+----+--------------+-------+--------+--------+\n",
      "|   7|        IPhone|  69000|  Projna| Kabiraj|\n",
      "|   3|         Phone|  15000|  Projna| Kabiraj|\n",
      "|   6|Dressing Table|  15000|  Nitika|  Sarkar|\n",
      "|   4|        Laptop|  45000|  Nitika|  Sarkar|\n",
      "|   2|        Scooty|  80000|  Nitika|  Sarkar|\n",
      "|null|          null|   null|   Tuhin|  Sarkar|\n",
      "|   5|           Car|1000000|    Atul|   Kumar|\n",
      "|null|          null|   null|  Sagnik|Bairagya|\n",
      "|   1|        Laptop|  35000|Debanjan|  Sarkar|\n",
      "+----+--------------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = (\n",
    "    orders_df\n",
    "    .join(\n",
    "        customers_df,\n",
    "        orders_df.cust_id == customers_df.cust_id,\n",
    "        \"right\"\n",
    "    )\n",
    "    .drop(\"cust_id\")\n",
    ")\n",
    "\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee48d4-6e1f-49ea-9a36-098153e9a2d5",
   "metadata": {},
   "source": [
    "### Writing to SQL Server:\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5865b1c-b0f4-4b08-a021-518b1dae0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    join_df.write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", CONNECTION_URL)\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "    .option(\"dbtable\", \"pyspark_tbl\")\n",
    "    .option(\"user\", USERNAME)\n",
    "    .option(\"password\", PASSWORD)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7d67e-0c82-430d-bc57-6ec4a76b91e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
