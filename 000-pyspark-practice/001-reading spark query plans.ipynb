{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f278a742-1127-42dd-83cd-7655cd8a4ff8",
   "metadata": {},
   "source": [
    "<h1><center>Understanding Spark Query Plans</center></h1>\n",
    "<hr><hr>\n",
    "<center><img src=\"./images/000-spark-execution.png\"></center>\n",
    "\n",
    "Reference: <a href=\"https://www.youtube.com/watch?v=KnUXztKueMU&list=PLWAuYt0wgRcLCtWzUxNg4BjnYlCZNEVth\" target=\"_blank\">Master Reading Spark Query Plans - Afaque Ahmad</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c135c-76e6-4527-a816-08f84e41fe26",
   "metadata": {},
   "source": [
    "## Imports and Configs:\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0868d45-362f-4716-80ff-980ac4ba75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87afefe1-0252-4305-85a9-bcb0608bd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"001-reading spark query plans\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e127e8d-380b-4023-9e06-71aba5f2ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d54e81-c2bb-4e76-9e08-a1e4836a600e",
   "metadata": {},
   "source": [
    "## Reading Files:\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef81f27-7bf0-4daa-8cf6-085b447147a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"./data/data_skew/transactions.parquet\"\n",
    "transactions_df = spark.read.parquet( transactions_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162b49d5-e76a-4dfe-9cc5-5c27768db966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|      boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|     chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97| los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|     chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T1SMX9EUG21BBSE|2015-02-11|2015|    2| 11|    Education| 54.14|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T449R5YAV3BMX7O|2012-11-14|2012|   11| 14|     Gambling| 88.34|     seattle|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZHEHBKEXF1TPS2|2016-11-19|2016|   11| 19|    Groceries| 95.69|philadelphia|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TVYNJ1ZYAZI6O6J|2013-05-02|2013|    5|  2|     Gambling| 50.53|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TMRZVDMBXYVOYSH|2011-09-03|2011|    9|  3|          Tax|228.39| los_angeles|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb402e84-68d9-4426-b385-8e95ffc5ddd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39790092"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df43795b-b484-4157-a19e-bda1211c3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = \"./data/data_skew/customers.parquet\"\n",
    "customers_df = spark.read.parquet( customers_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba26b73-d08d-4c69-a605-df932d5260b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+------+----------+-----+------------+\n",
      "|cust_id   |name         |age|gender|birthday  |zip  |city        |\n",
      "+----------+-------------+---+------+----------+-----+------------+\n",
      "|C007YEYTX9|Aaron Abbott |34 |Female|7/13/1991 |97823|boston      |\n",
      "|C00B971T1J|Aaron Austin |37 |Female|12/16/2004|30332|chicago     |\n",
      "|C00WRSJF1Q|Aaron Barnes |29 |Female|3/11/1977 |23451|denver      |\n",
      "|C01AZWQMF3|Aaron Barrett|31 |Male  |7/9/1998  |46613|los_angeles |\n",
      "|C01BKUFRHA|Aaron Becker |54 |Male  |11/24/1979|40284|san_diego   |\n",
      "|C01RGUNJV9|Aaron Bell   |24 |Female|8/16/1968 |86331|denver      |\n",
      "|C01USDV4EE|Aaron Blair  |35 |Female|9/9/1974  |80078|new_york    |\n",
      "|C01WMZQ7PN|Aaron Brady  |51 |Female|8/20/1994 |52204|philadelphia|\n",
      "|C021567NJZ|Aaron Briggs |57 |Male  |3/10/1990 |22008|philadelphia|\n",
      "|C023M6MKR3|Aaron Bryan  |29 |Male  |4/10/1976 |05915|philadelphia|\n",
      "+----------+-------------+---+------+----------+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show( 10, truncate=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105352ac-1aca-43bc-8bf5-bf78806243a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154031e-dd1d-4708-a1d6-2f52802adeee",
   "metadata": {},
   "source": [
    "## Narrow Transformations:\n",
    "----------------------------\n",
    "- filter rows where `city='boston'`\n",
    "- add a new column: adding `first_name` and `last_name`\n",
    "- alter an exisitng column: adding 5 to age column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ab2290-237b-4c22-ad70-58591108817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+------+---------+\n",
      "|   cust_id|first_name|last_name| age|gender| birthday|\n",
      "+----------+----------+---------+----+------+---------+\n",
      "|C007YEYTX9|     Aaron|   Abbott|39.0|Female|7/13/1991|\n",
      "|C08XAQUY73|     Aaron|  Lambert|59.0|Female|11/5/1966|\n",
      "|C094P1VXF9|     Aaron|  Lindsey|29.0|  Male|9/21/1990|\n",
      "|C097SHE1EF|     Aaron|    Lopez|27.0|Female|4/18/2001|\n",
      "|C0DTC6436T|     Aaron| Schwartz|57.0|Female| 7/9/1962|\n",
      "|C0R42FPHRH|     Abbie|    Reyes|68.0|  Male|10/8/1995|\n",
      "|C0RZV4BH7T|     Abbie|Stevenson|41.0|  Male|2/10/1971|\n",
      "|C0U9RV3VBE|       Ada|  Andrews|47.0|  Male|6/10/1961|\n",
      "|C0XNANAD6L|       Ada|   Harper|57.0|  Male|4/16/1996|\n",
      "|C1869HFVF8|      Adam|    Clark|35.0|Female|7/17/1972|\n",
      "+----------+----------+---------+----+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "narrow_trans_df = ( \n",
    "    customers_df.filter( F.col(\"city\") == \"boston\" )\n",
    "                .withColumn( \"first_name\", F.split( \"name\", \" \" )[0] )\n",
    "                .withColumn( \"last_name\", F.split( \"name\", \" \" )[1] )\n",
    "                .withColumn( \"age\", F.col(\"age\") + F.lit(5) )\n",
    "                .select( [\"cust_id\", \"first_name\", \"last_name\", \"age\", \"gender\", \"birthday\"] ) \n",
    ")\n",
    "\n",
    "narrow_trans_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e3ad7c-65db-431a-98f9-2d3d4b1b7f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['cust_id, 'first_name, 'last_name, 'age, 'gender, 'birthday]\n",
      "+- Project [cust_id#84, name#85, (cast(age#86 as double) + cast(5 as double)) AS age#618, gender#87, birthday#88, zip#89, city#90, first_name#599, last_name#608]\n",
      "   +- Project [cust_id#84, name#85, age#86, gender#87, birthday#88, zip#89, city#90, first_name#599, split(name#85,  , -1)[1] AS last_name#608]\n",
      "      +- Project [cust_id#84, name#85, age#86, gender#87, birthday#88, zip#89, city#90, split(name#85,  , -1)[0] AS first_name#599]\n",
      "         +- Filter (city#90 = boston)\n",
      "            +- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, first_name: string, last_name: string, age: double, gender: string, birthday: string\n",
      "Project [cust_id#84, first_name#599, last_name#608, age#618, gender#87, birthday#88]\n",
      "+- Project [cust_id#84, name#85, (cast(age#86 as double) + cast(5 as double)) AS age#618, gender#87, birthday#88, zip#89, city#90, first_name#599, last_name#608]\n",
      "   +- Project [cust_id#84, name#85, age#86, gender#87, birthday#88, zip#89, city#90, first_name#599, split(name#85,  , -1)[1] AS last_name#608]\n",
      "      +- Project [cust_id#84, name#85, age#86, gender#87, birthday#88, zip#89, city#90, split(name#85,  , -1)[0] AS first_name#599]\n",
      "         +- Filter (city#90 = boston)\n",
      "            +- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#84, split(name#85,  , -1)[0] AS first_name#599, split(name#85,  , -1)[1] AS last_name#608, (cast(age#86 as double) + 5.0) AS age#618, gender#87, birthday#88]\n",
      "+- Filter (isnotnull(city#90) AND (city#90 = boston))\n",
      "   +- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [cust_id#84, split(name#85,  , -1)[0] AS first_name#599, split(name#85,  , -1)[1] AS last_name#608, (cast(age#86 as double) + 5.0) AS age#618, gender#87, birthday#88]\n",
      "+- *(1) Filter (isnotnull(city#90) AND (city#90 = boston))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [cust_id#84,name#85,age#86,gender#87,birthday#88,city#90] Batched: true, DataFilters: [isnotnull(city#90), (city#90 = boston)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [IsNotNull(city), EqualTo(city,boston)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "narrow_trans_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ad80c-a1f7-4a1b-8882-7899f7cd58ed",
   "metadata": {},
   "source": [
    "### Explaination:\n",
    "--------------------\n",
    "- Parsed Logical Plan is the Unresolved Logical Plan(step 1).\n",
    "- Analysed Logical Plan is the Logical Plan after involving Catalog.\n",
    "- The Physical Plan generated, as shown, at the end is the one that will be executed on the cluster:\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "<img src=\"./images/001-narrow_transform_pjysical_plan.jpg\">\n",
    "\n",
    "- The Physical plan execution steps are read from bottom (most-indented line) to top (least indented line), with each step starting with `+-` symbol.\n",
    "- Based on the above plan:\n",
    "    1. The parquet file is first scanned from the data source\n",
    "    2. Spark converts data from columnar(parquet is a columnar format) to row format, as it finds that it will optimise the processing and transformations.\n",
    "    3. It then filters the records, as we have applied a `city=boston` filter.\n",
    "    4. Then the rest transformations are handled in a single projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1641b55-05b5-4daa-9a97-445f442fec25",
   "metadata": {},
   "source": [
    "## Wide Transformations:\n",
    "-----------------------------\n",
    "- Repartition\n",
    "- Coalesce\n",
    "- Joins\n",
    "- GroupBy\n",
    "    - count\n",
    "    - countDistinct\n",
    "    - sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec4a20-1e83-4ac6-8c3d-5a46f04931f7",
   "metadata": {},
   "source": [
    "### 1. Repartition:\n",
    "-----------------------\n",
    "- Increase or decrease the number of partitions of an RDD, and distribute it to the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c080eaae-83f5-451d-809b-50b46521e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39deb6bd-588d-4ffc-8604-0e53d4381f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 24, true\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(24), REPARTITION_BY_NUM, [plan_id=340]\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.repartition( 24 ).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce92e77-ec54-429f-9511-30f44497c7be",
   "metadata": {},
   "source": [
    "#### Repartition Physical Plan Explainantion:\n",
    "--------------------------------------------------\n",
    "<img src=\"./images/002-repartition_physical_plan.jpg\">\n",
    "\n",
    "- `Exchange` word in query plan `.explain()` means there is Shuffling occuring at that stage.\n",
    "- `RoundRobinPartitioning(24)` means the scheme/logic followed while partitioning. Parameter contains the number of resulting partitions. In this schema, distribution happens as(in this case):\n",
    "    - 1st row/record is sent to 1st partition,\n",
    "    - 2nd row/record is sent to 2nd partition,\n",
    "    - 3rd row/record is sent to 3rd partition, ...\n",
    "    - 24th row/record is sent to 24th partition,\n",
    "    - and again 25th row/record is sent to 1st partition,\n",
    "- `AdaptiveSparkPlan isFinalPlan=false` means the code hasn't run yet, and the displayed Physical Plan is not the final chosen plan. As AQE in on (by default is remains on), based on runtime statistics, it can choose some other physical plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f39fb-8b09-4303-b2e7-498832af8b29",
   "metadata": {},
   "source": [
    "### 2. Coalesce:\n",
    "-----------------\n",
    "- **Coalesce is better than Repartition for reducing number of partitions** because it tries to avoid shuffling and data transfer accross executors.(Does shuffling only for edge cases)\n",
    "- If possible, it combines multiple partitions within same executors, to reduce their number with the number of partitions asked to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "132b2d1a-8301-4d34-a424-e8fd12870ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d26770e-4746-403b-b899-e0835ddaadae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 1, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string\n",
      "Repartition 1, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 1, false\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "Coalesce 1\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.coalesce(1).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a0ad0-289c-450c-84f4-17c2736615b7",
   "metadata": {},
   "source": [
    "#### Why doesn't .coalesce() explicitly show the partitioning scheme?\n",
    "`.coalesce` doesn't show the partitioning scheme e.g. `RoundRobinPartitioning` because:\n",
    "\n",
    "- The operation only minimizes data movement by merging into fewer partitions, **it doesn't do any shuffling.**\n",
    "- Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by `.coalesce`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44f590-18f3-482e-b024-b767dca23b7a",
   "metadata": {},
   "source": [
    "### 3. Joins:\n",
    "------------------\n",
    "- By default, Broadcast join is enabled. We can disable it by following code: \\\n",
    "    `spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)`\n",
    "\n",
    "- `spark.sql.autoBroadcastJoinThreshold` configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. Its default value is 10MB. By setting this value to -1, broadcasting can be disabled, and the joins will then be Sort-Merge Join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c315290-0ce5-406a-b0cf-24c916b9301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760b\n"
     ]
    }
   ],
   "source": [
    "print( spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f1f1cf7-1e81-44ef-97a5-a7871b3a8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set( \"spark.sql.autoBroadcastJoinThreshold\", -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d5357c-042d-4154-b640-b151d9af347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print( spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f462fe28-92f6-4206-a53e-597fcc7454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    transactions_df.join(\n",
    "        customers_df,\n",
    "        how=\"inner\",\n",
    "        on=\"cust_id\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f2c61a8-e3f4-4ffd-b2b3-b9b2f999c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(cust_id))\n",
      ":- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "+- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, start_date: string, end_date: string, txn_id: string, date: string, year: string, month: string, day: string, expense_type: string, amt: string, city: string, name: string, age: string, gender: string, birthday: string, zip: string, city: string\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#85, age#86, gender#87, birthday#88, zip#89, city#90]\n",
      "+- Join Inner, (cust_id#0 = cust_id#84)\n",
      "   :- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#85, age#86, gender#87, birthday#88, zip#89, city#90]\n",
      "+- Join Inner, (cust_id#0 = cust_id#84)\n",
      "   :- Filter isnotnull(cust_id#0)\n",
      "   :  +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "   +- Filter isnotnull(cust_id#84)\n",
      "      +- Relation [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#0, start_date#1, end_date#2, txn_id#3, date#4, year#5, month#6, day#7, expense_type#8, amt#9, city#10, name#85, age#86, gender#87, birthday#88, zip#89, city#90]\n",
      "   +- SortMergeJoin [cust_id#0], [cust_id#84], Inner\n",
      "      :- Sort [cust_id#0 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [plan_id=382]\n",
      "      :     +- Filter isnotnull(cust_id#0)\n",
      "      :        +- FileScan parquet [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] Batched: true, DataFilters: [isnotnull(cust_id#0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,start_date:string,end_date:string,txn_id:string,date:string,year:string,mon...\n",
      "      +- Sort [cust_id#84 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#84, 200), ENSURE_REQUIREMENTS, [plan_id=383]\n",
      "            +- Filter isnotnull(cust_id#84)\n",
      "               +- FileScan parquet [cust_id#84,name#85,age#86,gender#87,birthday#88,zip#89,city#90] Batched: true, DataFilters: [isnotnull(cust_id#84)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:string,name:string,age:string,gender:string,birthday:string,zip:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186be9ae-dd13-4c08-a99f-c3ea464339b0",
   "metadata": {},
   "source": [
    "#### SortMergeJoin Physical plan:\n",
    "-------------------------------------\n",
    "<img src=\"./images/003-sortmergejoin-physical-plan.jpg\">\n",
    "\n",
    "\n",
    "- `Filter isnotnull(cust_id#84)` means as the `cust_id` is the join column, thus spark filters out all the `null` values in it as a part of its own optimization.\n",
    "- `+- Exchange hashpartitioning(cust_id#84, 200)` means it does a Haspartition shuffling, and number of partitions shuffled is 200.\n",
    "- After the above steps, the `cust_id` column is sorted.\n",
    "- It does it for both dataframe `cust_id` columns, then does SortMergeJoin operation, and then does projection.\n",
    "\n",
    "HashPartition Scheme\n",
    "----------------------\n",
    "- Partitioning schema is an algorithm that determines which row(record) will go to which partition (partition can also be called shuffle-partition).\n",
    "- In HashPartition Scheme, the partition number in which a record will go is decided by the following formula: \\\n",
    "    `(Target Partition number of a record) = Hash(Key column's value) modulo (Num of partitions)`\n",
    "- In our case, num of partitions of larger dataset = 12, and Key column is `cust_id`. Each partition is numbered/indexed as 0, 1, 2, ..., 11. Out of these indexes, eaxh record will be sent to specific indexed partition after shuffle, which is decided as: \\\n",
    "    `Target Partition Index = Hash(cust_id) % 12`\n",
    "For same `cust_id` value, is Hash() value will be same, thus records of both dataframes will go into same partition, and then Sorting and Joining will be easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23c2b1-37f6-4f97-a308-be86fe1485cb",
   "metadata": {},
   "source": [
    "### 4. GroupBy\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69582879-959a-4907-913c-909d3b063c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26cd3d81-ef44-43ab-8549-228946ae9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts_df = (\n",
    "    transactions_df.groupBy(\"city\").count() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "661dd5a5-6394-48bd-be78-3c650c48c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, count(1) AS count#733L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, count: bigint\n",
      "Aggregate [city#10], [city#10, count(1) AS count#733L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, count(1) AS count#733L]\n",
      "+- Project [city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[count(1)], output=[city#10, count#733L])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [plan_id=399]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_count(1)], output=[city#10, count#737L])\n",
      "         +- FileScan parquet [city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_counts_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f137068-71d1-4535-95cd-6f968311febe",
   "metadata": {},
   "source": [
    "#### GroupBy Count Physical Plan:\n",
    "------------------------------------\n",
    "<img src=\"./images/004-groupBy_count_physical_plan.jpg\">\n",
    "\n",
    "\n",
    "- `+- HashAggregate(keys=[city#10], functions=[partial_count(1)], output=[city#10, count#737L])` means by `HashAggregate`it counts all key column values in a single partition locally. For example, let's say, there are 3 city's(city is groupBy column thus the key column): A, B and C. Then, count of occurence of A, B and C is done locally at each partition level. It gives value counts at each partition, and is known as `Partial Count`, and is thus operated by `partial_count()` function.\n",
    "- `+- Exchange hashpartitioning(city#10, 200)` means the Keys and their partially counted values are shuffled, where same keys are sent to same partition, and then their total counting takes place.\n",
    "- After HashPartition shuffling, final count is again done by HashAggeration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "244f0bbc-a1bb-458c-b391-e04fcfd2014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "amount_per_city_df = (\n",
    "    transactions_df.groupBy(\"city\").agg( F.sum(\"amt\") )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "440129f2-f0d2-4c4b-a2c5-23e57926824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, sum('amt) AS sum(amt)#750]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, sum(amt): double\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS sum(amt)#750]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#10], [city#10, sum(cast(amt#9 as double)) AS sum(amt)#750]\n",
      "+- Project [amt#9, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#10], functions=[sum(cast(amt#9 as double))], output=[city#10, sum(amt)#750])\n",
      "   +- Exchange hashpartitioning(city#10, 200), ENSURE_REQUIREMENTS, [plan_id=412]\n",
      "      +- HashAggregate(keys=[city#10], functions=[partial_sum(cast(amt#9 as double))], output=[city#10, sum#754])\n",
      "         +- FileScan parquet [amt#9,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<amt:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amount_per_city_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba0357-acce-4626-802c-eba305a8b1ca",
   "metadata": {},
   "source": [
    "- In this example, everything - logical part is same, only in place of `partial_count`, the `partial_sum` function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feed05a-dd76-421c-b0bd-5342db1489f9",
   "metadata": {},
   "source": [
    "- **Another example of `groupBy()` with distinct count**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deab6caa-9381-49d8-92f0-afa703efa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_city_per_customer_df = (\n",
    "    transactions_df.groupBy(\"cust_id\")\n",
    "                   .agg( F.countDistinct(\"city\") )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf9cab29-7aed-44f5-a937-7aa905bc2067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   cust_id|count(city)|\n",
      "+----------+-----------+\n",
      "|CPP8BY8U93|         10|\n",
      "|CYB8BX9LU1|         10|\n",
      "|CFRT841CCD|         10|\n",
      "|CA0TSNMYDK|         10|\n",
      "|COZ8NONEVZ|         10|\n",
      "|C46OCVH3WG|         10|\n",
      "|C1QF29WCA6|         10|\n",
      "|CTJBQB0OJ1|         10|\n",
      "|CD0DXL8XTM|         10|\n",
      "|CADBQ5OL5C|         10|\n",
      "+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['cust_id], ['cust_id, 'count(distinct 'city) AS count(city)#766]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: string, count(city): bigint\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS count(city)#766L]\n",
      "+- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [cust_id#0], [cust_id#0, count(distinct city#10) AS count(city)#766L]\n",
      "+- Project [cust_id#0, city#10]\n",
      "   +- Relation [cust_id#0,start_date#1,end_date#2,txn_id#3,date#4,year#5,month#6,day#7,expense_type#8,amt#9,city#10] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[cust_id#0], functions=[count(distinct city#10)], output=[cust_id#0, count(city)#766L])\n",
      "   +- Exchange hashpartitioning(cust_id#0, 200), ENSURE_REQUIREMENTS, [plan_id=534]\n",
      "      +- HashAggregate(keys=[cust_id#0], functions=[partial_count(distinct city#10)], output=[cust_id#0, count#778L])\n",
      "         +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "            +- Exchange hashpartitioning(cust_id#0, city#10, 200), ENSURE_REQUIREMENTS, [plan_id=530]\n",
      "               +- HashAggregate(keys=[cust_id#0, city#10], functions=[], output=[cust_id#0, city#10])\n",
      "                  +- FileScan parquet [cust_id#0,city#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/E:/Programs & Codes/apache_spark/my_pyspark_notebooks/000-pyspar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:string,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_city_per_customer_df.show(10)\n",
    "distinct_city_per_customer_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2068e56-bf1d-4c5f-997c-2d0c196d4b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
