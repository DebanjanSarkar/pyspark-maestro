{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1f3ab4-5f49-4221-af16-df36fb0945ed",
   "metadata": {},
   "source": [
    "<h1><center>Spark Streaming Read from Files | Flatten JSON data</center></h1>\n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05293d-a35d-4d73-8224-ed92b8e67dc4",
   "metadata": {},
   "source": [
    "- Sample JSON file data using which we will be working in this notebook:\n",
    "```\n",
    "{\n",
    "  \"eventId\": \"e3cb26d3-41b2-49a2-84f3-0156ed8d7502\",\n",
    "  \"eventOffset\": 10001,\n",
    "  \"eventPublisher\": \"device\",\n",
    "  \"customerId\": \"CI00103\",\n",
    "  \"data\": {\n",
    "    \"devices\": [\n",
    "      {\n",
    "        \"deviceId\": \"D001\",\n",
    "        \"temperature\": 15,\n",
    "        \"measure\": \"C\",\n",
    "        \"status\": \"ERROR\"\n",
    "      },\n",
    "      {\n",
    "        \"deviceId\": \"D002\",\n",
    "        \"temperature\": 16,\n",
    "        \"measure\": \"C\",\n",
    "        \"status\": \"SUCCESS\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"eventTime\": \"2023-01-05 11:13:53.643364\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514c5fb-099a-44d4-b762-43a1bf887ff5",
   "metadata": {},
   "source": [
    "- Install the package `ipynbname`, if not preset, using `pip install ipynbname`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced93f54-5e45-459a-9aaa-5f522c4f500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ipynbname\n",
      "Version: 2024.1.0.0\n",
      "Summary: Simply returns either notebook filename or the full path to the notebook when run from Jupyter notebook in browser.\n",
      "Home-page: https://github.com/msm1089/ipynbname\n",
      "Author: Mark McPherson\n",
      "Author-email: msm1089@yahoo.co.uk\n",
      "License: MIT\n",
      "Location: e:\\programs & codes\\apache_spark\\_spark_venv\\lib\\site-packages\n",
      "Requires: ipykernel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bf270a-a308-4f7d-8d25-c0af2b69c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipynbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905bd9cc-7443-46fb-8d55-2dcfc7ff1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004-flatten_streaming_JSON_files\n"
     ]
    }
   ],
   "source": [
    "notebook_name = ipynbname.name()\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0c814b-8b5a-47ca-aa39-e9ee8388b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f167b8-f507-4362-866d-c983196e3924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>004 - flatten streaming JSON files data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14383de3940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = ( \n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"004 - flatten streaming JSON files data\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .getOrCreate() \n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f36ce0-77f7-4d56-aa95-065ba9eadc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463743a4-05d3-45fe-bcb7-f25d8f3189a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(f\"./checkpoints/{notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb24e6ed-a32a-4e40-ae37-22b9e9e63778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base checkpoint directory './checkpoints/004-flatten_streaming_JSON_files' already exists.\n"
     ]
    }
   ],
   "source": [
    "# This code snippet is the base directory for saving checkpoints of streaming jobs exists. If it does not exists, creates the base directory.\n",
    "BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK = f\"./checkpoints/{notebook_name}\"\n",
    "\n",
    "try:\n",
    "    os.listdir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )\n",
    "    print(f\"Base checkpoint directory '{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}' already exists.\")\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK)\n",
    "    print(f\"Base checkpoint directory, did not exist previously, so, it has been created with the relative path: '{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac29dce2-ea71-4ab9-a223-d9ec16d74e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The checkpoint directory to be utilised for current execution of spark streaming job: './checkpoints/004-flatten_streaming_JSON_files/7'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# For each run of the notebook streaming job, the directory to be used will be in the pattern \"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\", \"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/2\", ... and so on.\n",
    "# Thus, for each run, the below code snippet checks the highest existing numbered folder inside the \"BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK\". If that highest numbered folder is empty(does not contain any checkpoint files), that will be set as checkpoint directory for current run, else, a new folder with ( 1+ highest numbered folder ) will be created, and will be used as the checkpoint directory.\n",
    "\"\"\"\n",
    "def update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK ):\n",
    "    # get list of all folders inside the 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK'\n",
    "    past_checkpoint_dirs_of_this_notebook = os.listdir(BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK)\n",
    "    \n",
    "    \n",
    "    if len(past_checkpoint_dirs_of_this_notebook) == 0:\n",
    "        # If 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK' is empty, then create a folder named '1' inside it, and select this '1' named folder path as 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "        os.mkdir(f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\")\n",
    "        CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\"\n",
    "    else:\n",
    "        # If 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK' is not empty, then we need to get the highest numbered folder.\n",
    "    \n",
    "        # Converting folder number strings to integers, to currently get the folder name having highest number\n",
    "        checkpoint_folder_names_converted_to_integers = [int(i) for i in past_checkpoint_dirs_of_this_notebook]\n",
    "        highest_existing_folder_number = max(checkpoint_folder_names_converted_to_integers)  # max() gives the largest integer value in this list of folder names\n",
    "    \n",
    "        # checking if the highest numbered folder is empty, or it contains checkpoint related files inside it.\n",
    "        # This check is important as it might happen that a new folder is created but not used for storing any checkpoint data. Then, without creating another new folder with higher number, we will use that existing highest numbered empty folder.\n",
    "        if len( os.listdir( f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{highest_existing_folder_number}\" ) ) == 0:\n",
    "            # If existing highest numbered folder is empty, then its path will be set to 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "            CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{highest_existing_folder_number}\"\n",
    "        else:\n",
    "            # If existing highest numbered folder is not empty(contains checkpoint files), then a new folder with number 1 higher than highest number will be created, and its path will be set to 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "            new_folder_number = highest_existing_folder_number + 1\n",
    "            os.mkdir(f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{new_folder_number}\")\n",
    "            CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{new_folder_number}\"\n",
    "\n",
    "    return CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN\n",
    "    \n",
    "print(f\"The checkpoint directory to be utilised for current execution of spark streaming job: '{update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26360b-1467-4f66-abe5-2bed1d11ccd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Batch Code for JSON transformation and flattening:\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef48b9e-09ab-48bd-a708-056037f1cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(f\"./data/{notebook_name}/input/device_files/device_01.json\")\n",
    ")\n",
    "\n",
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599e376a-8d6d-4d4f-a452-a2d29b39374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|customerId|data                                            |eventId                             |eventOffset|eventPublisher|eventTime                 |\n",
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|CI00103   |{[{D001, C, ERROR, 15}, {D002, C, SUCCESS, 16}]}|e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|\n",
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28130fa-6973-4aa8-8987-b7414c5b66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "|eventId                             |eventOffset|eventPublisher|customerId|deviceId|temperature|measure|status |eventTime                 |\n",
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "|e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |CI00103   |D001    |15         |C      |ERROR  |2023-01-05 11:13:53.643364|\n",
      "|e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |CI00103   |D002    |16         |C      |SUCCESS|2023-01-05 11:13:53.643364|\n",
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_df = ( \n",
    "    json_df.withColumn( \"data\", F.explode( \"data.devices\" ) )\n",
    "    .withColumn( \"deviceId\", F.col(\"data.deviceId\") )\n",
    "    .withColumn( \"temperature\", F.col(\"data.temperature\") )\n",
    "    .withColumn( \"measure\", F.col(\"data.measure\") )\n",
    "    .withColumn( \"status\", F.col(\"data.status\") )\n",
    "    # .drop(\"data\")\n",
    "    .select( \"eventId\", \"eventOffset\", \"eventPublisher\", \"customerId\", \"deviceId\", \"temperature\", \"measure\", \"status\", \"eventTime\" )\n",
    ")\n",
    "\n",
    "flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cc92c8-8460-498c-8007-60a0248763c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|map_type_data                                                         |\n",
      "+----------------------------------------------------------------------+\n",
      "|{deviceId -> D001, temperature -> 15, measure -> C, status -> ERROR}  |\n",
      "|{deviceId -> D002, temperature -> 16, measure -> C, status -> SUCCESS}|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting StructType data to MapType data. \n",
    "# The column \"data\" in \"json_df\" is a struct-type column containing single key \"devices\", which contains an array of StructType objects. Below code flattents each StructType object in the array.\n",
    "\n",
    "map_type_df = ( \n",
    "    json_df.select( F.explode(\"data.devices\").alias(\"exploded_devices\") )\n",
    "    .select( F.create_map(\n",
    "        F.lit(\"deviceId\"), F.col(\"exploded_devices.deviceId\"),\n",
    "        F.lit(\"temperature\"), F.col(\"exploded_devices.temperature\"),\n",
    "        F.lit(\"measure\"), F.col(\"exploded_devices.measure\"),\n",
    "        F.lit(\"status\"), F.col(\"exploded_devices.status\")\n",
    "    ).alias( \"map_type_data\" ) ) \n",
    ")\n",
    "\n",
    "map_type_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "287fcc9c-3c22-4c9c-839d-2f95a667b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+-----------+----------+\n",
      "|map_type_data                                                         |data_key   |data_value|\n",
      "+----------------------------------------------------------------------+-----------+----------+\n",
      "|{deviceId -> D001, temperature -> 15, measure -> C, status -> ERROR}  |deviceId   |D001      |\n",
      "|{deviceId -> D001, temperature -> 15, measure -> C, status -> ERROR}  |temperature|15        |\n",
      "|{deviceId -> D001, temperature -> 15, measure -> C, status -> ERROR}  |measure    |C         |\n",
      "|{deviceId -> D001, temperature -> 15, measure -> C, status -> ERROR}  |status     |ERROR     |\n",
      "|{deviceId -> D002, temperature -> 16, measure -> C, status -> SUCCESS}|deviceId   |D002      |\n",
      "|{deviceId -> D002, temperature -> 16, measure -> C, status -> SUCCESS}|temperature|16        |\n",
      "|{deviceId -> D002, temperature -> 16, measure -> C, status -> SUCCESS}|measure    |C         |\n",
      "|{deviceId -> D002, temperature -> 16, measure -> C, status -> SUCCESS}|status     |SUCCESS   |\n",
      "+----------------------------------------------------------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now apply \"explode()\" and \"pos_explode()\" to MapType() column\n",
    "\n",
    "map_type_df.select( \"*\" , F.explode(\"map_type_data\").alias( \"data_key\", \"data_value\" ) ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67ff9b-7780-4b87-b54f-28f71a7bdfb9",
   "metadata": {},
   "source": [
    "## Converting Batch Code to Streaming Code for setting the JSON flattening as a streaming job:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ec4d15-d661-40f8-aa9f-dcfbdd2816bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus configuration allows automatic Schema Inference, while reading streaming data\n",
    "# This configuration for spark streaming is equivalent to \"inferSchema=True\" in pyspark batch processing\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9049c5a-9b9c-4b9d-af40-f4b9436f0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The \"cleanSource\" option is used to delete or archive the input files, as soon they are read by streaming query. It has 3 possible values: \"off\"(default), \"archive\", and \"delete\"\n",
    "# By default, it leaves the read files untouched. When value of \"cleanSource\" option is set to \"delete\", it deletes the files as soon they are read.\n",
    "# When \"cleanSource\" option value is set to \"archive\", then as soon as a file is read, it moves that file to the directory mentioned using \n",
    "\n",
    "\n",
    "# (\"maxFilesPerTrigger\", 1) option: For each trigger/microbatch, only 1 file will be processed by this streaming read query. Without setting this option, spark will try to process as many files as it gets in the input directory, during execution. Setting this option is good for production workloads.\n",
    "\"\"\"\n",
    "\n",
    "streaming_json_df = (\n",
    "    spark.readStream\n",
    "    .format(\"json\")\n",
    "    .option(\"cleanSource\", \"archive\")\n",
    "    .option(\"sourceArchiveDir\", \"./data/\"+notebook_name+\"/archive\")\n",
    "    .option(\"spark.sql.streaming.fileSource.cleaner.numThreads\", \"0\")  # If thread is set to 0, it will force the main thread to do the cleanup\n",
    "    .option(\"maxFilesPerTrigger\", 1)   \n",
    "    .load(f\"./data/{notebook_name}/input/device_files/\")  # This path must be a path to a directory, and not path to a file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b8c63d-5f41-4592-b1bf-4cf217fb5928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904c0aee-1ca9-4f61-a89c-c958f0bbe5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_flattened_df = ( \n",
    "    streaming_json_df.withColumn( \"data\", F.explode( \"data.devices\" ) )\n",
    "    .withColumn( \"deviceId\", F.col(\"data.deviceId\") )\n",
    "    .withColumn( \"temperature\", F.col(\"data.temperature\") )\n",
    "    .withColumn( \"measure\", F.col(\"data.measure\") )\n",
    "    .withColumn( \"status\", F.col(\"data.status\") )\n",
    "    # .drop(\"data\")\n",
    "    .select( \"eventId\", \"eventOffset\", \"eventPublisher\", \"customerId\", \"deviceId\", \"temperature\", \"measure\", \"status\", \"eventTime\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bc477aa-70d5-49d4-bdb0-4cb0496ec8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ace2e0-d7a9-46a8-b3cf-ff99692d1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )\n",
    "CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = \"./checkpoints/004-flatten_streaming_JSON_files/7\"\n",
    "print(CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28eb074-8143-4555-a055-350877792550",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_to_console_query = (\n",
    "    streaming_flattened_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2739e1-75cf-4a44-a990-07947851dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_as_csv_query = (\n",
    "    streaming_flattened_df.writeStream\n",
    "    .format(\"csv\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    .option(\"path\", \"./data/\"+notebook_name+\"/output/device_data.csv\")  # Path where the streaming data will be written\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d54dff-d97c-453f-94aa-77961ea2ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_to_console_query.awaitTermination()\n",
    "# stream_write_as_csv_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb3ab5-3ddd-4d7d-a8d0-7df1d3d7942c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
