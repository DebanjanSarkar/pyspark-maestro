{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78c15ff-d759-4b29-9efe-c308c6c81503",
   "metadata": {},
   "source": [
    "<h1><center>Spark Streaming Read from Kafka | Real time streaming from Kafka</center></h1>\n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e773b9-a0dd-40b9-9215-09aaca150705",
   "metadata": {},
   "source": [
    "- Spark Streaming connects to a kafka broker and subscribes to a topic, in order to receive data from that topic.\n",
    "- By default, the number of partitions a Kafka topic has will be the number of partitions spark will make for the kafka topic data being read, and thus number of tasks for processing each microBatch will be also the same as number of partitions of the topic in Kafka.\n",
    "\n",
    "#### NOTE:\n",
    "------------\n",
    "- Thus, **to tune Kafka streaming jobs that runs slowly, we need to partition the topic in Kafka to appropriate number of partitions, such that spark can acheive parallelism while reading data from kafka.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35b1e7-3598-49dc-ab44-5d8fb9eb8ec1",
   "metadata": {},
   "source": [
    "- Required jar files for executing kafka related codes of this notebook, for this specific spark version `3.3.2`: \\\n",
    "    - `spark-sql-kafka-0-10_2.12:3.3.2`\n",
    "    - `kafka-clients-3.7.1`\n",
    "    - `spark-streaming-kafka-0-10-assembly_2.12-3.3.2`\n",
    "    - `spark-token-provider-kafka-0-10_2.12-3.3.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40abbecd-23f4-413f-995a-cd4ccc92772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f114d8-ce80-49e5-b977-b0114b4c3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005-kafka_streaming_data_read\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "import os\n",
    "\n",
    "notebook_name = ipynbname.name()\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b26f90-5792-4257-9d89-fd4eaa359ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>005-kafka_streaming_data_read</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21de94b1f90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName( notebook_name )\n",
    "    .config(\"spark.streaming.stopGracefulllyOnShutdown\", True)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84627776-f50c-4cf5-8a18-a2b84aa81057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97dadf42-1770-4e72-ad03-19b910d6234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_SERVER = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"device_data2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0577c-162a-43cf-b16f-c12a54810d9e",
   "metadata": {},
   "source": [
    "## Batch Code for Kafka data processing:\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d290b62f-e515-4e97-8206-6e2636674efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark.read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af779c3d-0795-447f-9fff-2fedd9cf9f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
      "|[65 37 32 36 37 3...|[7B 22 65 76 65 6...|device_data2|        0|     0|2024-07-23 13:07:...|            0|\n",
      "|[39 66 34 65 65 3...|[7B 22 65 76 65 6...|device_data2|        0|     1|2024-07-23 13:07:...|            0|\n",
      "|[66 33 31 31 30 3...|[7B 22 65 76 65 6...|device_data2|        0|     2|2024-07-23 13:07:...|            0|\n",
      "|[65 31 62 37 34 3...|[7B 22 65 76 65 6...|device_data2|        0|     3|2024-07-23 13:07:...|            0|\n",
      "|[61 35 64 66 31 6...|[7B 22 65 76 65 6...|device_data2|        0|     4|2024-07-23 13:07:...|            0|\n",
      "|[34 65 37 36 35 3...|[7B 22 65 76 65 6...|device_data2|        0|     5|2024-07-23 13:07:...|            0|\n",
      "|[38 37 36 31 30 3...|[7B 22 65 76 65 6...|device_data2|        0|     6|2024-07-23 13:07:...|            0|\n",
      "|[65 61 38 32 32 6...|[7B 22 65 76 65 6...|device_data2|        0|     7|2024-07-23 13:07:...|            0|\n",
      "|[61 35 38 66 34 3...|[7B 22 65 76 65 6...|device_data2|        0|     8|2024-07-23 13:07:...|            0|\n",
      "|[66 32 61 32 65 3...|[7B 22 65 76 65 6...|device_data2|        0|     9|2024-07-23 13:07:...|            0|\n",
      "|[39 66 36 33 64 3...|[7B 22 65 76 65 6...|device_data2|        0|    10|2024-07-23 13:08:...|            0|\n",
      "|[32 39 66 65 38 3...|[7B 22 65 76 65 6...|device_data2|        0|    11|2024-07-23 13:08:...|            0|\n",
      "|[33 30 32 62 66 3...|[7B 22 65 76 65 6...|device_data2|        0|    12|2024-07-23 13:08:...|            0|\n",
      "|[62 64 39 33 66 6...|[7B 22 65 76 65 6...|device_data2|        0|    13|2024-07-23 13:08:...|            0|\n",
      "|[34 37 37 36 61 3...|[7B 22 65 76 65 6...|device_data2|        0|    14|2024-07-23 13:08:...|            0|\n",
      "|[35 63 63 32 36 3...|[7B 22 65 76 65 6...|device_data2|        0|    15|2024-07-23 13:08:...|            0|\n",
      "|[37 38 35 30 33 3...|[7B 22 65 76 65 6...|device_data2|        0|    16|2024-07-23 13:08:...|            0|\n",
      "|[32 33 30 61 33 3...|[7B 22 65 76 65 6...|device_data2|        0|    17|2024-07-23 13:08:...|            0|\n",
      "|[37 30 38 62 62 6...|[7B 22 65 76 65 6...|device_data2|        0|    18|2024-07-23 13:08:...|            0|\n",
      "|[37 30 32 31 38 6...|[7B 22 65 76 65 6...|device_data2|        0|    19|2024-07-23 13:08:...|            0|\n",
      "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6544e69-3c2b-4775-981c-108042b1a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b173fc26-3986-4afb-a5af-47ac97cd3f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"eventId\": \"d1c50fc5-d6ef-42ee-a94b-c68f0c1b22c3\", \"eventOffset\": 10000, \"eventPublisher\": \"device\", \"customerId\": \"CI00102\", \"data\": {\"devices\": [{\"deviceId\": null, \"temperature\": 14, \"measure\": \"C\", \"status\": \"ERROR\"}, {\"deviceId\": \"D002\", \"temperature\": 15, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D001\", \"temperature\": 10, \"measure\": \"C\", \"status\": \"STANDBY\"}]}, \"eventTime\": \"2024-07-23 13:07:40.751972\"}    |\n",
      "|{\"eventId\": \"728a1974-43a7-47c6-a67e-76e7d578a798\", \"eventOffset\": 10001, \"eventPublisher\": \"device\", \"customerId\": \"CI00105\", \"data\": {\"devices\": [{\"deviceId\": \"D003\", \"temperature\": 9, \"measure\": \"C\", \"status\": \"SUCCESS\"}, {\"deviceId\": \"D002\", \"temperature\": 12, \"measure\": \"C\", \"status\": \"SUCCESS\"}]}, \"eventTime\": \"2024-07-23 13:07:42.907318\"}                                                                               |\n",
      "|{\"eventId\": \"902b9b46-b665-4f5f-ab74-170cc593373e\", \"eventOffset\": 10002, \"eventPublisher\": \"device\", \"customerId\": \"CI00101\", \"data\": {\"devices\": [{\"deviceId\": null, \"temperature\": 7, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D004\", \"temperature\": 15, \"measure\": \"C\", \"status\": null}]}, \"eventTime\": \"2024-07-23 13:07:43.227707\"}                                                                                      |\n",
      "|{\"eventId\": \"145328aa-d8cb-42c9-9941-49ea2f3ab50a\", \"eventOffset\": 10003, \"eventPublisher\": \"device\", \"customerId\": \"CI00108\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:07:45.358356\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"f3082e6b-78f7-465f-a6c6-904858101494\", \"eventOffset\": 10004, \"eventPublisher\": \"device\", \"customerId\": \"CI00109\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:07:45.480498\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"798e55fa-c9d9-49b0-a33a-296ea07afa24\", \"eventOffset\": 10005, \"eventPublisher\": \"device\", \"customerId\": \"CI00117\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:07:48.612853\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"43208442-74cd-42f5-99e7-4c3bb9299a8b\", \"eventOffset\": 10006, \"eventPublisher\": \"device\", \"customerId\": \"CI00116\", \"data\": {\"devices\": [{\"deviceId\": \"D003\", \"temperature\": 13, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D005\", \"temperature\": 29, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D003\", \"temperature\": 28, \"measure\": \"C\", \"status\": \"STANDBY\"}]}, \"eventTime\": \"2024-07-23 13:07:52.749140\"}|\n",
      "|{\"eventId\": \"5bc238e1-add2-43ae-b77c-9a6ee429c274\", \"eventOffset\": 10007, \"eventPublisher\": \"device\", \"customerId\": \"CI00116\", \"data\": {\"devices\": [{\"deviceId\": null, \"temperature\": 2, \"measure\": \"C\", \"status\": null}]}, \"eventTime\": \"2024-07-23 13:07:53.881108\"}                                                                                                                                                                    |\n",
      "|{\"eventId\": \"e7f906fc-c578-4a3a-b035-cf20abda0a0a\", \"eventOffset\": 10008, \"eventPublisher\": \"device\", \"customerId\": \"CI00106\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:07:55.003669\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"543bd562-1e38-486c-b4c1-00f35b1ea7e3\", \"eventOffset\": 10009, \"eventPublisher\": \"device\", \"customerId\": \"CI00101\", \"data\": {\"devices\": [{\"deviceId\": \"D002\", \"temperature\": 20, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D003\", \"temperature\": 22, \"measure\": \"C\", \"status\": \"SUCCESS\"}, {\"deviceId\": \"D005\", \"temperature\": 25, \"measure\": \"C\", \"status\": null}]}, \"eventTime\": \"2024-07-23 13:07:57.133888\"}     |\n",
      "|{\"eventId\": \"365688f4-b6bb-443b-b5f6-cca01ebf28af\", \"eventOffset\": 10010, \"eventPublisher\": \"device\", \"customerId\": \"CI00107\", \"data\": {\"devices\": [{\"deviceId\": \"D002\", \"temperature\": 6, \"measure\": \"C\", \"status\": \"SUCCESS\"}]}, \"eventTime\": \"2024-07-23 13:08:01.270458\"}                                                                                                                                                             |\n",
      "|{\"eventId\": \"8c4d3a2f-5fc5-4e82-848d-4c030219e843\", \"eventOffset\": 10011, \"eventPublisher\": \"device\", \"customerId\": \"CI00112\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:08:06.399806\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"22f0aba5-8090-4bb6-a206-3d855c839315\", \"eventOffset\": 10012, \"eventPublisher\": \"device\", \"customerId\": \"CI00110\", \"data\": {\"devices\": [{\"deviceId\": \"D001\", \"temperature\": 17, \"measure\": \"C\", \"status\": \"STANDBY\"}]}, \"eventTime\": \"2024-07-23 13:08:11.524757\"}                                                                                                                                                            |\n",
      "|{\"eventId\": \"d6072ebc-ca69-4ed7-9b09-9670766f68e8\", \"eventOffset\": 10013, \"eventPublisher\": \"device\", \"customerId\": \"CI00108\", \"data\": {\"devices\": [{\"deviceId\": \"D003\", \"temperature\": 14, \"measure\": \"C\", \"status\": \"SUCCESS\"}]}, \"eventTime\": \"2024-07-23 13:08:13.656152\"}                                                                                                                                                            |\n",
      "|{\"eventId\": \"5be3694e-df51-4808-8b58-6f5b81227585\", \"eventOffset\": 10014, \"eventPublisher\": \"device\", \"customerId\": \"CI00104\", \"data\": {\"devices\": [{\"deviceId\": \"D001\", \"temperature\": 23, \"measure\": \"C\", \"status\": null}]}, \"eventTime\": \"2024-07-23 13:08:18.790828\"}                                                                                                                                                                 |\n",
      "|{\"eventId\": \"d015ce3d-6737-4130-8540-48e31940da4c\", \"eventOffset\": 10015, \"eventPublisher\": \"device\", \"customerId\": \"CI00116\", \"data\": {\"devices\": [{\"deviceId\": \"D001\", \"temperature\": 30, \"measure\": \"C\", \"status\": \"STANDBY\"}, {\"deviceId\": \"D005\", \"temperature\": 15, \"measure\": \"C\", \"status\": \"ERROR\"}]}, \"eventTime\": \"2024-07-23 13:08:23.925962\"}                                                                                |\n",
      "|{\"eventId\": \"5ed09318-23c8-48fc-8de6-b4037f7bca86\", \"eventOffset\": 10016, \"eventPublisher\": \"device\", \"customerId\": \"CI00118\", \"data\": {\"devices\": [{\"deviceId\": \"D004\", \"temperature\": 21, \"measure\": \"C\", \"status\": null}, {\"deviceId\": \"D004\", \"temperature\": 10, \"measure\": \"C\", \"status\": \"STANDBY\"}]}, \"eventTime\": \"2024-07-23 13:08:25.051022\"}                                                                                   |\n",
      "|{\"eventId\": \"6329791c-b7de-46da-a042-c5c06ad848f3\", \"eventOffset\": 10017, \"eventPublisher\": \"device\", \"customerId\": \"CI00107\", \"data\": {\"devices\": []}, \"eventTime\": \"2024-07-23 13:08:29.176958\"}                                                                                                                                                                                                                                        |\n",
      "|{\"eventId\": \"69305bca-8816-40c6-84de-63312741fdfa\", \"eventOffset\": 10018, \"eventPublisher\": \"device\", \"customerId\": \"CI00113\", \"data\": {\"devices\": [{\"deviceId\": \"D004\", \"temperature\": 7, \"measure\": \"C\", \"status\": \"ERROR\"}]}, \"eventTime\": \"2024-07-23 13:08:29.302610\"}                                                                                                                                                               |\n",
      "|{\"eventId\": \"0b1b77f5-d00b-45b7-9f71-3011ed05c401\", \"eventOffset\": 10019, \"eventPublisher\": \"device\", \"customerId\": \"CI00114\", \"data\": {\"devices\": [{\"deviceId\": \"D002\", \"temperature\": 26, \"measure\": \"C\", \"status\": \"ERROR\"}]}, \"eventTime\": \"2024-07-23 13:08:30.430538\"}                                                                                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.select( F.col(\"value\").cast(\"string\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc34254e-328b-47e7-b120-5bed0402b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking one value as string to get the schema:\n",
    "json_string = \"\"\"{\"eventId\": \"e3cb26d3-41b2-49a2-84f3-0156ed8d7502\", \"eventOffset\": 10001, \"eventPublisher\": \"device\", \"customerId\": \"CI00103\", \"data\": {\"devices\": [{\"deviceId\": \"D001\", \"temperature\": 15, \"measure\": \"C\", \"status\": \"ERROR\"}, {\"deviceId\": \"D002\", \"temperature\": 16, \"measure\": \"C\", \"status\": \"SUCCESS\"}]}, \"eventTime\": \"2023-01-05 11:13:53.643364\"}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f40fa117-66ae-4870-ba96-b2339033423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|schema                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|STRUCT<customerId: STRING, data: STRUCT<devices: ARRAY<STRUCT<deviceId: STRING, measure: STRING, status: STRING, temperature: BIGINT>>>, eventId: STRING, eventOffset: BIGINT, eventPublisher: STRING, eventTime: STRING>|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    kafka_df\n",
    "    .withColumn( \"schema\", F.schema_of_json( json_string ) )\n",
    "    .select(\"schema\")\n",
    "    .show(1, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae8f286-4d46-4482-9d33-06c400bd83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = \"STRUCT<customerId: STRING, data: STRUCT<devices: ARRAY<STRUCT<deviceId: STRING, measure: STRING, status: STRING, temperature: BIGINT>>>, eventId: STRING, eventOffset: BIGINT, eventPublisher: STRING, eventTime: STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1aa7c6-05c4-4b5f-b7f5-3fa61540fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json_df = (\n",
    "    kafka_df\n",
    "    .withColumn( \"value\", F.col(\"value\").cast(\"string\") )\n",
    "    .withColumn( \"json_value\", F.from_json( F.col(\"value\"), json_schema ) )\n",
    "    # .drop(\"value\")\n",
    "    .select( \"json_value.*\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7790eaa-333a-4f35-8d17-415d2ee199a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|customerId|data                                                                      |eventId                             |eventOffset|eventPublisher|eventTime                 |\n",
      "+----------+--------------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "|CI00102   |{[{null, C, ERROR, 14}, {D002, C, STANDBY, 15}, {D001, C, STANDBY, 10}]}  |d1c50fc5-d6ef-42ee-a94b-c68f0c1b22c3|10000      |device        |2024-07-23 13:07:40.751972|\n",
      "|CI00105   |{[{D003, C, SUCCESS, 9}, {D002, C, SUCCESS, 12}]}                         |728a1974-43a7-47c6-a67e-76e7d578a798|10001      |device        |2024-07-23 13:07:42.907318|\n",
      "|CI00101   |{[{null, C, STANDBY, 7}, {D004, C, null, 15}]}                            |902b9b46-b665-4f5f-ab74-170cc593373e|10002      |device        |2024-07-23 13:07:43.227707|\n",
      "|CI00108   |{[]}                                                                      |145328aa-d8cb-42c9-9941-49ea2f3ab50a|10003      |device        |2024-07-23 13:07:45.358356|\n",
      "|CI00109   |{[]}                                                                      |f3082e6b-78f7-465f-a6c6-904858101494|10004      |device        |2024-07-23 13:07:45.480498|\n",
      "|CI00117   |{[]}                                                                      |798e55fa-c9d9-49b0-a33a-296ea07afa24|10005      |device        |2024-07-23 13:07:48.612853|\n",
      "|CI00116   |{[{D003, C, STANDBY, 13}, {D005, C, STANDBY, 29}, {D003, C, STANDBY, 28}]}|43208442-74cd-42f5-99e7-4c3bb9299a8b|10006      |device        |2024-07-23 13:07:52.749140|\n",
      "|CI00116   |{[{null, C, null, 2}]}                                                    |5bc238e1-add2-43ae-b77c-9a6ee429c274|10007      |device        |2024-07-23 13:07:53.881108|\n",
      "|CI00106   |{[]}                                                                      |e7f906fc-c578-4a3a-b035-cf20abda0a0a|10008      |device        |2024-07-23 13:07:55.003669|\n",
      "|CI00101   |{[{D002, C, STANDBY, 20}, {D003, C, SUCCESS, 22}, {D005, C, null, 25}]}   |543bd562-1e38-486c-b4c1-00f35b1ea7e3|10009      |device        |2024-07-23 13:07:57.133888|\n",
      "|CI00107   |{[{D002, C, SUCCESS, 6}]}                                                 |365688f4-b6bb-443b-b5f6-cca01ebf28af|10010      |device        |2024-07-23 13:08:01.270458|\n",
      "|CI00112   |{[]}                                                                      |8c4d3a2f-5fc5-4e82-848d-4c030219e843|10011      |device        |2024-07-23 13:08:06.399806|\n",
      "|CI00110   |{[{D001, C, STANDBY, 17}]}                                                |22f0aba5-8090-4bb6-a206-3d855c839315|10012      |device        |2024-07-23 13:08:11.524757|\n",
      "|CI00108   |{[{D003, C, SUCCESS, 14}]}                                                |d6072ebc-ca69-4ed7-9b09-9670766f68e8|10013      |device        |2024-07-23 13:08:13.656152|\n",
      "|CI00104   |{[{D001, C, null, 23}]}                                                   |5be3694e-df51-4808-8b58-6f5b81227585|10014      |device        |2024-07-23 13:08:18.790828|\n",
      "|CI00116   |{[{D001, C, STANDBY, 30}, {D005, C, ERROR, 15}]}                          |d015ce3d-6737-4130-8540-48e31940da4c|10015      |device        |2024-07-23 13:08:23.925962|\n",
      "|CI00118   |{[{D004, C, null, 21}, {D004, C, STANDBY, 10}]}                           |5ed09318-23c8-48fc-8de6-b4037f7bca86|10016      |device        |2024-07-23 13:08:25.051022|\n",
      "|CI00107   |{[]}                                                                      |6329791c-b7de-46da-a042-c5c06ad848f3|10017      |device        |2024-07-23 13:08:29.176958|\n",
      "|CI00113   |{[{D004, C, ERROR, 7}]}                                                   |69305bca-8816-40c6-84de-63312741fdfa|10018      |device        |2024-07-23 13:08:29.302610|\n",
      "|CI00114   |{[{D002, C, ERROR, 26}]}                                                  |0b1b77f5-d00b-45b7-9f71-3011ed05c401|10019      |device        |2024-07-23 13:08:30.430538|\n",
      "+----------+--------------------------------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "441eaa66-b612-4ab6-9b69-39c7f5120fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9525d0f8-7d6d-4ab5-b995-55b1f4b26df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_json_df = ( \n",
    "    parsed_json_df.withColumn( \"data\", F.explode( \"data.devices\" ) )\n",
    "    .withColumn( \"deviceId\", F.col(\"data.deviceId\") )\n",
    "    .withColumn( \"temperature\", F.col(\"data.temperature\") )\n",
    "    .withColumn( \"measure\", F.col(\"data.measure\") )\n",
    "    .withColumn( \"status\", F.col(\"data.status\") )\n",
    "    # .drop(\"data\")\n",
    "    .select( \"eventId\", \"eventOffset\", \"eventPublisher\", \"customerId\", \"deviceId\", \"temperature\", \"measure\", \"status\", \"eventTime\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3d1f5e-d9e3-4dd8-92e2-0c6b977f5af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "|eventId                             |eventOffset|eventPublisher|customerId|deviceId|temperature|measure|status |eventTime                 |\n",
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "|d1c50fc5-d6ef-42ee-a94b-c68f0c1b22c3|10000      |device        |CI00102   |null    |14         |C      |ERROR  |2024-07-23 13:07:40.751972|\n",
      "|d1c50fc5-d6ef-42ee-a94b-c68f0c1b22c3|10000      |device        |CI00102   |D002    |15         |C      |STANDBY|2024-07-23 13:07:40.751972|\n",
      "|d1c50fc5-d6ef-42ee-a94b-c68f0c1b22c3|10000      |device        |CI00102   |D001    |10         |C      |STANDBY|2024-07-23 13:07:40.751972|\n",
      "|728a1974-43a7-47c6-a67e-76e7d578a798|10001      |device        |CI00105   |D003    |9          |C      |SUCCESS|2024-07-23 13:07:42.907318|\n",
      "|728a1974-43a7-47c6-a67e-76e7d578a798|10001      |device        |CI00105   |D002    |12         |C      |SUCCESS|2024-07-23 13:07:42.907318|\n",
      "|902b9b46-b665-4f5f-ab74-170cc593373e|10002      |device        |CI00101   |null    |7          |C      |STANDBY|2024-07-23 13:07:43.227707|\n",
      "|902b9b46-b665-4f5f-ab74-170cc593373e|10002      |device        |CI00101   |D004    |15         |C      |null   |2024-07-23 13:07:43.227707|\n",
      "|43208442-74cd-42f5-99e7-4c3bb9299a8b|10006      |device        |CI00116   |D003    |13         |C      |STANDBY|2024-07-23 13:07:52.749140|\n",
      "|43208442-74cd-42f5-99e7-4c3bb9299a8b|10006      |device        |CI00116   |D005    |29         |C      |STANDBY|2024-07-23 13:07:52.749140|\n",
      "|43208442-74cd-42f5-99e7-4c3bb9299a8b|10006      |device        |CI00116   |D003    |28         |C      |STANDBY|2024-07-23 13:07:52.749140|\n",
      "|5bc238e1-add2-43ae-b77c-9a6ee429c274|10007      |device        |CI00116   |null    |2          |C      |null   |2024-07-23 13:07:53.881108|\n",
      "|543bd562-1e38-486c-b4c1-00f35b1ea7e3|10009      |device        |CI00101   |D002    |20         |C      |STANDBY|2024-07-23 13:07:57.133888|\n",
      "|543bd562-1e38-486c-b4c1-00f35b1ea7e3|10009      |device        |CI00101   |D003    |22         |C      |SUCCESS|2024-07-23 13:07:57.133888|\n",
      "|543bd562-1e38-486c-b4c1-00f35b1ea7e3|10009      |device        |CI00101   |D005    |25         |C      |null   |2024-07-23 13:07:57.133888|\n",
      "|365688f4-b6bb-443b-b5f6-cca01ebf28af|10010      |device        |CI00107   |D002    |6          |C      |SUCCESS|2024-07-23 13:08:01.270458|\n",
      "|22f0aba5-8090-4bb6-a206-3d855c839315|10012      |device        |CI00110   |D001    |17         |C      |STANDBY|2024-07-23 13:08:11.524757|\n",
      "|d6072ebc-ca69-4ed7-9b09-9670766f68e8|10013      |device        |CI00108   |D003    |14         |C      |SUCCESS|2024-07-23 13:08:13.656152|\n",
      "|5be3694e-df51-4808-8b58-6f5b81227585|10014      |device        |CI00104   |D001    |23         |C      |null   |2024-07-23 13:08:18.790828|\n",
      "|d015ce3d-6737-4130-8540-48e31940da4c|10015      |device        |CI00116   |D001    |30         |C      |STANDBY|2024-07-23 13:08:23.925962|\n",
      "|d015ce3d-6737-4130-8540-48e31940da4c|10015      |device        |CI00116   |D005    |15         |C      |ERROR  |2024-07-23 13:08:23.925962|\n",
      "+------------------------------------+-----------+--------------+----------+--------+-----------+-------+-------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39cd0333-72b1-4392-9390-7114a744c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ac401-a9f1-4724-a42e-e816985b3d85",
   "metadata": {},
   "source": [
    "## Streaming Code for Kafka data processing:\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc3baade-f2f2-4a84-a8ae-5d73229011cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base checkpoint directory './checkpoints/005-kafka_streaming_data_read' already exists.\n",
      "The checkpoint directory to be utilised for current execution of spark streaming job: './checkpoints/005-kafka_streaming_data_read/3'\n"
     ]
    }
   ],
   "source": [
    "BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK = f\"./checkpoints/{notebook_name}\"\n",
    "\n",
    "def update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK ):\n",
    "    \"\"\"\n",
    "    # For each run of the notebook streaming job, the directory to be used will be in the pattern \"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\", \"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/2\", ... and so on.\n",
    "    # Thus, for each run, the below code snippet checks the highest existing numbered folder inside the \"BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK\". If that highest numbered folder is empty(does not contain any checkpoint files), that will be set as checkpoint directory for current run, else, a new folder with ( 1+ highest numbered folder ) will be created, and will be used as the checkpoint directory.\n",
    "    \"\"\"\n",
    "    # This code snippet is the base directory for saving checkpoints of streaming jobs exists. If it does not exists, creates the base directory.    \n",
    "    try:\n",
    "        os.listdir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )\n",
    "        print(f\"Base checkpoint directory '{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}' already exists.\")\n",
    "    except FileNotFoundError:\n",
    "        os.mkdir(BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK)\n",
    "        print(f\"Base checkpoint directory, did not exist previously, so, it has been created with the relative path: '{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}'\")\n",
    "    \n",
    "    # get list of all folders inside the 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK'\n",
    "    past_checkpoint_dirs_of_this_notebook = os.listdir(BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK)\n",
    "    \n",
    "    \n",
    "    if len(past_checkpoint_dirs_of_this_notebook) == 0:\n",
    "        # If 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK' is empty, then create a folder named '1' inside it, and select this '1' named folder path as 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "        os.mkdir(f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\")\n",
    "        CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/1\"\n",
    "    else:\n",
    "        # If 'BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK' is not empty, then we need to get the highest numbered folder.\n",
    "    \n",
    "        # Converting folder number strings to integers, to currently get the folder name having highest number\n",
    "        checkpoint_folder_names_converted_to_integers = [int(i) for i in past_checkpoint_dirs_of_this_notebook]\n",
    "        highest_existing_folder_number = max(checkpoint_folder_names_converted_to_integers)  # max() gives the largest integer value in this list of folder names\n",
    "    \n",
    "        # checking if the highest numbered folder is empty, or it contains checkpoint related files inside it.\n",
    "        # This check is important as it might happen that a new folder is created but not used for storing any checkpoint data. Then, without creating another new folder with higher number, we will use that existing highest numbered empty folder.\n",
    "        if len( os.listdir( f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{highest_existing_folder_number}\" ) ) == 0:\n",
    "            # If existing highest numbered folder is empty, then its path will be set to 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "            CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{highest_existing_folder_number}\"\n",
    "        else:\n",
    "            # If existing highest numbered folder is not empty(contains checkpoint files), then a new folder with number 1 higher than highest number will be created, and its path will be set to 'CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN'\n",
    "            new_folder_number = highest_existing_folder_number + 1\n",
    "            os.mkdir(f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{new_folder_number}\")\n",
    "            CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = f\"{BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK}/{new_folder_number}\"\n",
    "\n",
    "    return CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN\n",
    "    \n",
    "print(f\"The checkpoint directory to be utilised for current execution of spark streaming job: '{update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305bb9d2-0861-42fa-9d9e-16462670cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base checkpoint directory './checkpoints/005-kafka_streaming_data_read' already exists.\n",
      "./checkpoints/005-kafka_streaming_data_read/3\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = update_checkpoint_dir( BASE_CHECKPOINT_DIR_FOR_CURRENT_NOTEBOOK )\n",
    "# CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN = \"./checkpoints/005-kafka_streaming_data_read/2\"\n",
    "print(CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6167c7-9e13-49fe-9c63-74b2dce7ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c704a6-e452-4b4f-9c9c-d227afeef2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = \"STRUCT<customerId: STRING, data: STRUCT<devices: ARRAY<STRUCT<deviceId: STRING, measure: STRING, status: STRING, temperature: BIGINT>>>, eventId: STRING, eventOffset: BIGINT, eventPublisher: STRING, eventTime: STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a693fc-7ad9-4821-b486-134113c6c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_streaming_json_df = (\n",
    "    kafka_streaming_df\n",
    "    .withColumn( \"value\", F.col(\"value\").cast(\"string\") )\n",
    "    .withColumn( \"json_value\", F.from_json( F.col(\"value\"), json_schema ) )\n",
    "    # .drop(\"value\")\n",
    "    .select( \"json_value.*\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c837fd96-cff0-4901-b084-d3a784d38faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_streaming_json_df = ( \n",
    "    parsed_streaming_json_df.withColumn( \"data\", F.explode( \"data.devices\" ) )\n",
    "    .withColumn( \"deviceId\", F.col(\"data.deviceId\") )\n",
    "    .withColumn( \"temperature\", F.col(\"data.temperature\") )\n",
    "    .withColumn( \"measure\", F.col(\"data.measure\") )\n",
    "    .withColumn( \"status\", F.col(\"data.status\") )\n",
    "    # .drop(\"data\")\n",
    "    .select( \"eventId\", \"eventOffset\", \"eventPublisher\", \"customerId\", \"deviceId\", \"temperature\", \"measure\", \"status\", \"eventTime\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6dfeeed2-edf4-490c-ba81-00fef77f6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_query = (\n",
    "    flat_streaming_json_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32960f84-462a-4b74-bc14-355823873cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c364c-fdda-4c79-92e3-4a6376eeadb7",
   "metadata": {},
   "source": [
    "#### Applying Triggers:-\n",
    "------------------------------\n",
    "- `.trigger(once=True)`: Used to consume all the available data for a spark streaming job, and the job behaves as a Batch Job. It is used when we want to process all the available data using our streaming job, and shut down the pipeline after the processing is complete(the job will not continue to run indefinitely).\n",
    "  \n",
    "- `.trigger(availableNow=True)`: This trigger is available in newer versions of spark. Works same as `.trigger(once=True)`\n",
    "  \n",
    "- `.trigger(processingTime=\"15 seconds\")`: It makes the pipeline trigger in microBatch format, and we specificy the time interval at which the pipeline gets triggered. Suppose, we pass here `\"15 seconds\"`, so the pipeline will process data incrementally, by triggering itself once every 10 seconds. **Using this trigger mode, we mainly define at what time interval does incoming incremental data get processed and written to sink**.\n",
    "  \n",
    "- `.trigger(continuos=\"15 seconds\")`: Available in spark 3.3.0+ versions, as experimental option. Does not support all sources and sinks.\n",
    "    - The `continuos` trigger mode does not process data in microBatch fashion. It processes data continuosly with very low latency(latency here means processing interval, which is in milliseconds). The time passed to this trigger mode is actually the time-interval after which it writes the data offsets, metadata, etc., into the checkpoint directory.\n",
    "    - For e.g.: `.trigger(continuos=\"15 seconds\")` means the data will be **processed continuosly after some milliseconds interval**, but the checkpoint directory data will be written every 15 seconds, thus **checkpoints will be created once every 15 seconds**.\n",
    "    - Does support some data sinks, such as `memory`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dc6878a-dce4-42ed-a0b5-317946cbef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    flat_streaming_json_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"Kafka_writeStream_trigger_once\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    # .trigger(once=True)\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a79acd-1a91-4861-a652-c6923635cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    flat_streaming_json_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"Kafka_writeStream_trigger_procesingTime_15_seconds\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    .trigger(processingTime=\"15 seconds\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed86a2aa-4823-4b39-a978-549ed8b2e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"D:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\Debanjan Sarkar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m (\n\u001b[0;32m      2\u001b[0m     \u001b[43mparsed_streaming_json_df\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKafka_writeStream_trigger_continuos_15_seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_DIRECTORY_FOR_CURRENT_RUN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(\n",
    "    parsed_streaming_json_df\n",
    "    .writeStream\n",
    "    .queryName(\"Kafka_writeStream_trigger_continuos_15_seconds\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_DIRECTORY_FOR_CURRENT_RUN)\n",
    "    .trigger(continuous=\"10 seconds\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a596f07-ccc1-4e02-91a5-5b1a3a3b9c0d",
   "metadata": {},
   "source": [
    "### Reading data from memory:\n",
    "--------------------------------\n",
    "- When data is written to memory, fr spark streaming, the query name can be used as table name, to view the records, using `\"spark.sql( 'SELECT * FROM <query_name>' )\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86f92c4e-4d0d-41d0-905c-7b78b2015cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "|customerId|                data|             eventId|eventOffset|eventPublisher|           eventTime|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "|   CI00102|{[{null, C, ERROR...|d1c50fc5-d6ef-42e...|      10000|        device|2024-07-23 13:07:...|\n",
      "|   CI00105|{[{D003, C, SUCCE...|728a1974-43a7-47c...|      10001|        device|2024-07-23 13:07:...|\n",
      "|   CI00101|{[{null, C, STAND...|902b9b46-b665-4f5...|      10002|        device|2024-07-23 13:07:...|\n",
      "|   CI00108|                {[]}|145328aa-d8cb-42c...|      10003|        device|2024-07-23 13:07:...|\n",
      "|   CI00109|                {[]}|f3082e6b-78f7-465...|      10004|        device|2024-07-23 13:07:...|\n",
      "|   CI00117|                {[]}|798e55fa-c9d9-49b...|      10005|        device|2024-07-23 13:07:...|\n",
      "|   CI00116|{[{D003, C, STAND...|43208442-74cd-42f...|      10006|        device|2024-07-23 13:07:...|\n",
      "|   CI00116|{[{null, C, null,...|5bc238e1-add2-43a...|      10007|        device|2024-07-23 13:07:...|\n",
      "|   CI00106|                {[]}|e7f906fc-c578-4a3...|      10008|        device|2024-07-23 13:07:...|\n",
      "|   CI00101|{[{D002, C, STAND...|543bd562-1e38-486...|      10009|        device|2024-07-23 13:07:...|\n",
      "|   CI00107|{[{D002, C, SUCCE...|365688f4-b6bb-443...|      10010|        device|2024-07-23 13:08:...|\n",
      "|   CI00112|                {[]}|8c4d3a2f-5fc5-4e8...|      10011|        device|2024-07-23 13:08:...|\n",
      "|   CI00110|{[{D001, C, STAND...|22f0aba5-8090-4bb...|      10012|        device|2024-07-23 13:08:...|\n",
      "|   CI00108|{[{D003, C, SUCCE...|d6072ebc-ca69-4ed...|      10013|        device|2024-07-23 13:08:...|\n",
      "|   CI00104|{[{D001, C, null,...|5be3694e-df51-480...|      10014|        device|2024-07-23 13:08:...|\n",
      "|   CI00116|{[{D001, C, STAND...|d015ce3d-6737-413...|      10015|        device|2024-07-23 13:08:...|\n",
      "|   CI00118|{[{D004, C, null,...|5ed09318-23c8-48f...|      10016|        device|2024-07-23 13:08:...|\n",
      "|   CI00107|                {[]}|6329791c-b7de-46d...|      10017|        device|2024-07-23 13:08:...|\n",
      "|   CI00113|{[{D004, C, ERROR...|69305bca-8816-40c...|      10018|        device|2024-07-23 13:08:...|\n",
      "|   CI00114|{[{D002, C, ERROR...|0b1b77f5-d00b-45b...|      10019|        device|2024-07-23 13:08:...|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If the streaming job is stopped, then from the memory, it can be viewed as a spark sql table\n",
    "# Kill the running job from spark UI, and then run this code\n",
    "spark.sql(\"SELECT * FROM Kafka_writeStream_trigger_continuos_15_seconds\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbaa8c-25e7-4737-bc8c-d2080bc8301d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
