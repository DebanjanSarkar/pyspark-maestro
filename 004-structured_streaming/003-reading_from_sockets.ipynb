{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b7b934-85a5-4427-b776-9c2e74c7b3db",
   "metadata": {},
   "source": [
    "<center><h1>Reading Streaming Data from Sockets</h1></center>\n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6ecaa2-dc1e-4678-b9e0-b9de0608f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d59b86-2057-4f7b-806e-1e74dd64aa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>003 - Reading streaming data from sockets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d86e0ae560>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"003 - Reading streaming data from sockets\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712344f1-db02-487a-ac92-0996f9c360cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>003 - Reading streaming data from sockets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=003 - Reading streaming data from sockets>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076e72ab-8b70-47fb-a195-71a03f627456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e59634b-abc9-4cbb-8ae8-fd597dca6dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The default value in the beginning will be 200, thus, each stage will be broken down into \n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88326e-3448-451d-8167-088630fcf94f",
   "metadata": {},
   "source": [
    "- By default, the value of `\"spark.sql.shuffle.partitions\"` is `200`, thus each non-initial stage(any stage after the initial stage, that is stage created after at least one shuffle operation) will be divided into 200 tasks each, and the subsequent stages will be divided into that many tasks as many partition will exist after shuffling (which is again number set in `\"spark.sql.shuffle.partitions\"`). Below image shows, the number of tasks execution for non-initial stage(s) of this streaming job (see the last value shows `(195 + 5)`, means `200` task executed for each stage after shuffling happens once). `Stage 15` is the stage number shown in below image because in the same kernel, same job is run multiple times: <br><br>\n",
    "    <img src=\"./images/001-each_stage_divides_into_200_tasks_by_default.jpg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2166f20e-6fdc-4479-936e-8b4892ad000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce1df85-26b9-466d-9d86-177c6f69f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the value of shuffle partitions has been changed\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399d576-3597-43cf-b8db-e32caa8a1251",
   "metadata": {},
   "source": [
    "- Now, the value of `spark.sql.shuffle.partitions` has been set to number in `sc.defaultParallelism`, which is `8` in my system's case. So, now, number of tasks created for each non-initial stage(stages formed after at least one shuffle operation), will be `8`, as shown in the image below:\n",
    "    <img src=\"./images/002-each_stage_divides_into_spark-sql-shuffle-partition_num_of_tasks.jpg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f963fbde-a4a7-470f-a76c-a658954a2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f664da3-298f-4f43-b975-81d333b46ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#  This directory path will be utilised as the checkpoint directory for all the streaming jobs in this notebook\n",
    "CHECKPOINT_DIRECTORY = \"./checkpoints/003-reading_from_sockets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbcc19bd-641a-4955-8678-02a5f54fdcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents:  []\n",
      "Checkpoint directory is empty. Streaming jobs of this notebook can be started afresh successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check if the checkpoint folder used as checkpoint location for the below streaming jobs is empty\n",
    "\n",
    "checkpoint_dir_contents = os.listdir(CHECKPOINT_DIRECTORY)\n",
    "print( \"Contents: \", checkpoint_dir_contents )\n",
    "\n",
    "if len(checkpoint_dir_contents) == 0:\n",
    "    print(\"Checkpoint directory is empty. Streaming jobs of this notebook can be started afresh successfully!\")\n",
    "else:\n",
    "    print(\"Please delete the checkpoint directory set for this notebook, before running the streaming jobs below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd6e4ec-9122-4b27-9584-dd89c16d2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe will be an unbounded dataframe, where new rows as received from socket, will keep getting appended, and the dataframe will continue to grow in size indefinitely (if no duration is specifid as in this case):\n",
    "\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", \"9999\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2834a533-9d74-4e86-9201-fbafe058096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f779f526-c5a0-4f31-891c-e4bb8fa914c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df = (\n",
    "    streaming_df.withColumn(\"value\", F.regexp_replace( F.col(\"value\"), r\"([.,])\", \"\" ) )\n",
    "    .withColumn(\"value\", F.lower(F.col(\"value\")) )\n",
    "    .withColumn(\"words\", F.split(F.col(\"value\"), \" \") )\n",
    "    .drop(\"value\")\n",
    "    .withColumn(\"words\", F.explode(F.col(\"words\")) )\n",
    "    .groupBy( \"words\" )\n",
    "    .agg( F.count(\"words\").alias(\"num_of_occurences\") )\n",
    "    # .orderBy( F.col(\"num_of_occurences\").desc() )         # sorting could only be applied if writeStream outputMode is \"complete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519342a0-db31-4876-8abc-2223b746e628",
   "metadata": {},
   "source": [
    "- `awaitTermination()` function makes sure that when the structured streaming query is running on the executors and the driver is idle, even in that time the driver will still remain connected to the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080f6933-ccf5-4699-9136-68e49276ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to view the microbatch received into your console.\n",
    "\n",
    "# complete outputMode\n",
    "# -------------------------\n",
    "# 1. Sorting is only applicable for aggregated streaming dataset in \"complete\" mode\n",
    "\n",
    "\"\"\"\n",
    "stream_write_query = (\n",
    "    word_count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .outputMode(\"update\")\n",
    "    # .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/003-reading_from_sockets\")\n",
    "    .start()\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# update outputMode\n",
    "# -------------------------\n",
    "# 1. update output mode results in only those values existing in the output, that are common in past result and new microbatch, and the new records in the microbatch. For this aggregation, words which are already in the existing output and new microbatch will have their count updated(increased), and the words which are new in the latest microbatch will be written in the result. The words which were in the output but not in new microbatch, their count/record will be dropped.\n",
    "\n",
    "# 2. \"console\", \"delta lake\", \"rdbms system\" - these formats support \"update\" outputMode.\n",
    "#    \"file\": writing to files does not support \"update\" outputMode.\n",
    "\"\"\"\n",
    "stream_write_query = (\n",
    "    word_count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/003-reading_from_sockets\")\n",
    "    .start()\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# append outputMode\n",
    "# -------------------------\n",
    "# 1. In \"append\" outputMode, we cannot update our data, old data is logged. Thus, \"append\" iis very popular with logs, when maintaining and writing logs in a persistent storage, using spark streeaming, as logs are not updated, they are only appended at the end.\n",
    "\n",
    "# 2. \"append\" output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark. So, the below query will fail because no watermark has been added.\n",
    "\n",
    "\"\"\"\n",
    "stream_write_query = (\n",
    "    word_count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/003-reading_from_sockets\")\n",
    "    .start()\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 3. However, we can use it to see the streaming_df, directly as output.\n",
    "\n",
    "# The below code will simple show each microbtch, as it is obtained, and received by spark streaming.\n",
    "# \"\"\"\n",
    "stream_write_query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/003-reading_from_sockets\")\n",
    "    .start()\n",
    ")\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e5b39-26dd-40cd-9d21-4c261ef95492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# awaitTermination() will make the driver remain connected to executor even when the driver is idle.\n",
    "# Addition of triggerOnce() and trigger(availableNow=True) may change its behavior.\n",
    "stream_write_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddfdc3-b6b1-4cf5-9230-f0fec9b0d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717792e-41a9-4f9f-a71f-0270d0bb49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_write_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
