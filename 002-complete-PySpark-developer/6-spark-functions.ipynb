{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7864bbb7-d665-4eef-a858-b4259eeae514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af2c544-5562-4718-bf6c-cf7295c117d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSession object methods\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473fef4b-392c-47c0-8148-8e5121fa94c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSession object methods</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22e700f2490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559461f7-2cc1-4812-afaa-520cce1c48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1696091739737'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.app.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c6158f-3118-4e9d-85fa-89eefe3cbf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'client'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.submit.deployMode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8859331e-6fae-4112-86ef-1b1eafbb5e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SparkSession object methods'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.app.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddae8487-8075-4fe5-8216-9d5852ba8927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.isModifiable(\"spark.scheduler.mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01bee494-c0d3-4e39-a91a-9495003885dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad82c2-ba11-46a8-9fc8-2c112dc6935b",
   "metadata": {},
   "source": [
    "# 1. createDataFrame()\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a02ae6-a4d9-4278-b494-e6a8791bde01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Union[float, NoneType] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81d4f9-e65c-409f-8559-e2b5b47651f3",
   "metadata": {},
   "source": [
    "## Creating DataFrame from List of dictionaries:-\n",
    "--------------------------------------------\n",
    "Dictionaries contain keys and values, and each key becomes the column name, value becomes the value of the column, and datatypes are also automatically inferred from python datatypes. <b> So, passing <i>'schema'</i> explicitly is not necessary.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b924836c-8abf-4dd4-a5b4-90df5431799e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = [\n",
    "    {\n",
    "    \"fname\": \"Debanjan\",\n",
    "    \"lname\": \"Sarkar\",\n",
    "    \"age\": 23,\n",
    "    \"degree\": \"btech\",\n",
    "    \"stream\": \"ECE\",\n",
    "    \"dgpa\": 8.84\n",
    "    },\n",
    "    {\n",
    "    \"fname\": \"Subhash Chandra\",\n",
    "    \"lname\": \"Bose\",\n",
    "    \"age\": 120,\n",
    "    \"degree\": \"llb\",\n",
    "    \"stream\": \"Law\",\n",
    "    \"dgpa\": 10.00\n",
    "    }\n",
    "]\n",
    "\n",
    "type(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79fb3bb-5076-44ff-bd0f-77ed76390a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame( data = details )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f4402cd-7a29-4f5a-bf17-8234fa12b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---------------+------+------+\n",
      "|age|degree|dgpa|          fname| lname|stream|\n",
      "+---+------+----+---------------+------+------+\n",
      "| 23| btech|8.84|       Debanjan|Sarkar|   ECE|\n",
      "|120|   llb|10.0|Subhash Chandra|  Bose|   Law|\n",
      "+---+------+----+---------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21cebc51-2636-4141-a8f1-3144cacef225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- degree: string (nullable = true)\n",
      " |-- dgpa: double (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- stream: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a7c05f6-3c08-488e-9eb9-7c88009efc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'),\n",
       " ('degree', 'string'),\n",
       " ('dgpa', 'double'),\n",
       " ('fname', 'string'),\n",
       " ('lname', 'string'),\n",
       " ('stream', 'string')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "177aec34-67d3-427d-8b2b-3b16cc232cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', LongType(), True), StructField('degree', StringType(), True), StructField('dgpa', DoubleType(), True), StructField('fname', StringType(), True), StructField('lname', StringType(), True), StructField('stream', StringType(), True)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7ce12-08df-46a8-bb0a-2c956c117021",
   "metadata": {},
   "source": [
    "## Creating DataFrame from List of tuples:-\n",
    "--------------------------------------------------\n",
    "Datatypes are automatically inferred, and we just need to pass the column names as a list of strings in the <b><i>'schema'</i></b> parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7f48215-7587-4bcd-9301-eb3866ef18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    ('Debanjan', 'Sarkar', 23, 'Male'),\n",
    "    ('Kadombini', 'Devi', 165, 'Female')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84a62c82-2c7f-47a9-b0a8-d9efc4cb2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = spark.createDataFrame( data=data_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "705e3da2-61e3-4963-9199-edec4ed12832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|       _1|    _2| _3|    _4|\n",
      "+---------+------+---+------+\n",
      "| Debanjan|Sarkar| 23|  Male|\n",
      "|Kadombini|  Devi|165|Female|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75c962d8-9e99-4adf-854e-552ed887f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_schema = [\"fname\", \"lname\", \"age\", \"gender\"]\n",
    "\n",
    "df_list2 = spark.createDataFrame( data = data_list, schema = list_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "534229b5-a64a-4935-b0a1-803ecef27d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|    fname| lname|age|gender|\n",
      "+---------+------+---+------+\n",
      "| Debanjan|Sarkar| 23|  Male|\n",
      "|Kadombini|  Devi|165|Female|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55d52f-3d5c-43a3-8b5b-40e105dda143",
   "metadata": {},
   "source": [
    "## Creating DataFrame from RDD:-\n",
    "--------------------------------\n",
    "RDD does not recognises datatypes or schema on its own, and also do not have column names contained.\n",
    "Thus, in the <b><i>'schema'</i></b> parameter, the schema must be of type <b><i>'StructType()'</i></b>, or must be a <b>string, containing column names and datatypes, separated by comma.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c31a81-3fdb-49ca-ab24-141511503fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  '2',\n",
       "  'Quest Q64 10 FT. x 10 FT. Slant Leg Instant U',\n",
       "  '59.98',\n",
       "  'http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy'),\n",
       " ('2',\n",
       "  '2',\n",
       "  \"Under Armour Men's Highlight MC Football Clea\",\n",
       "  '129.99',\n",
       "  'http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat'),\n",
       " ('3',\n",
       "  '2',\n",
       "  \"Under Armour Men's Renegade D Mid Football Cl\",\n",
       "  '89.99',\n",
       "  'http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat'),\n",
       " ('4',\n",
       "  '2',\n",
       "  \"Under Armour Men's Renegade D Mid Football Cl\",\n",
       "  '89.99',\n",
       "  'http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat'),\n",
       " ('5',\n",
       "  '2',\n",
       "  'Riddell Youth Revolution Speed Custom Footbal',\n",
       "  '199.99',\n",
       "  'http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet'),\n",
       " ('6',\n",
       "  '2',\n",
       "  \"Jordan Men's VI Retro TD Football Cleat\",\n",
       "  '134.99',\n",
       "  'http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat'),\n",
       " ('7',\n",
       "  '2',\n",
       "  'Schutt Youth Recruit Hybrid Custom Football H',\n",
       "  '99.99',\n",
       "  'http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014'),\n",
       " ('8',\n",
       "  '2',\n",
       "  \"Nike Men's Vapor Carbon Elite TD Football Cle\",\n",
       "  '129.99',\n",
       "  'http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat'),\n",
       " ('9',\n",
       "  '2',\n",
       "  'Nike Adult Vapor Jet 3.0 Receiver Gloves',\n",
       "  '50.0',\n",
       "  'http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves'),\n",
       " ('10',\n",
       "  '2',\n",
       "  \"Under Armour Men's Highlight MC Football Clea\",\n",
       "  '129.99',\n",
       "  'http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('data/products.csv').map( lambda x: x.split(\",\") ).map( lambda x: (x[0], x[1],  x[2], x[4], x[5]) )\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7b698b-c25c-408b-8d9d-6783fa0655a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d1e12e-d6bb-4fdb-87a7-409288b8204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_small = rdd.sample(False, 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078a80e8-725d-43a1-979e-b3325a6a6568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_small.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afda06fe-a77b-44fd-94b7-37dccdb03a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('305',\n",
       "  '38',\n",
       "  'Mio ALPHA Heart Rate Monitor/Sport Watch',\n",
       "  '199.0',\n",
       "  'http://images.acmesports.sports/Mio+ALPHA+Heart+Rate+Monitor%2FSport+Watch'),\n",
       " ('346',\n",
       "  '16',\n",
       "  'Fitness Gear Pro Half Rack',\n",
       "  '349.99',\n",
       "  'http://images.acmesports.sports/Fitness+Gear+Pro+Half+Rack'),\n",
       " ('382',\n",
       "  '18',\n",
       "  \"PUMA Men's evoPOWER 1 Tricks FG Soccer Cleat\",\n",
       "  '189.99',\n",
       "  'http://images.acmesports.sports/PUMA+Men%27s+evoPOWER+1+Tricks+FG+Soccer+Cleat'),\n",
       " ('463',\n",
       "  '21',\n",
       "  \"Nike Kids' Grade School KD VI Basketball Shoe\",\n",
       "  '99.99',\n",
       "  'http://images.acmesports.sports/Nike+Kids%27+Grade+School+KD+VI+Basketball+Shoe'),\n",
       " ('625',\n",
       "  '29',\n",
       "  \"Nike Men's Kobe IX Elite Low Basketball Shoe\",\n",
       "  '199.99',\n",
       "  'http://images.acmesports.sports/Nike+Men%27s+Kobe+IX+Elite+Low+Basketball+Shoe'),\n",
       " ('1231',\n",
       "  '55',\n",
       "  '\"adidas Original Men\\'s 2014 MLS All-Star Game \"',\n",
       "  '28.0',\n",
       "  'http://images.acmesports.sports/adidas+Original+Men%27s+2014+MLS+All-Star+Game+Homestyle...')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_small.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794b7c2d-4ff3-46ca-94c2-d0012c5aaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_df1 = spark.createDataFrame( data = rdd_small )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fffab55d-65cb-4525-9daf-48a0cd91ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----------------------------------------------+------+--------------------------------------------------+\n",
      "|  _1| _2|                                             _3|    _4|                                                _5|\n",
      "+----+---+-----------------------------------------------+------+--------------------------------------------------+\n",
      "| 224| 11|\"PowerBlock Classic 50 lb Adjustable Dumbbell \"|299.99|http://images.acmesports.sports/PowerBlock+Clas...|\n",
      "| 261| 12|      ASICS Women's GEL-Cumulus 15 Running Shoe| 89.99|http://images.acmesports.sports/ASICS+Women%27s...|\n",
      "| 292| 38|         Garmin Women's Forerunner 10 GPS Watch|129.99|http://images.acmesports.sports/Garmin+Women%27...|\n",
      "| 329| 15|  Under Armour Women's Essential Banded Tank To| 49.99|http://images.acmesports.sports/Under+Armour+Wo...|\n",
      "| 355| 16|      Nike Men's Free Trainer 5.0 Training Shoe| 99.99|http://images.acmesports.sports/Nike+Men%27s+Fr...|\n",
      "| 580| 27|\"adidas Original Men's 2014 MLS All-Star Game \"|  28.0|http://images.acmesports.sports/adidas+Original...|\n",
      "| 595| 27|        TYR Girls' Phoenix Maxfit Back Swimsuit| 75.99|http://images.acmesports.sports/TYR+Girls%27+Ph...|\n",
      "| 756| 34|              TRUE linkswear Lyt Dry Golf Shoes|149.99|http://images.acmesports.sports/TRUE+linkswear+...|\n",
      "| 876| 39|  Top Flite Kids' XLj Driver (Height 45'' and u| 34.99|http://images.acmesports.sports/Top+Flite+Kids%...|\n",
      "|1119| 50|  Majestic Men's 2014 All-Star Game Yadier Moli| 130.0|http://images.acmesports.sports/Majestic+Men%27...|\n",
      "|1128| 50|  Nike Men's Los Angeles Angels Mike Trout 2014|  34.0|http://images.acmesports.sports/Nike+Men%27s+Lo...|\n",
      "|1231| 55|\"adidas Original Men's 2014 MLS All-Star Game \"|  28.0|http://images.acmesports.sports/adidas+Original...|\n",
      "+----+---+-----------------------------------------------+------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df1.show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64978662-6d29-44e7-a481-235d2950c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Define the schema using StructType and StructField\n",
    "# rdd_schema = StructType([\n",
    "#     StructField(\"productId\", IntegerType(), True),\n",
    "#     StructField(\"categoryId\", IntegerType(), True),\n",
    "#     StructField(\"description\", StringType(), True),\n",
    "#     StructField(\"price\", FloatType(), True),\n",
    "#     StructField(\"imageLink\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "rdd_schema = [\"product_id\", \"category_id\", \"description\", \"price\", \"image_link\"]\n",
    "#rdd_schema = ('productId int, categoryId int, description string, price float, imageLink string')\n",
    "\n",
    "rdd_df2 = spark.createDataFrame( data = rdd_small, schema = rdd_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1201d81-2b02-42db-9f94-fe6eb308fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+------+--------------------+\n",
      "|product_id|category_id|         description| price|          image_link|\n",
      "+----------+-----------+--------------------+------+--------------------+\n",
      "|       305|         38|Mio ALPHA Heart R...| 199.0|http://images.acm...|\n",
      "|       346|         16|Fitness Gear Pro ...|349.99|http://images.acm...|\n",
      "|       382|         18|PUMA Men's evoPOW...|189.99|http://images.acm...|\n",
      "|       463|         21|Nike Kids' Grade ...| 99.99|http://images.acm...|\n",
      "|       625|         29|Nike Men's Kobe I...|199.99|http://images.acm...|\n",
      "|      1231|         55|\"adidas Original ...|  28.0|http://images.acm...|\n",
      "+----------+-----------+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4adcc84-fe6f-471b-b9ec-ff06e1fd6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- image_link: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce891d0-95ed-4b78-b7d9-5ad4197b139a",
   "metadata": {},
   "source": [
    "## Creating DataFrame from List of <i>'pyspark.sql.Row'</i> objects :-\n",
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e7c045b-ca60-43a1-a652-fc262e9eb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb6e4e37-a037-4e7d-a9ae-6373643a7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data = [\n",
    "    Row(fname='Debanjan', lname='Sarkar', age=23),\n",
    "    Row(fname='JC', lname='Bose', age=180)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38d71fb7-410a-4fe5-80c5-1b7a5cbaa408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da441090-1a1f-40ed-bf41-5223e85466ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df = spark.createDataFrame( data = row_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3be23084-d848-49d3-adc6-bdbf1715a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+\n",
      "|   fname| lname|age|\n",
      "+--------+------+---+\n",
      "|Debanjan|Sarkar| 23|\n",
      "|      JC|  Bose|180|\n",
      "+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f335481-4a07-4bb1-bf7d-e72c7e4067d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5da55074-9bcc-4a8c-9c15-916138e07757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('fname', StringType(), True), StructField('lname', StringType(), True), StructField('age', LongType(), True)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d070a-b630-4a8c-abdf-ad6b4f460200",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87575f-0fa7-4b40-820c-9fed3770e762",
   "metadata": {},
   "source": [
    "# <i>'pyspark.sql.Row' object:-\n",
    "<hr>\n",
    "<ul>\n",
    "    <li> <i>'Row'</i> object takes any number of keyword arguments, and the data passed as keyword arguments form key-value paired data. </li>\n",
    "    <li> The values inside a <i>'Row'</i> object can be accessed using dot(.) notation. </li>\n",
    "    <li> <i>'Row'</i> object is an iterable, and can be operated by a <b><i>for</i></b> loop to get the values in the object. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc48fea-494a-47a7-a17d-c8e29ca03c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d7a7c3-c4d3-4f58-8a86-a57bf4f504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_obj = Row( fname='Debanjan', lname = 'Sarkar', age=23, degree='btech', stream='ECE', dgpa=8.84 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c411af-1498-4166-b6a6-1546a1dfb93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debanjan\n",
      "Sarkar\n"
     ]
    }
   ],
   "source": [
    "print( row_obj.fname )\n",
    "print( row_obj.lname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14e9f819-7238-4de6-b6e9-14336d28b63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'age' in row_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29f3a495-afae-4e6d-a997-1bcfc3083d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fname' in row_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4b8ca31-b1af-4c0a-8eba-cd0b7ecb979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fullName' in row_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a09db904-8abb-48fb-b449-4cdb937150a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debanjan\n",
      "Sarkar\n",
      "23\n",
      "btech\n",
      "ECE\n",
      "8.84\n"
     ]
    }
   ],
   "source": [
    "for i in row_obj:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088836e4-d0d8-453f-93f9-26ea834e1ce1",
   "metadata": {},
   "source": [
    "# 2. sql()\n",
    "<hr>\n",
    "Return the result set of an sql query, as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4351120d-e57d-4e23-934d-01a5290d7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sql in module pyspark.sql.session:\n",
      "\n",
      "sql(sqlQuery: str, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Returns a :class:`DataFrame` representing the result of the given query.\n",
      "    When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      "    standard formatter.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    sqlQuery : str\n",
      "        SQL query string.\n",
      "    kwargs : dict\n",
      "        Other variables that the user wants to set that can be referenced in the query\n",
      "    \n",
      "        .. versionchanged:: 3.3.0\n",
      "           Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      "           This feature is experimental and unstable.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Executing a SQL query.\n",
      "    \n",
      "    >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      "    +---+\n",
      "    | id|\n",
      "    +---+\n",
      "    |  8|\n",
      "    |  9|\n",
      "    +---+\n",
      "    \n",
      "    Executing a SQL query with variables as Python formatter standard.\n",
      "    \n",
      "    >>> spark.sql(\n",
      "    ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      "    ... ).show()\n",
      "    +---+\n",
      "    | id|\n",
      "    +---+\n",
      "    |  8|\n",
      "    +---+\n",
      "    \n",
      "    >>> mydf = spark.range(10)\n",
      "    >>> spark.sql(\n",
      "    ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      "    ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      "    +---+\n",
      "    | id|\n",
      "    +---+\n",
      "    |  0|\n",
      "    |  1|\n",
      "    |  2|\n",
      "    |  3|\n",
      "    +---+\n",
      "    \n",
      "    >>> spark.sql('''\n",
      "    ...   SELECT m1.a, m2.b\n",
      "    ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      "    ...   ON m1.key = m2.key\n",
      "    ...   ORDER BY m1.a, m2.b''',\n",
      "    ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      "    ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      "    +---+---+\n",
      "    |  a|  b|\n",
      "    +---+---+\n",
      "    |  1|  3|\n",
      "    |  2|  4|\n",
      "    |  2|  5|\n",
      "    +---+---+\n",
      "    \n",
      "    Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      "    \n",
      "    >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      "    >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      "    +---+---+\n",
      "    |  A|  B|\n",
      "    +---+---+\n",
      "    |  1|  4|\n",
      "    |  2|  4|\n",
      "    |  3|  6|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6c91c1f-d832-47ba-a80d-34a54b00593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    ('Debanjan', 'Sarkar', 23, 'Male'),\n",
    "    ('Kadombini', 'Devi', 165, 'Female')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68663821-5d3a-46aa-be26-9138931bb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = spark.createDataFrame( data = data_list, schema = ['fname', 'lname', 'age', 'gender'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9d94ab2-3773-42f4-9b09-1687cfb01622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|    fname| lname|age|gender|\n",
      "+---------+------+---+------+\n",
      "| Debanjan|Sarkar| 23|  Male|\n",
      "|Kadombini|  Devi|165|Female|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbd2c2b1-4713-48ad-939b-db730503f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list.createOrReplaceTempView('basicTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abbb78f5-886d-4a16-a070-8d5f18fd9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = spark.sql(\"SELECT fname, age FROM basicTable;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5bd58ef-58f2-4564-9fed-4d72741959e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|    fname|age|\n",
      "+---------+---+\n",
      "| Debanjan| 23|\n",
      "|Kadombini|165|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24915b91-0485-40f6-9764-ebec341f94a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|Current_date              |\n",
      "+--------------------------+\n",
      "|2023-09-29 19:13:27.858621|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT current_timestamp AS Current_date; \").show( truncate = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830c8cd-64e6-44f4-bb68-c620c3de081d",
   "metadata": {},
   "source": [
    "# 3. table()\n",
    "<hr>\n",
    "Reads a table and returns it as a dataframe. Can be used to read temporary views too.<br>\n",
    "We just need to pass the table name as a string in the parameter, and no sql query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c65e250-10f4-4c57-afb7-6f7819b45d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|    fname| lname|age|gender|\n",
      "+---------+------+---+------+\n",
      "| Debanjan|Sarkar| 23|  Male|\n",
      "|Kadombini|  Devi|165|Female|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('basicTable').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e45e41-ea1a-413a-bf20-815e26087940",
   "metadata": {},
   "source": [
    "# 4. Spark UDFs:-\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f95b7ea-202a-4268-a613-1b40fb866e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------------------------------------------+------+--------------------------------------------------+\n",
      "|product_id|category_id|                                    description| price|                                        image_link|\n",
      "+----------+-----------+-----------------------------------------------+------+--------------------------------------------------+\n",
      "|       305|         38|       Mio ALPHA Heart Rate Monitor/Sport Watch| 199.0|http://images.acmesports.sports/Mio+ALPHA+Heart...|\n",
      "|       346|         16|                     Fitness Gear Pro Half Rack|349.99|http://images.acmesports.sports/Fitness+Gear+Pr...|\n",
      "|       382|         18|   PUMA Men's evoPOWER 1 Tricks FG Soccer Cleat|189.99|http://images.acmesports.sports/PUMA+Men%27s+ev...|\n",
      "|       463|         21|  Nike Kids' Grade School KD VI Basketball Shoe| 99.99|http://images.acmesports.sports/Nike+Kids%27+Gr...|\n",
      "|       625|         29|   Nike Men's Kobe IX Elite Low Basketball Shoe|199.99|http://images.acmesports.sports/Nike+Men%27s+Ko...|\n",
      "|      1231|         55|\"adidas Original Men's 2014 MLS All-Star Game \"|  28.0|http://images.acmesports.sports/adidas+Original...|\n",
      "+----------+-----------+-----------------------------------------------+------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the df created above using rdd\n",
    "rdd_df2.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01bcaf37-ebe3-468d-92aa-261ebb52af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating  UDF that takes a string and returns s String that is the unification of first alphabets of consecutive words in the main sentence\n",
    "# Ex:- \"I am very happy\" ---> compress_str() ---> \"Iavh\"\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf( returnType = StringType() )\n",
    "def compress_str(main_str: str):\n",
    "    if(main_str):\n",
    "        # This will ensure that this logic is not applied if Null is passed.\n",
    "        comp_str = \"\"\n",
    "        for word in main_str.split():\n",
    "            comp_str += word[0]\n",
    "        return comp_str\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bbf9f48-4aa7-4592-ad2a-2cfafd2ed494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-------------------------+\n",
      "|                                    description|compress_str(description)|\n",
      "+-----------------------------------------------+-------------------------+\n",
      "|       Mio ALPHA Heart Rate Monitor/Sport Watch|                   MAHRMW|\n",
      "|                     Fitness Gear Pro Half Rack|                    FGPHR|\n",
      "|   PUMA Men's evoPOWER 1 Tricks FG Soccer Cleat|                 PMe1TFSC|\n",
      "|  Nike Kids' Grade School KD VI Basketball Shoe|                 NKGSKVBS|\n",
      "|   Nike Men's Kobe IX Elite Low Basketball Shoe|                 NMKIELBS|\n",
      "|\"adidas Original Men's 2014 MLS All-Star Game \"|                 \"OM2MAG\"|\n",
      "+-----------------------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the created UDF on DataFrame column\n",
    "# ------------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "rdd_df2.select( [ F.col(\"description\"), compress_str(F.col(\"description\")) ] ).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680b161-e3b0-461f-9449-0ab217e26650",
   "metadata": {},
   "source": [
    "## Registering the UDF for Spark SQL:-\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5461016a-6872-42e7-8164-dc8814544bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: compressString. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.compressstring.; line 1 pos 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_15644/2837749046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\" SELECT compressString(\"I am currently living in Jaipur, Rajasthan.\"); \"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Undefined function: compressString. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.compressstring.; line 1 pos 8"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT compressString(\"I am currently living in Jaipur, Rajasthan.\"); \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "258d7d51-07f5-496a-8fcc-307a1d46a481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compress_str(main_str: str)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"compressString\", compress_str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5df8f35b-707e-4879-b73b-24ef1bd7efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|compressString(I am currently living in Jaipur, Rajasthan.)|\n",
      "+-----------------------------------------------------------+\n",
      "|                                                    IacliJR|\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT compressString(\"I am currently living in Jaipur, Rajasthan.\"); \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea36dcf-120a-45b0-8c4c-5830a7c1ed48",
   "metadata": {},
   "source": [
    "# 5. <i>'catalog'</i> Module :\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abe59b3-d258-4065-b211-9e7d13dc9a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Catalog in module pyspark.sql.catalog object:\n",
      "\n",
      "class Catalog(builtins.object)\n",
      " |  Catalog(sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      " |  \n",
      " |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      " |  \n",
      " |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession: pyspark.sql.session.SparkSession) -> None\n",
      " |      Create a new Catalog that wraps the underlying JVM object.\n",
      " |  \n",
      " |  cacheTable(self, tableName: str) -> None\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  clearCache(self) -> None\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createExternalTable(self, tableName: str, path: Union[str, NoneType] = None, source: Union[str, NoneType] = None, schema: Union[pyspark.sql.types.StructType, NoneType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  createTable(self, tableName: str, path: Union[str, NoneType] = None, source: Union[str, NoneType] = None, schema: Union[pyspark.sql.types.StructType, NoneType] = None, description: Union[str, NoneType] = None, **options: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
      " |      created from the data at the given path. Otherwise a managed table is created.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created table.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 3.1\n",
      " |         Added the ``description`` parameter.\n",
      " |  \n",
      " |  currentDatabase(self) -> str\n",
      " |      Returns the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  databaseExists(self, dbName: str) -> bool\n",
      " |      Check if the database with the specified name exists.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dbName : str\n",
      " |           name of the database to check existence\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Indicating whether the database exists\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      " |      False\n",
      " |      >>> df = spark.sql(\"CREATE DATABASE test_new_database\")\n",
      " |      >>> spark.catalog.databaseExists(\"test_new_database\")\n",
      " |      True\n",
      " |      >>> df = spark.sql(\"DROP DATABASE test_new_database\")\n",
      " |  \n",
      " |  dropGlobalTempView(self, viewName: str) -> None\n",
      " |      Drops the global temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      " |      True\n",
      " |      >>> spark.table(\"global_temp.my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  dropTempView(self, viewName: str) -> None\n",
      " |      Drops the local temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The return type of this method was None in Spark 2.0, but changed to Boolean\n",
      " |      in Spark 2.1.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      " |      True\n",
      " |      >>> spark.table(\"my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  functionExists(self, functionName: str, dbName: Union[str, NoneType] = None) -> bool\n",
      " |      Check if the function with the specified name exists.\n",
      " |      This can either be a temporary function or a function.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      functionName : str\n",
      " |          name of the function to check existence\n",
      " |      dbName : str, optional\n",
      " |          name of the database to check function existence in.\n",
      " |          If no database is specified, the current database is used\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Indicating whether the function exists\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.catalog.functionExists(\"unexisting_function\")\n",
      " |      False\n",
      " |  \n",
      " |  isCached(self, tableName: str) -> bool\n",
      " |      Returns true if the table is currently cached in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listColumns(self, tableName: str, dbName: Union[str, NoneType] = None) -> List[pyspark.sql.catalog.Column]\n",
      " |      Returns a list of columns for the given table/view in the specified database.\n",
      " |      \n",
      " |       If no database is specified, the current database is used.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |       Notes\n",
      " |       -----\n",
      " |       the order of arguments here is different from that of its JVM counterpart\n",
      " |       because Python does not support method overloading.\n",
      " |  \n",
      " |  listDatabases(self) -> List[pyspark.sql.catalog.Database]\n",
      " |      Returns a list of databases available across all sessions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listFunctions(self, dbName: Union[str, NoneType] = None) -> List[pyspark.sql.catalog.Function]\n",
      " |      Returns a list of functions registered in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary functions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listTables(self, dbName: Union[str, NoneType] = None) -> List[pyspark.sql.catalog.Table]\n",
      " |      Returns a list of tables/views in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary views.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  recoverPartitions(self, tableName: str) -> None\n",
      " |      Recovers all the partitions of the given table and update the catalog.\n",
      " |      \n",
      " |      Only works with a partitioned table, and not a view.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.1\n",
      " |  \n",
      " |  refreshByPath(self, path: str) -> None\n",
      " |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      " |      DataFrame that contains the given data source path.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  refreshTable(self, tableName: str) -> None\n",
      " |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  registerFunction(self, name: str, f: Callable[..., Any], returnType: Union[ForwardRef('DataType'), NoneType] = None) -> 'UserDefinedFunctionLike'\n",
      " |      An alias for :func:`spark.udf.register`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. deprecated:: 2.3.0\n",
      " |          Use :func:`spark.udf.register` instead.\n",
      " |  \n",
      " |  setCurrentDatabase(self, dbName: str) -> None\n",
      " |      Sets the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  tableExists(self, tableName: str, dbName: Union[str, NoneType] = None) -> bool\n",
      " |      Check if the table or view with the specified name exists.\n",
      " |      This can either be a temporary view or a table/view.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |                  name of the table to check existence\n",
      " |      dbName : str, optional\n",
      " |               name of the database to check table existence in.\n",
      " |               If no database is specified, the current database is used\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Indicating whether the table/view exists\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      This function can check if a table is defined or not:\n",
      " |      \n",
      " |      >>> spark.catalog.tableExists(\"unexisting_table\")\n",
      " |      False\n",
      " |      >>> df = spark.sql(\"CREATE TABLE tab1 (name STRING, age INT) USING parquet\")\n",
      " |      >>> spark.catalog.tableExists(\"tab1\")\n",
      " |      True\n",
      " |      >>> df = spark.sql(\"DROP TABLE tab1\")\n",
      " |      >>> spark.catalog.tableExists(\"unexisting_table\")\n",
      " |      False\n",
      " |      \n",
      " |      It also works for views:\n",
      " |      \n",
      " |      >>> spark.catalog.tableExists(\"view1\")\n",
      " |      False\n",
      " |      >>> df = spark.sql(\"CREATE VIEW view1 AS SELECT 1\")\n",
      " |      >>> spark.catalog.tableExists(\"view1\")\n",
      " |      True\n",
      " |      >>> df = spark.sql(\"DROP VIEW view1\")\n",
      " |      >>> spark.catalog.tableExists(\"view1\")\n",
      " |      False\n",
      " |      \n",
      " |      And also for temporary views:\n",
      " |      \n",
      " |      >>> df = spark.sql(\"CREATE TEMPORARY VIEW view1 AS SELECT 1\")\n",
      " |      >>> spark.catalog.tableExists(\"view1\")\n",
      " |      True\n",
      " |      >>> df = spark.sql(\"DROP VIEW view1\")\n",
      " |      >>> spark.catalog.tableExists(\"view1\")\n",
      " |      False\n",
      " |  \n",
      " |  uncacheTable(self, tableName: str) -> None\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( spark.catalog )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68685c22-6405-4c4d-8858-2bd10f7f307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parquet'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.sources.default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771874a7-fc6a-4e56-b69e-7e3aa567add7",
   "metadata": {},
   "source": [
    "### a. Database Functions:-\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05637377-edd4-4f77-860b-68bf572435ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352f6721-8acb-4464-af4b-9cfe489bf1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/Debanjan%20Sarkar/PySpark/prac1/2-complete-PySpark-developer-course/spark-warehouse')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b855b4-33bf-4717-b030-6926a7bb3a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE test_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d569361b-e595-4c06-97ef-e7950efbc756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/Debanjan%20Sarkar/PySpark/prac1/2-complete-PySpark-developer-course/spark-warehouse'),\n",
       " Database(name='test_db', description='', locationUri='file:/C:/Users/Debanjan%20Sarkar/PySpark/prac1/2-complete-PySpark-developer-course/spark-warehouse/test_db.db')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4392dd8-1185-4ed4-be97-b208c1bf761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase('test_db')\n",
    "# spark.catalog.setCurrentDatabase('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd8e74f-27a8-41e9-85ea-fc3959fbb303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_db'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353c43e1-4de7-4dfc-a983-093d2dc5ed0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default', 'test_db']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of all the databases present currently in the session:\n",
    "\n",
    "databases_present = [ db.name for db in spark.catalog.listDatabases() ]\n",
    "databases_present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68f01d-fe04-4d50-9cae-880dc55af045",
   "metadata": {},
   "source": [
    "### b. Table Functions:-\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112f6a52-e87f-440e-af2e-ad51771dbcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a188646a-6d20-410c-abc1-0d43d4f11f71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`testTbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_20260/2786704185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" CREATE TABLE testTbl (id INT, name STRING); \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`testTbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" CREATE TABLE testTbl (id INT, name STRING); \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983a4dd-6288-41f7-8233-90294db7d573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
