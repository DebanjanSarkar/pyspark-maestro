{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7840047-3825-4ce7-a4a3-b0b8b620cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89191a2b-c09f-45ea-b13d-c8e9c25599c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Partioning - Repartition and Coalesce\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e1b767-c129-4390-8128-6e21dcc020dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c14d3c-a397-4aa1-a651-96b9fe87fea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Partioning - Repartition and Coalesce</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25f022303d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2e28c5-cb3c-4803-8473-06b55e3ed346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Partioning - Repartition and Coalesce</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Partioning - Repartition and Coalesce>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285178d2-1b6b-4f59-91b6-90af46e9b8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255f9667-4ca2-47a5-aa9b-f1f962df9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = spark.range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1a0388-49dd-475e-b92d-ce4ce5146f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(range_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065f6321-8213-428d-8f63-869eee06941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27000f8f-6ecc-48bd-9060-2ce0133b74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_rdd = sc.range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e737c104-7974-42b1-bc74-ed1968ee2955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(range_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b691b23-d961-4a05-a271-e09b9fcb2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff340546-e427-4bd8-847b-ad202ee5cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|200|\n",
      "|198|\n",
      "|196|\n",
      "|194|\n",
      "|192|\n",
      "|190|\n",
      "|188|\n",
      "|186|\n",
      "|184|\n",
      "|182|\n",
      "|180|\n",
      "|178|\n",
      "|176|\n",
      "|174|\n",
      "|172|\n",
      "|170|\n",
      "|168|\n",
      "|166|\n",
      "|164|\n",
      "|162|\n",
      "|160|\n",
      "|158|\n",
      "|156|\n",
      "|154|\n",
      "|152|\n",
      "|150|\n",
      "|148|\n",
      "|146|\n",
      "|144|\n",
      "|142|\n",
      "|140|\n",
      "|138|\n",
      "|136|\n",
      "|134|\n",
      "|132|\n",
      "|130|\n",
      "|128|\n",
      "|126|\n",
      "|124|\n",
      "|122|\n",
      "|120|\n",
      "|118|\n",
      "|116|\n",
      "|114|\n",
      "|112|\n",
      "|110|\n",
      "|108|\n",
      "|106|\n",
      "|104|\n",
      "|102|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(200,100,-2).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a2d79-d3c3-444b-b379-fb1ff374c174",
   "metadata": {},
   "source": [
    "Both the below functions take same parameter as of Python <b><i>range()</i></b> function:-\n",
    "<ul>\n",
    "    <li><b><u>spark.range()</u></b> - Creates a Spark Dataframe, with single column named <i>'id'</i></li>\n",
    "    <li><b><u>spark.range()</u></b> - Creates an RDD, with specified range.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e537c95-f8fd-4495-905e-3ca71560d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "| id|id2|(id * 3)|\n",
      "+---+---+--------+\n",
      "|  0|  0|       0|\n",
      "|  1|  2|       3|\n",
      "|  2|  4|       6|\n",
      "|  3|  6|       9|\n",
      "|  4|  8|      12|\n",
      "|  5| 10|      15|\n",
      "|  6| 12|      18|\n",
      "|  7| 14|      21|\n",
      "|  8| 16|      24|\n",
      "|  9| 18|      27|\n",
      "| 10| 20|      30|\n",
      "| 11| 22|      33|\n",
      "| 12| 24|      36|\n",
      "| 13| 26|      39|\n",
      "| 14| 28|      42|\n",
      "| 15| 30|      45|\n",
      "| 16| 32|      48|\n",
      "| 17| 34|      51|\n",
      "| 18| 36|      54|\n",
      "| 19| 38|      57|\n",
      "+---+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_df.select(range_df.id, F.col('id')*2, range_df.id*3).withColumnRenamed('(id * 2)','id2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35679de-f495-41a5-abc1-f9a4d9368e3d",
   "metadata": {},
   "source": [
    "## Creating Dump Data File :\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6611be40-982c-4037-b0ce-4ad14864cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1000000)\n",
    "df = df.select(df.id,df.id*2,df.id*3)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96f67492-9e24-450c-afa9-479935fe46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|(id * 2)|(id * 3)|\n",
      "+---+--------+--------+\n",
      "|  0|       0|       0|\n",
      "|  1|       2|       3|\n",
      "|  2|       4|       6|\n",
      "|  3|       6|       9|\n",
      "|  4|       8|      12|\n",
      "+---+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22ff6373-cca9-44d1-ad1b-ffaf9b0e4391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, (id * 2)=0, (id * 3)=0),\n",
       " Row(id=1, (id * 2)=2, (id * 3)=3),\n",
       " Row(id=2, (id * 2)=4, (id * 3)=6),\n",
       " Row(id=3, (id * 2)=6, (id * 3)=9),\n",
       " Row(id=4, (id * 2)=8, (id * 3)=12)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "442ebac0-e718-4b17-9b7e-4cc09b2ec717",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd.map( lambda x: str(x[0]) + ',' + str(x[1]) + ',' + str(x[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "427950ad-a17a-4613-a4b7-972ae8b21c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,0,0', '1,2,3', '2,4,6', '3,6,9', '4,8,12']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b72e28cb-3e04-49fa-a4bc-ceb671f85f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.coalesce(1).saveAsTextFile('data/dump_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6aca4-d28f-4dc7-98a7-f5d8e8fd3bbc",
   "metadata": {},
   "source": [
    "## Reading the dumped file, and checking it for parttition change:-\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd5324e-034b-40c4-933b-91cebb4ff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data/dump_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4846a8-da94-4586-8904-a7188fdcd589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,0,0',\n",
       " '1,2,3',\n",
       " '2,4,6',\n",
       " '3,6,9',\n",
       " '4,8,12',\n",
       " '5,10,15',\n",
       " '6,12,18',\n",
       " '7,14,21',\n",
       " '8,16,24',\n",
       " '9,18,27']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4274606-f580-4ef4-8758-4d8f00d627ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83b0647-70d5-47bb-9db9-1fc8ebc8a951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5773cca4-5276-4721-ab28-264eddadf4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d23b71-66b1-4fce-ae0d-601d4b69cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map( lambda x: x.split(',') ).filter( lambda x: int(x[0]) == 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b1b09a-04b2-4ab1-8a5c-551e2e84b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c77a2b0-fde5-4ddc-9963-cf3db7d4e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3'],\n",
       " ['1', '2', '3']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48245220-45e1-4df4-9b1e-989d4e242e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370bdac-35aa-4c96-9d5f-e5528c455d10",
   "metadata": {},
   "source": [
    "Here, we can see that, even if <u>*rdd2*</u> has only 32 records, it is also divided into 21 partitons, as because it is a pipelinedRDD formed by applying *filter()* transformation on *rdd*, and *rdd* has 21 partitions due to its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaa7d49e-45c1-486a-b574-e24eb1a9479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.coalesce(22).getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db55de2b-653c-47b9-869f-0011948a6db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.repartition(22).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1bc8e-b946-4bd2-8e97-6cd0b5d7fafb",
   "metadata": {},
   "source": [
    "# Repartition and Sorting:-\n",
    "<hr>\n",
    "Using <b><i>repartitionAndSortWithinPartitions(numPartitions=None, \"partition_function\", ascending=True</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9522e00c-b080-4663-afa9-55d3c5fbbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (9,('a','z')),\n",
    "    (4,('c','s')),\n",
    "    (11,('f','f')),\n",
    "    (24,('g','g')),\n",
    "    (19,('f','h')),\n",
    "    (18,('m','t')),\n",
    "    (76,('f','k')),\n",
    "    (-4,('s','t')),\n",
    "    (8,('f','k')),\n",
    "    (129,('p','o')),\n",
    "    (56,('f','l')),\n",
    "    (85,('q','q')),\n",
    "    (108,('f','t')),\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a5fdce-393e-49dd-a75b-cd3280e820db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x%2, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf8addd-0c80-40af-b0bb-14a9d5bcf055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f09024b6-fae5-4307-8a89-d6c84f6692fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-4, ('s', 't')), (4, ('c', 's')), (8, ('f', 'k')), (18, ('m', 't')), (24, ('g', 'g')), (56, ('f', 'l')), (76, ('f', 'k')), (108, ('f', 't'))]\n",
      "\n",
      "[(9, ('a', 'z')), (11, ('f', 'f')), (19, ('f', 'h')), (85, ('q', 'q')), (129, ('p', 'o'))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in rdd2.glom().collect():\n",
    "    print(i, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80255e36-b382-41d9-b51e-6edfe868b3cf",
   "metadata": {},
   "source": [
    "All the elements are partitioned according to their key being odd or even, and in the partitions, the elements exists in sorted order, as can bee seen in the result above.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d202d5-17d7-4346-aed4-8fe5435222e5",
   "metadata": {},
   "source": [
    "# Extraction:-\n",
    "---------------\n",
    "### Saving results in RDD as textFile in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efa0bf2-3f2b-4397-8052-ba4d393964e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses. Trace:\npy4j.Py4JException: Method getCodecClasses([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_31012/1694646257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the list of supported compression codecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompression_codecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompressionCodecFactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetCodecClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# codec_names = [codec.getSimpleName() for codec in compression_codecs]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 raise Py4JError(\n\u001b[0m\u001b[0;32m    331\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses. Trace:\npy4j.Py4JException: Method getCodecClasses([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n"
     ]
    }
   ],
   "source": [
    "# Get the list of supported compression codecs\n",
    "compression_codecs = sc._jvm.org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses()\n",
    "codec_names = [codec.getSimpleName() for codec in compression_codecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa70146-b02e-45c7-991d-2d8c1e158652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the codec names\n",
    "print(\"Supported Compression Codecs:\")\n",
    "for codec_name in codec_names:\n",
    "    print(codec_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57df0d2-ad74-4ea1-bb29-1b4598cc87e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses. Trace:\npy4j.Py4JException: Method getCodecClasses([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_31012/3368693200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the list of supported compression codecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompression_codecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompressionCodecFactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetCodecClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcodec_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcodec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSimpleName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcodec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompression_codecs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Print the codec names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 raise Py4JError(\n\u001b[0m\u001b[0;32m    331\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses. Trace:\npy4j.Py4JException: Method getCodecClasses([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n"
     ]
    }
   ],
   "source": [
    "# Get the list of supported compression codecs\n",
    "compression_codecs = spark._jvm.org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses()\n",
    "codec_names = [codec.getSimpleName() for codec in compression_codecs]\n",
    "\n",
    "# Print the codec names\n",
    "print(\"Supported Compression Codecs:\")\n",
    "for codec_name in codec_names:\n",
    "    print(codec_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05e9eb-a421-4129-912a-766b4986690f",
   "metadata": {},
   "source": [
    "### Q3-1. Find the number of customers placed order in the month of July and August. Store the output at HDFS in textFile format. Create 5 files. Compressions format should be bzip2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42f6bca0-07c5-408b-9e95-e87c9061e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_rdd = sc.textFile('data/orders.csv').map( lambda x: x.split(',') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539ef01b-27d1-4935-8974-55b49504e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED'],\n",
       " ['2', '2013-07-25 00:00:00.0', '256', 'PENDING_PAYMENT'],\n",
       " ['3', '2013-07-25 00:00:00.0', '12111', 'COMPLETE'],\n",
       " ['4', '2013-07-25 00:00:00.0', '8827', 'CLOSED'],\n",
       " ['5', '2013-07-25 00:00:00.0', '11318', 'COMPLETE']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee89ce9-024e-4e9c-9550-231fa1d50df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_in_jul_aug = ord_rdd.map( lambda x: (int(x[1].split('-')[1]), int(x[2])) ).filter( lambda x: x[0] in (7,8) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa342f45-71be-4a23-81fe-f3be744dc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_ordered_in_jul_aug = ord_in_jul_aug.map( lambda x: x[1] ).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fcf78bd-aa92-41ee-95ca-4ba593628710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256,\n",
       " 11318,\n",
       " 7130,\n",
       " 4530,\n",
       " 5648,\n",
       " 918,\n",
       " 9842,\n",
       " 2568,\n",
       " 7276,\n",
       " 9488,\n",
       " 9198,\n",
       " 7562,\n",
       " 656,\n",
       " 196,\n",
       " 3960,\n",
       " 4840,\n",
       " 11586,\n",
       " 8214,\n",
       " 12092,\n",
       " 8136]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1493563d-12b2-45d4-8bd7-de1de5f5ffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7633"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a615964-6df4-476c-b4c0-3d40f8bf4dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44f66a48-824f-47fb-9579-e4fe5e7b8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_ordered_in_jul_aug = cust_ordered_in_jul_aug.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8529d85e-4f7c-4357-a0ad-6355486891c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bef39c61-c666-49eb-bf68-b10e1caa4e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1526, 1527, 1520, 1530, 1530]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48a98311-5df5-4de9-a024-3a4a30a95047",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_ordered_in_jul_aug.saveAsTextFile('data/customers_ordered_in_july_august.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74b1be98-b69a-4d83-8458-cb1dfdb26ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_ordered_in_jul_aug.saveAsTextFile('data/test', compressionCodecClass='org.apache.hadoop.io.compress.BZip2Codec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4e775-e497-42ee-827f-26491fc24228",
   "metadata": {},
   "source": [
    "## Saving as Sequence File:-\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "335765a3-d3ee-4217-a3fe-538c31e97484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[824, 8912, 6932, 2256, 9032]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e67c916-757d-4f19-93a3-aaca131581ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: org.apache.spark.SparkException: RDD element of type java.lang.Integer cannot be used\r\n\tat org.apache.spark.api.python.SerDeUtil$.pythonToPairRDD(SerDeUtil.scala:210)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:608)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:585)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_31012/443186770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcust_ordered_in_jul_aug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsSequenceFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/ord_jul_aug_seq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsSequenceFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   2114\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2116\u001b[1;33m         self.ctx._jvm.PythonRDD.saveAsSequenceFile(\n\u001b[0m\u001b[0;32m   2117\u001b[0m             \u001b[0mpickledRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodecClass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2118\u001b[0m         )\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: org.apache.spark.SparkException: RDD element of type java.lang.Integer cannot be used\r\n\tat org.apache.spark.api.python.SerDeUtil$.pythonToPairRDD(SerDeUtil.scala:210)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:608)\r\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:585)\r\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "cust_ordered_in_jul_aug.saveAsSequenceFile('data/ord_jul_aug_seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b251cfe-e712-4f21-a1cf-9be4da6181ae",
   "metadata": {},
   "source": [
    "### The above code shows error, because we are trying to write a normal rdd as sequence file.\n",
    "<hr>\n",
    "<h3> A sequence file should always be used for writing a key-value RDD, as follows: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "587a9ae9-11f9-4a1a-bb92-70a68eea984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 11599),\n",
       " (7, 256),\n",
       " (7, 12111),\n",
       " (7, 8827),\n",
       " (7, 11318),\n",
       " (7, 7130),\n",
       " (7, 4530),\n",
       " (7, 2911),\n",
       " (7, 5657),\n",
       " (7, 5648)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_in_jul_aug.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e87b131-f4b9-473e-8b30-89bad02c9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_in_jul_aug.saveAsSequenceFile('data/ord_jul_aug_seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453ce60-d09b-4b07-adac-a22128d00025",
   "metadata": {},
   "source": [
    "### Checking the contents of the sequence file being written:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79a43500-a4ba-4be7-830e-3d0d245fc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_file_rdd = sc.sequenceFile('data/ord_jul_aug_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2589d5a0-387d-4b44-badb-f940e035db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 11599),\n",
       " (7, 256),\n",
       " (7, 12111),\n",
       " (7, 8827),\n",
       " (7, 11318),\n",
       " (7, 7130),\n",
       " (7, 4530),\n",
       " (7, 2911),\n",
       " (7, 5657),\n",
       " (7, 5648)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_file_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ff04c-8611-4874-acf0-760ff693dbce",
   "metadata": {},
   "source": [
    "If we do not have suitable key in an RDD, then to write that RDD as sequence file, we can use <b><i>None</i></b> as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e60abb-0ad6-4e81-9262-896177270bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
