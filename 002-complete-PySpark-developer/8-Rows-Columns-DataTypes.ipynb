{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ae6177-5f9d-41b5-b54f-19c13be48b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a9d7ca-d1f8-4798-a9bd-59a27b7b87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('Rows, Columns and Datatypes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0549c14-d388-46f7-9e81-d3a84c29ed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Rows, Columns and Datatypes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x119f34203d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1c416f-5f2e-4ac6-8479-398426e85dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0037db7-0897-4c85-a166-7dd9efc41325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5adb4b-0175-4927-96b1-e14ad19010a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d42182-2a19-4ae5-b46d-6b4ac3d64a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e696dd71-d464-4d52-8f9a-c8f7a8a8d521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3466ca-6c8b-468c-8781-26923f0124b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.codegen.wholeStage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb67155-79b1-4d2e-af3e-d964220e37ab",
   "metadata": {},
   "source": [
    "# Spark Data Types :-\n",
    "----------------------\n",
    "These are the datatype classes used in Schema definitions of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d8b0af-8fae-438b-ac38-2130e80f824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, LongType, FloatType, DoubleType, StringType, BooleanType, ArrayType, MapType, StructType, StructField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8901c63-e70b-475c-a858-90cea94fb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "    StructField('fname', StringType(), True),\n",
    "    StructField('lname', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('univ_roll', LongType(), True),\n",
    "    StructField('dgpa', FloatType(), True),\n",
    "    StructField('intelligence', DoubleType(), True),\n",
    "    StructField('profiles', ArrayType(StringType()), True),\n",
    "    StructField('marks', MapType( StringType(), IntegerType() ), True ),\n",
    "    StructField('properties', MapType( StringType(), StringType() ), True),\n",
    "    StructField('isCreative', BooleanType(), True)\n",
    "])\n",
    "\n",
    "data_list = [\n",
    "    (\n",
    "        'Debanjan', \n",
    "        'Sarkar', \n",
    "        23, \n",
    "        19101105023,\n",
    "        8.84,\n",
    "        100.00,\n",
    "        ['facebook', 'instagram', 'twitter', 'linkedin', 'github', 'bugcrowd', 'telegram', 'whatsapp'],\n",
    "        {\n",
    "            'maths': 95,\n",
    "            'physics': 95,\n",
    "            'chemistry': 95,\n",
    "            'biology': 95,\n",
    "            'english': 87\n",
    "        },\n",
    "        {\n",
    "            'height': '168 cm',\n",
    "            'weight': '74 kg',\n",
    "            'irish': 'black'\n",
    "        },\n",
    "        True\n",
    "    ),\n",
    "    (\n",
    "        'Atul',\n",
    "        'Kumar',\n",
    "        24,\n",
    "        20408192350,\n",
    "        9.26,\n",
    "        100.00,\n",
    "        ['facebook', 'whatsapp', 'instagram'],\n",
    "        {\n",
    "            'maths': 98,\n",
    "            'physics': 96,\n",
    "            'chemistry': 91,\n",
    "            'biology': 70,\n",
    "            'english': 85\n",
    "        },\n",
    "        {\n",
    "            'height': '180 cm',\n",
    "            'weight': '65 kg',\n",
    "            'irish': 'brown'\n",
    "        },\n",
    "        False\n",
    "    ),\n",
    "    (\n",
    "        'Soumyadip',\n",
    "        'Mondal',\n",
    "        24,\n",
    "        1593891120,\n",
    "        8.21,\n",
    "        90.85,\n",
    "        ['facebook', 'whatsapp', 'instagram', 'snapchat'],\n",
    "        {\n",
    "            'maths': 88,\n",
    "            'physics': 76,\n",
    "            'chemistry': 91,\n",
    "            'biology': 71,\n",
    "            'english': 89\n",
    "        },\n",
    "        {\n",
    "            'height': '170 cm',\n",
    "            'weight': '59 kg',\n",
    "            'irish': 'brown'\n",
    "        },\n",
    "        True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b2cdf0-e321-45c4-9188-11752e89432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_df = spark.createDataFrame( data = data_list, schema = data_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9745bd1-c5b5-42b6-8c77-026eeb8d1d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- univ_roll: long (nullable = true)\n",
      " |-- dgpa: float (nullable = true)\n",
      " |-- intelligence: double (nullable = true)\n",
      " |-- profiles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- marks: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- isCreative: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friend_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395fd302-9168-4263-ab91-22a6c6bf9cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+-----------+----+------------+--------------------+--------------------+--------------------+----------+\n",
      "|    fname| lname|age|  univ_roll|dgpa|intelligence|            profiles|               marks|          properties|isCreative|\n",
      "+---------+------+---+-----------+----+------------+--------------------+--------------------+--------------------+----------+\n",
      "| Debanjan|Sarkar| 23|19101105023|8.84|       100.0|[facebook, instag...|{chemistry -> 95,...|{irish -> black, ...|      true|\n",
      "|     Atul| Kumar| 24|20408192350|9.26|       100.0|[facebook, whatsa...|{chemistry -> 91,...|{irish -> brown, ...|     false|\n",
      "|Soumyadip|Mondal| 24| 1593891120|8.21|       90.85|[facebook, whatsa...|{chemistry -> 91,...|{irish -> brown, ...|      true|\n",
      "+---------+------+---+-----------+----+------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friend_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14f16b7-01ca-4887-aef1-fa618f35fc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fname', 'string'),\n",
       " ('lname', 'string'),\n",
       " ('age', 'int'),\n",
       " ('univ_roll', 'bigint'),\n",
       " ('dgpa', 'float'),\n",
       " ('intelligence', 'double'),\n",
       " ('profiles', 'array<string>'),\n",
       " ('marks', 'map<string,int>'),\n",
       " ('properties', 'map<string,string>'),\n",
       " ('isCreative', 'boolean')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4d75c3-8168-40a7-9321-b35419d70e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('fname', StringType(), True), StructField('lname', StringType(), True), StructField('age', IntegerType(), True), StructField('univ_roll', LongType(), True), StructField('dgpa', FloatType(), True), StructField('intelligence', DoubleType(), True), StructField('profiles', ArrayType(StringType(), True), True), StructField('marks', MapType(StringType(), IntegerType(), True), True), StructField('properties', MapType(StringType(), StringType(), True), True), StructField('isCreative', BooleanType(), True)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ac1cad-803f-4b04-a19e-206c5c5c38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8174246-c4fc-483f-88cd-7276806bbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+\n",
      "|    fname| lname|  handles|\n",
      "+---------+------+---------+\n",
      "| Debanjan|Sarkar| facebook|\n",
      "| Debanjan|Sarkar|instagram|\n",
      "| Debanjan|Sarkar|  twitter|\n",
      "| Debanjan|Sarkar| linkedin|\n",
      "| Debanjan|Sarkar|   github|\n",
      "| Debanjan|Sarkar| bugcrowd|\n",
      "| Debanjan|Sarkar| telegram|\n",
      "| Debanjan|Sarkar| whatsapp|\n",
      "|     Atul| Kumar| facebook|\n",
      "|     Atul| Kumar| whatsapp|\n",
      "|     Atul| Kumar|instagram|\n",
      "|Soumyadip|Mondal| facebook|\n",
      "|Soumyadip|Mondal| whatsapp|\n",
      "|Soumyadip|Mondal|instagram|\n",
      "|Soumyadip|Mondal| snapchat|\n",
      "+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friend_df.select( 'fname', 'lname', F.explode('profiles').alias('handles') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f656f97-69c6-4759-b154-f0fceef01319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+-----+\n",
      "|    fname| lname|      key|value|\n",
      "+---------+------+---------+-----+\n",
      "| Debanjan|Sarkar|chemistry|   95|\n",
      "| Debanjan|Sarkar|  english|   87|\n",
      "| Debanjan|Sarkar|  biology|   95|\n",
      "| Debanjan|Sarkar|    maths|   95|\n",
      "| Debanjan|Sarkar|  physics|   95|\n",
      "|     Atul| Kumar|chemistry|   91|\n",
      "|     Atul| Kumar|  english|   85|\n",
      "|     Atul| Kumar|  biology|   70|\n",
      "|     Atul| Kumar|    maths|   98|\n",
      "|     Atul| Kumar|  physics|   96|\n",
      "|Soumyadip|Mondal|chemistry|   91|\n",
      "|Soumyadip|Mondal|  english|   89|\n",
      "|Soumyadip|Mondal|  biology|   71|\n",
      "|Soumyadip|Mondal|    maths|   88|\n",
      "|Soumyadip|Mondal|  physics|   76|\n",
      "+---------+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friend_df.select( 'fname', 'lname', F.explode('marks') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c56b42-0dd4-4912-babe-ab4098e8d3b9",
   "metadata": {},
   "source": [
    "# <i>'pyspark.sql.Row'</i> objects:\n",
    " <hr>\n",
    " <i>'pyspark.sql.Row'</i> object is immutable (read-only), that is, once created, the values cannot be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2663617b-7281-4d4e-a316-83a87a4caa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d84052a6-c252-4e4c-9506-91d28658369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_obj = Row(\n",
    "            fname = 'Debanjan',\n",
    "            lname = 'Sarkar',\n",
    "            age = 23,\n",
    "            univ_roll = 19101105023\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c1499de-9822-4c04-9277-39ed10a4c855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Debanjan'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e5b10f4-0c67-4800-b4bb-055def0a89b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lname' in row_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aa5f9-4126-4838-9f2d-74460c536a22",
   "metadata": {},
   "source": [
    "### Creating Custom class using 'Row', and using it:-\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8aaa6e-2aa8-42a0-bf6f-b46c3dc9bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Person = Row(\"name\", \"age\", \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6959d423-c0f5-467d-ad90-3d5c0b190229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca959890-ffc8-40fd-ae71-3375378371af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__fields__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_10456/3664013933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPerson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1895\u001b[0m             \u001b[1;31m# it will be slow when it has many fields,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1896\u001b[0m             \u001b[1;31m# but this will not be used in normal cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1897\u001b[1;33m             \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1891\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1893\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1894\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1895\u001b[0m             \u001b[1;31m# it will be slow when it has many fields,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __fields__"
     ]
    }
   ],
   "source": [
    "Person.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8687f06-5de8-4efb-a06e-ef3b29b1e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Person(\"Debanjan Sarkar\", 23, \"Male\")\n",
    "p2 = Person(\"Atul Kumar\", 24, \"Male\")\n",
    "p3 = Person(\"Debadrita Malakar\", 6, \"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f853f1-c9f0-4b58-ad55-9daf86aa6cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Debanjan Sarkar', age=23, gender='Male')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dc279d-00a1-453a-b7f0-c69cf682a5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Debanjan Sarkar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba5997c-27e2-4a67-8f80-3d685e632531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f49201f7-11d3-4550-8ced-1e070aee6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df = spark.createDataFrame( data = [p1,p2,p3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98196449-4959-4f20-b351-858f3672edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             name|age|gender|\n",
      "+-----------------+---+------+\n",
      "|  Debanjan Sarkar| 23|  Male|\n",
      "|       Atul Kumar| 24|  Male|\n",
      "|Debadrita Malakar|  6|Female|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13122cfa-25b0-4a95-8f7f-d2a151392186",
   "metadata": {},
   "source": [
    "### Row class functions:-\n",
    "<hr>\n",
    "<ul>\n",
    "    <li><b>count()</b> - Returns the number of occurences of the value, that is passed as parameter.</li>\n",
    "    <li><b>index()</b> - Returns the first index at which the value (data in parameter) occurs in the object.</li>\n",
    "    <li><b>asDict()</b> - Return the Row object as a Dictionary.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34d513a-454f-4bd0-bd48-592229c9df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_obj = Row(\n",
    "            fname = 'Debanjan',\n",
    "            lname = 'Sarkar',\n",
    "            age = 23,\n",
    "            univ_roll = 19101105023,\n",
    "            family_name = 'Sarkar'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81eb271c-c380-41f6-83ed-f702eb41dfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(fname='Debanjan', lname='Sarkar', age=23, univ_roll=19101105023)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b74f461-d504-40ef-b51c-4ccb8ea98bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.count('Sarkar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a91d8382-3fb5-4938-8e0e-41af0464cad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.count(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81123e18-bbfd-499d-9118-5ffaa3161498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.index('Sarkar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ac7371-ad1d-47b0-90e7-6733943ca737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.index('Debanjan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62e2cdb2-20c2-4497-b7ce-23d3b6b20700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.index(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f4fdb44-f61d-42af-afb8-9908b1fcd1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fname': 'Debanjan',\n",
       " 'lname': 'Sarkar',\n",
       " 'age': 23,\n",
       " 'univ_roll': 19101105023,\n",
       " 'family_name': 'Sarkar'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_obj.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac0e14f1-e36e-4b74-b559-13de93135eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = row_obj.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a759dac-694d-4e35-b3e5-28a50282c6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8238e1ea-e646-4d40-901d-db3960f0f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname\n",
      "lname\n",
      "age\n",
      "univ_roll\n",
      "family_name\n"
     ]
    }
   ],
   "source": [
    "for i in row_dict.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7604fa68-1a33-4b85-b5c4-5be5ebaa3411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Debanjan', 'Sarkar', 23, 19101105023, 'Sarkar'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6057df-d1da-49f9-a166-a51005923c0d",
   "metadata": {},
   "source": [
    "# Columns :-\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a6efe7f-4efc-4a48-ba42-ff78dec45bdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Column in pyspark.sql.column:\n",
      "\n",
      "pyspark.sql.column.Column = class Column(builtins.object)\n",
      " |  pyspark.sql.column.Column(jc: py4j.java_gateway.JavaObject) -> None\n",
      " |  \n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self) -> None\n",
      " |  \n",
      " |  __contains__(self, item: Any) -> None\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item: Any) -> 'Column'\n",
      " |  \n",
      " |  __getitem__(self, k: Any) -> 'Column'\n",
      " |  \n",
      " |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __iter__(self) -> None\n",
      " |  \n",
      " |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__(self, other: Any) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __neg__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __nonzero__(self) -> None\n",
      " |  \n",
      " |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          desired column names (collects all positional arguments passed)\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      metadata: dict\n",
      " |          a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |  \n",
      " |  asc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |  \n",
      " |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n",
      " |      Casts the column into type ``dataType``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |  \n",
      " |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          string in line. A value as a literal or a :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |  \n",
      " |  dropFields(self, *fieldNames: str) -> 'Column'\n",
      " |      An expression that drops fields in :class:`StructType` by name.\n",
      " |      This is a no-op if schema doesn't contain field name(s).\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import col, lit\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      " |      +-----------------+\n",
      " |      |                a|\n",
      " |      +-----------------+\n",
      " |      |{2, 3, {4, 5, 6}}|\n",
      " |      +-----------------+\n",
      " |      \n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{3, {4, 5, 6}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      This method supports dropping multiple nested fields directly e.g.\n",
      " |      \n",
      " |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      However, if you are going to add/replace multiple nested fields,\n",
      " |      it is preferred to extract out the nested struct before\n",
      " |      adding/replacing multiple fields e.g.\n",
      " |      \n",
      " |      >>> df.select(col(\"a\").withField(\n",
      " |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      " |      ... ).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |  \n",
      " |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      " |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      " |      for details.\n",
      " |  \n",
      " |  getField(self, name: Any) -> 'Column'\n",
      " |      An expression that gets a field by name in a :class:`StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |  \n",
      " |  getItem(self, key: Any) -> 'Column'\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |  \n",
      " |  ilike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n",
      " |      based on a case insensitive match.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.ilike('%Ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  isNotNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(name='Tom', height=80)]\n",
      " |  \n",
      " |  isNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(name='Alice', height=None)]\n",
      " |  \n",
      " |  isin(self, *cols: Any) -> 'Column'\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  like = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  over(self, window: 'WindowSpec') -> 'Column'\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : :class:`WindowSpec`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |  \n",
      " |  rlike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          an extended regex expression\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      startPos : :class:`Column` or int\n",
      " |          start position\n",
      " |      length : :class:`Column` or int\n",
      " |          length of the substring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |  \n",
      " |  when(self, condition: 'Column', value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column`\n",
      " |          a boolean :class:`Column` expression.\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n",
      " |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import lit\n",
      " |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      " |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      " |      +---+\n",
      " |      |  d|\n",
      " |      +---+\n",
      " |      |  4|\n",
      " |      +---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( \"pyspark.sql.column.Column\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95481986-92bd-4586-85a5-5d13bef3ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, TimestampType, StringType\n",
    "\n",
    "ord_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", TimestampType(), True),\n",
    "    StructField(\"cust_id\", LongType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "ord_df = spark.read.csv('data/orders.csv', schema = ord_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6786969-3485-42c3-b2a4-68731ddcc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|  12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|   8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|  11318|       COMPLETE|\n",
      "+--------+-------------------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a15403e-d6ee-447e-834e-570629234798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), cust_id=11599, order_status='CLOSED'),\n",
       " Row(order_id=2, order_date=datetime.datetime(2013, 7, 25, 0, 0), cust_id=256, order_status='PENDING_PAYMENT'),\n",
       " Row(order_id=3, order_date=datetime.datetime(2013, 7, 25, 0, 0), cust_id=12111, order_status='COMPLETE'),\n",
       " Row(order_id=4, order_date=datetime.datetime(2013, 7, 25, 0, 0), cust_id=8827, order_status='CLOSED'),\n",
       " Row(order_id=5, order_date=datetime.datetime(2013, 7, 25, 0, 0), cust_id=11318, order_status='COMPLETE')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb02e74a-cb6a-4363-8b51-983956680d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- cust_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6cba0f-4ca6-4c7b-be26-30e907555633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ord_df.order_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1e4d9-dd44-4445-a876-02dc646ded8e",
   "metadata": {},
   "source": [
    "### Accessing column in the form of a datatype:-\n",
    "-------------------------------------------------\n",
    "Suppose, we want to select a column named <b><i>col1</i></b> in a dataframe named <b><i>df1</i></b> :\n",
    "<ul>\n",
    "    <li> <b><i>df1.col1</i></b> - Accessing using dot notation, just like member variable.</li>\n",
    "    <li> <b><i>df1[ 'col1' ]</i></b> - Accesssing the column as we access a particulary value using key in dictionary.</li>\n",
    "    <li> <b><i>F.col( \"col1\" )</i></b> - Accessing the column by passing the column name as string to the <i>pyspark.sql.functions.col()</i> function</li>\n",
    "    <li> <b><i>\"col1\"</i></b> - In some places such as in <i>df.select()</i> method, column can be accessed by simply passing the column name as string.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e37c9d-62ba-44da-9aec-192e431cf4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_id ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495d3639-5463-4e87-9da6-499c7ba89beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df[\"order_id\"] ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b5fdff0-f3f0-4c64-bd15-dd3fec530a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ord_df.select( F.col('order_id') ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f72b2910-8e57-4ebe-a6a9-dc5cdfbcc28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( \"order_id\" ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc1b62-824e-4b60-9aeb-21f6a192e35b",
   "metadata": {},
   "source": [
    "### Selecting all the columns:-\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ee71963-e036-4fe4-b5b4-754fe9a15513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|  12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|   8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|  11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|   7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|   4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|   2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|    918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|   9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|   2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|   2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|   9198|     PROCESSING|\n",
      "+--------+-------------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44314f0e-1ba2-4d04-9114-9443ceb97960",
   "metadata": {},
   "source": [
    "### Setting column alias:-\n",
    "<hr>\n",
    "Alias of a column can be set using the <b><i>'alias()'</i></b> column method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531dad33-719f-4125-b9ba-2f0087abfc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|Customer Id to whom the order belongs|\n",
      "+-------------------------------------+\n",
      "|                                11599|\n",
      "|                                  256|\n",
      "|                                12111|\n",
      "|                                 8827|\n",
      "|                                11318|\n",
      "|                                 7130|\n",
      "|                                 4530|\n",
      "|                                 2911|\n",
      "|                                 5657|\n",
      "|                                 5648|\n",
      "|                                  918|\n",
      "|                                 1837|\n",
      "|                                 9149|\n",
      "|                                 9842|\n",
      "|                                 2568|\n",
      "|                                 7276|\n",
      "|                                 2667|\n",
      "|                                 1205|\n",
      "|                                 9488|\n",
      "|                                 9198|\n",
      "+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.cust_id.alias(\"Customer Id to whom the order belongs\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471b0fd-0e9b-4768-a853-82d37ecd89f8",
   "metadata": {},
   "source": [
    "### Ordering based on column values / sorting :-\n",
    "-------------------------------------------------\n",
    "- asc()\n",
    "- asc_nulls__first()\n",
    "- asc_nulls_last()\n",
    "- desc()\n",
    "- desc_nulls_first()\n",
    "- desc_nulls_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d3215e7-86ef-4d4a-9210-28c805fbe4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|  12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|   8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|  11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|   7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|   4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|   2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|    918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|   9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|   2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|   2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|   9198|     PROCESSING|\n",
      "+--------+-------------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c554c95e-b460-4f9b-8ed0-0b8bb3f68506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------------+\n",
      "|order_id|order_date|cust_id|order_status|\n",
      "+--------+----------+-------+------------+\n",
      "+--------+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.filter( \"order_status is NULL\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a308475-413e-49fe-a24d-b19d51aaf64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------------+\n",
      "|order_id|order_date|cust_id|order_status|\n",
      "+--------+----------+-------+------------+\n",
      "+--------+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.filter( \"order_date is NULL\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efab22d0-9eae-445f-b401-ce00548adcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|     215|2013-07-26 00:00:00|   5925|         CLOSED|\n",
      "|     537|2013-07-28 00:00:00|  10437|       COMPLETE|\n",
      "|     663|2013-07-28 00:00:00|   3531| PAYMENT_REVIEW|\n",
      "|     685|2013-07-29 00:00:00|  10745|        ON_HOLD|\n",
      "|     708|2013-07-29 00:00:00|   5695|         CLOSED|\n",
      "|    1214|2013-07-31 00:00:00|   7892|        ON_HOLD|\n",
      "|    1280|2013-07-31 00:00:00|   5712|         CLOSED|\n",
      "|    1351|2013-08-01 00:00:00|   9452|     PROCESSING|\n",
      "|    1739|2013-08-03 00:00:00|   1791|PENDING_PAYMENT|\n",
      "|    2251|2013-08-06 00:00:00|   4876| PAYMENT_REVIEW|\n",
      "|    2326|2013-08-06 00:00:00|   8716|       COMPLETE|\n",
      "|    2573|2013-08-07 00:00:00|   5992|         CLOSED|\n",
      "|    2680|2013-08-08 00:00:00|   8781|SUSPECTED_FRAUD|\n",
      "|    3124|2013-08-11 00:00:00|   5509|         CLOSED|\n",
      "|    3897|2013-08-17 00:00:00|   6769|       COMPLETE|\n",
      "|    4013|2013-08-17 00:00:00|  11255|PENDING_PAYMENT|\n",
      "|    4064|2013-08-17 00:00:00|   6476|PENDING_PAYMENT|\n",
      "|    4073|2013-08-17 00:00:00|   9639|       COMPLETE|\n",
      "|    4085|2013-08-17 00:00:00|   3375|       COMPLETE|\n",
      "|    4481|2013-08-21 00:00:00|   8076|       COMPLETE|\n",
      "+--------+-------------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.orderBy( ord_df.order_status.asc() ).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30ac7d-41e7-4416-bcdf-a064ced4ba47",
   "metadata": {},
   "source": [
    "### Casting the dataype of a Column:-\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd09013-f87c-425b-a0c0-c3ab832b8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_id ).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007daa4c-e008-4527-b669-739f0d79bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_id.cast('string') ).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3007012-28c2-41a2-bf79-7e0b3ae28616",
   "metadata": {},
   "source": [
    "### Applying filters:-\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd1803b-027a-479a-8f12-66a4a8c28cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+--------------------------+\n",
      "|order_id|         order_date|cust_id|   order_status|order id between 10 and 20|\n",
      "+--------+-------------------+-------+---------------+--------------------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|                     false|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|                     false|\n",
      "|       3|2013-07-25 00:00:00|  12111|       COMPLETE|                     false|\n",
      "|       4|2013-07-25 00:00:00|   8827|         CLOSED|                     false|\n",
      "|       5|2013-07-25 00:00:00|  11318|       COMPLETE|                     false|\n",
      "|       6|2013-07-25 00:00:00|   7130|       COMPLETE|                     false|\n",
      "|       7|2013-07-25 00:00:00|   4530|       COMPLETE|                     false|\n",
      "|       8|2013-07-25 00:00:00|   2911|     PROCESSING|                     false|\n",
      "|       9|2013-07-25 00:00:00|   5657|PENDING_PAYMENT|                     false|\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|                      true|\n",
      "|      11|2013-07-25 00:00:00|    918| PAYMENT_REVIEW|                      true|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|                      true|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|                      true|\n",
      "|      14|2013-07-25 00:00:00|   9842|     PROCESSING|                      true|\n",
      "|      15|2013-07-25 00:00:00|   2568|       COMPLETE|                      true|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|                      true|\n",
      "|      17|2013-07-25 00:00:00|   2667|       COMPLETE|                      true|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|                      true|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|                      true|\n",
      "|      20|2013-07-25 00:00:00|   9198|     PROCESSING|                      true|\n",
      "+--------+-------------------+-------+---------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.withColumn( \"order id between 10 and 20\", ord_df.order_id.between(10,20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80efdab7-214a-4f69-80bb-804f7dd3da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|    918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|   9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|   2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|   2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|   9198|     PROCESSING|\n",
      "+--------+-------------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.where( ord_df.order_id.between(10,20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a010b7-6e9f-48c4-be85-5ee5ea732cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|    918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|   9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|   2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|   2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|   9198|     PROCESSING|\n",
      "+--------+-------------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.filter( (ord_df.order_id>=10) & (ord_df.order_id<=20) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e61396-1f7e-4a10-be61-cfbe2de7eec7",
   "metadata": {},
   "source": [
    "### Column methods for string processing and pattern matching:-\n",
    "-----------------------------------------------------------------\n",
    "- contains()\n",
    "- startswith()\n",
    "- endswith()\n",
    "- like()\n",
    "- rlike()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7740449-cd91-4d09-9b84-43e66fa8457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|contains(order_status, PAYMENT)|\n",
      "+-------------------------------+\n",
      "|                           true|\n",
      "|                          false|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_status.contains( \"PAYMENT\" ) ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c714173-38b6-466d-bb97-77419b003b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|         CLOSED|\n",
      "|PENDING_PAYMENT|\n",
      "|       COMPLETE|\n",
      "|         CLOSED|\n",
      "|       COMPLETE|\n",
      "|       COMPLETE|\n",
      "|       COMPLETE|\n",
      "|     PROCESSING|\n",
      "|PENDING_PAYMENT|\n",
      "|PENDING_PAYMENT|\n",
      "| PAYMENT_REVIEW|\n",
      "|         CLOSED|\n",
      "|PENDING_PAYMENT|\n",
      "|     PROCESSING|\n",
      "|       COMPLETE|\n",
      "|PENDING_PAYMENT|\n",
      "|       COMPLETE|\n",
      "|         CLOSED|\n",
      "|PENDING_PAYMENT|\n",
      "|     PROCESSING|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df = ord_df.select(\"order_status\")\n",
    "\n",
    "order_status_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28012275-be80-452c-b504-10f845936a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|PENDING_PAYMENT|\n",
      "| PAYMENT_REVIEW|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df.where( order_status_df.order_status.contains(\"PAYMENT\") ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18cc7211-ba51-4351-a39d-79a68502f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|PENDING_PAYMENT|\n",
      "|        PENDING|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df.where( order_status_df.order_status.like(\"%END%\") ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b6831b-2873-4002-8218-2dbe9927be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|PENDING_PAYMENT|\n",
      "| PAYMENT_REVIEW|\n",
      "|     PROCESSING|\n",
      "|        PENDING|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df.where( order_status_df.order_status.startswith(\"P\") ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "271db5e6-74d8-4783-87c8-9a21937548a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|order_status|\n",
      "+------------+\n",
      "|  PROCESSING|\n",
      "|     PENDING|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df.where( order_status_df.order_status.endswith(\"ING\") ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "546886b1-16f0-41ae-a6e8-16ddbbbf09db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|order_status|\n",
      "+------------+\n",
      "|  PROCESSING|\n",
      "|     PENDING|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_status_df.filter( order_status_df.order_status.like(\"%ING\") ).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e946e9b-11e3-4a1b-b272-226991366e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+------------+\n",
      "|order_id|         order_date|cust_id|order_status|\n",
      "+--------+-------------------+-------+------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|      CLOSED|\n",
      "|       4|2013-07-25 00:00:00|   8827|      CLOSED|\n",
      "|      12|2013-07-25 00:00:00|   1837|      CLOSED|\n",
      "|      18|2013-07-25 00:00:00|   1205|      CLOSED|\n",
      "|      24|2013-07-25 00:00:00|  11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:00|   9503|      CLOSED|\n",
      "|      37|2013-07-25 00:00:00|   5863|      CLOSED|\n",
      "|      51|2013-07-25 00:00:00|  12271|      CLOSED|\n",
      "|      57|2013-07-25 00:00:00|   7073|      CLOSED|\n",
      "|      61|2013-07-25 00:00:00|   4791|      CLOSED|\n",
      "|      62|2013-07-25 00:00:00|   9111|      CLOSED|\n",
      "|      87|2013-07-25 00:00:00|   3065|      CLOSED|\n",
      "|      90|2013-07-25 00:00:00|   9131|      CLOSED|\n",
      "|     101|2013-07-25 00:00:00|   5116|      CLOSED|\n",
      "|     116|2013-07-26 00:00:00|   8763|      CLOSED|\n",
      "|     129|2013-07-26 00:00:00|   9937|      CLOSED|\n",
      "|     133|2013-07-26 00:00:00|  10604|      CLOSED|\n",
      "|     191|2013-07-26 00:00:00|     16|      CLOSED|\n",
      "|     201|2013-07-26 00:00:00|   9055|      CLOSED|\n",
      "|     211|2013-07-26 00:00:00|  10372|      CLOSED|\n",
      "+--------+-------------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Orders with status CLOSED\n",
    "\n",
    "ord_df.where( ord_df.order_status.contains(\"CLOSED\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d06cc3d8-2ca6-4bd3-a159-9ffca4ce16e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+------------+\n",
      "|order_id|         order_date|cust_id|order_status|\n",
      "+--------+-------------------+-------+------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|      CLOSED|\n",
      "|       4|2013-07-25 00:00:00|   8827|      CLOSED|\n",
      "|      12|2013-07-25 00:00:00|   1837|      CLOSED|\n",
      "|      18|2013-07-25 00:00:00|   1205|      CLOSED|\n",
      "|      24|2013-07-25 00:00:00|  11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:00|   9503|      CLOSED|\n",
      "|      37|2013-07-25 00:00:00|   5863|      CLOSED|\n",
      "|      51|2013-07-25 00:00:00|  12271|      CLOSED|\n",
      "|      57|2013-07-25 00:00:00|   7073|      CLOSED|\n",
      "|      61|2013-07-25 00:00:00|   4791|      CLOSED|\n",
      "|      62|2013-07-25 00:00:00|   9111|      CLOSED|\n",
      "|      87|2013-07-25 00:00:00|   3065|      CLOSED|\n",
      "|      90|2013-07-25 00:00:00|   9131|      CLOSED|\n",
      "|     101|2013-07-25 00:00:00|   5116|      CLOSED|\n",
      "|     116|2013-07-26 00:00:00|   8763|      CLOSED|\n",
      "|     129|2013-07-26 00:00:00|   9937|      CLOSED|\n",
      "|     133|2013-07-26 00:00:00|  10604|      CLOSED|\n",
      "|     191|2013-07-26 00:00:00|     16|      CLOSED|\n",
      "|     201|2013-07-26 00:00:00|   9055|      CLOSED|\n",
      "|     211|2013-07-26 00:00:00|  10372|      CLOSED|\n",
      "+--------+-------------------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.filter( ord_df.order_status.contains(\"CLOSED\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76310760-dcb6-4ed4-9361-cfa5705940c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+---------------+\n",
      "|order_id|         order_date|cust_id|   order_status|\n",
      "+--------+-------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:00|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|    256|PENDING_PAYMENT|\n",
      "|       4|2013-07-25 00:00:00|   8827|         CLOSED|\n",
      "|       9|2013-07-25 00:00:00|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|   5648|PENDING_PAYMENT|\n",
      "|      12|2013-07-25 00:00:00|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|   9149|PENDING_PAYMENT|\n",
      "|      16|2013-07-25 00:00:00|   7276|PENDING_PAYMENT|\n",
      "|      18|2013-07-25 00:00:00|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|   9488|PENDING_PAYMENT|\n",
      "|      21|2013-07-25 00:00:00|   2711|        PENDING|\n",
      "|      23|2013-07-25 00:00:00|   4367|PENDING_PAYMENT|\n",
      "|      24|2013-07-25 00:00:00|  11441|         CLOSED|\n",
      "|      25|2013-07-25 00:00:00|   9503|         CLOSED|\n",
      "|      27|2013-07-25 00:00:00|   3241|PENDING_PAYMENT|\n",
      "|      30|2013-07-25 00:00:00|  10039|PENDING_PAYMENT|\n",
      "|      33|2013-07-25 00:00:00|   5793|PENDING_PAYMENT|\n",
      "|      36|2013-07-25 00:00:00|   5649|        PENDING|\n",
      "|      37|2013-07-25 00:00:00|   5863|         CLOSED|\n",
      "|      39|2013-07-25 00:00:00|   8214|        PENDING|\n",
      "+--------+-------------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all the orders which are CLOSED or PENDING\n",
    "\n",
    "ord_df.where( ord_df.order_status.isin( \"CLOSED\", \"PENDING\", \"PENDING_PAYMENT\" ) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645df3a0-6992-41bd-ab9b-729db32c146d",
   "metadata": {},
   "source": [
    "### Handling 'NULL' value while checking for equality:\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a407910c-511c-4656-8765-431f2d6447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "things_data = [\n",
    "                {\"category\": \"car\", \"brand\": \"maruti\"},\n",
    "                {\"category\": \"mobile\", \"brand\": \"realme\"},\n",
    "                {\"category\": \"tv\", \"brand\": \"toshiba\"},\n",
    "                {\"category\": \"salt\", \"brand\": \"tata\"},\n",
    "                {\"category\": \"tea\", \"brand\": \"tata\"},\n",
    "                {\"category\": \"sugar\", \"brand\": None},\n",
    "                {\"category\": \"flour\", \"brand\": None}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b93ba20-4224-46a3-a2d3-bfa4551d40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "things_df = spark.createDataFrame( data = things_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d329fe4a-ba6a-4f0e-99c5-7af838a348e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  brand|category|\n",
      "+-------+--------+\n",
      "| maruti|     car|\n",
      "| realme|  mobile|\n",
      "|toshiba|      tv|\n",
      "|   tata|    salt|\n",
      "|   tata|     tea|\n",
      "|   null|   sugar|\n",
      "|   null|   flour|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f571b-32bb-4219-870c-9383a8846888",
   "metadata": {},
   "source": [
    "As can be seen in the cell below, when equality operator is used to filter out record, it does not give truth value for null values. For null value handling, <b><i>'eqNullSafe()'</i></b> Column method iss best, as it gives <b>False</b> for the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "579f8d74-9bf4-4311-8d88-65a420978746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|(brand = tata)|(brand <=> tata)|\n",
      "+--------------+----------------+\n",
      "|         false|           false|\n",
      "|         false|           false|\n",
      "|         false|           false|\n",
      "|          true|            true|\n",
      "|          true|            true|\n",
      "|          null|           false|\n",
      "|          null|           false|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.select( things_df.brand == 'tata', things_df.brand.eqNullSafe(\"tata\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a69aa31-52a3-4e61-917e-55f074f3068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  brand|category|\n",
      "+-------+--------+\n",
      "| maruti|     car|\n",
      "| realme|  mobile|\n",
      "|toshiba|      tv|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.where( things_df.brand != 'tata' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03fe1062-5953-48c7-889a-b96aa9061dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  brand|category|\n",
      "+-------+--------+\n",
      "| maruti|     car|\n",
      "| realme|  mobile|\n",
      "|toshiba|      tv|\n",
      "|   null|   sugar|\n",
      "|   null|   flour|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.where( ~things_df.brand.eqNullSafe(\"tata\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573e1fd-257f-43ed-9e0a-2f4420063832",
   "metadata": {},
   "source": [
    "### Fetching only those records that has Null in a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4208d873-d790-434c-85ce-b0e3ba91f201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|brand|category|\n",
      "+-----+--------+\n",
      "| null|   sugar|\n",
      "| null|   flour|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.where( things_df.brand.isNull() ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "328a3939-66e9-48a6-b6db-576c1cd1909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  brand|category|\n",
      "+-------+--------+\n",
      "| maruti|     car|\n",
      "| realme|  mobile|\n",
      "|toshiba|      tv|\n",
      "|   tata|    salt|\n",
      "|   tata|     tea|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "things_df.where( things_df.brand.isNotNull() ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67015a8c-a0e6-4132-a7b4-576ecf49e3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
