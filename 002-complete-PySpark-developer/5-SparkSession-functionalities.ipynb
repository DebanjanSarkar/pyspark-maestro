{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd28cedb-a77c-4c41-99d5-e1eeff89aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1220a489-a0a2-48bf-88ee-48c8a71a2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSession-functionalities\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563a663f-7d44-4534-9b5c-bbddc05e9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f771b68f-9d6c-40f6-b056-e270b6dcc336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSession-functionalities</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1137bac03d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb76f9e-b119-412f-8cbc-f7ba7ed9b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkSession in module pyspark.sql.session object:\n",
      "\n",
      "class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n",
      " |  SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Union[py4j.java_gateway.JavaObject, NoneType] = None, options: Dict[str, Any] = {})\n",
      " |  \n",
      " |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      " |  \n",
      " |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  To create a :class:`SparkSession`, use the following builder pattern:\n",
      " |  \n",
      " |  .. autoattribute:: builder\n",
      " |     :annotation:\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> spark = SparkSession.builder \\\n",
      " |  ...     .master(\"local\") \\\n",
      " |  ...     .appName(\"Word Count\") \\\n",
      " |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      " |  ...     .getOrCreate()\n",
      " |  \n",
      " |  >>> from datetime import datetime\n",
      " |  >>> from pyspark.sql import Row\n",
      " |  >>> spark = SparkSession(sc)\n",
      " |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |  >>> df = allTypes.toDF()\n",
      " |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |  >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |  ...            'from allTypes where b and i > 0').collect()\n",
      " |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SparkSession\n",
      " |      pyspark.sql.pandas.conversion.SparkConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self) -> 'SparkSession'\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __exit__(self, exc_type: Union[Type[BaseException], NoneType], exc_val: Union[BaseException, NoneType], exc_tb: Union[traceback, NoneType]) -> None\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the SparkSession on exit of the with block.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __init__(self, sparkContext: pyspark.context.SparkContext, jsparkSession: Union[py4j.java_gateway.JavaObject, NoneType] = None, options: Dict[str, Any] = {})\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Union[float, NoneType] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of either :class:`Row`,\n",
      " |      :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      " |      Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 2.1.0\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : :class:`RDD` or iterable\n",
      " |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      " |          :class:`pandas.DataFrame`.\n",
      " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>``.\n",
      " |      samplingRatio : float, optional\n",
      " |          the sample ratio of rows used for inferring\n",
      " |      verifySchema : bool, optional\n",
      " |          verify data types of every row against schema. Enabled by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> spark.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> spark.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> spark.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = spark.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = spark.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |  \n",
      " |  newSession(self) -> 'SparkSession'\n",
      " |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  range(self, start: int, end: Union[int, NoneType] = None, step: int = 1, numPartitions: Union[int, NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          the start value\n",
      " |      end : int, optional\n",
      " |          the end value (exclusive)\n",
      " |      step : int, optional\n",
      " |          the incremental step (default: 1)\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions of the DataFrame\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> spark.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |  \n",
      " |  sql(self, sqlQuery: str, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      " |      standard formatter.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sqlQuery : str\n",
      " |          SQL query string.\n",
      " |      kwargs : dict\n",
      " |          Other variables that the user wants to set that can be referenced in the query\n",
      " |      \n",
      " |          .. versionchanged:: 3.3.0\n",
      " |             Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      " |             This feature is experimental and unstable.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Executing a SQL query.\n",
      " |      \n",
      " |      >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  8|\n",
      " |      |  9|\n",
      " |      +---+\n",
      " |      \n",
      " |      Executing a SQL query with variables as Python formatter standard.\n",
      " |      \n",
      " |      >>> spark.sql(\n",
      " |      ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      " |      ... ).show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  8|\n",
      " |      +---+\n",
      " |      \n",
      " |      >>> mydf = spark.range(10)\n",
      " |      >>> spark.sql(\n",
      " |      ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      " |      ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      " |      +---+\n",
      " |      | id|\n",
      " |      +---+\n",
      " |      |  0|\n",
      " |      |  1|\n",
      " |      |  2|\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      \n",
      " |      >>> spark.sql('''\n",
      " |      ...   SELECT m1.a, m2.b\n",
      " |      ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      " |      ...   ON m1.key = m2.key\n",
      " |      ...   ORDER BY m1.a, m2.b''',\n",
      " |      ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      " |      ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      " |      +---+---+\n",
      " |      |  a|  b|\n",
      " |      +---+---+\n",
      " |      |  1|  3|\n",
      " |      |  2|  4|\n",
      " |      |  2|  5|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      " |      >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      " |      +---+---+\n",
      " |      |  A|  B|\n",
      " |      +---+---+\n",
      " |      |  1|  4|\n",
      " |      |  2|  4|\n",
      " |      |  3|  6|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  stop(self) -> None\n",
      " |      Stop the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getActiveSession() -> Union[ForwardRef('SparkSession'), NoneType] from builtins.type\n",
      " |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkSession`\n",
      " |          Spark session if an active session exists for the current thread\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = SparkSession.getActiveSession()\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> rdd = s.sparkContext.parallelize(l)\n",
      " |      >>> df = s.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.select(\"age\").collect()\n",
      " |      [Row(age=1)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  catalog\n",
      " |      Interface through which the user may create, drop, alter or query underlying\n",
      " |      databases, tables, functions, etc.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Catalog`\n",
      " |  \n",
      " |  conf\n",
      " |      Runtime configuration interface for Spark.\n",
      " |      \n",
      " |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      " |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      " |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pyspark.sql.conf.RuntimeConfig`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameReader`\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamReader`\n",
      " |  \n",
      " |  sparkContext\n",
      " |      Returns the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` instances active on `this` context.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StreamingQueryManager`\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`UDFRegistration`\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      " |      Builder for :class:`SparkSession`.\n",
      " |  \n",
      " |  __annotations__ = {'_activeSession': typing.ClassVar[typing.Union[Forw...\n",
      " |  \n",
      " |  builder = <pyspark.sql.session.SparkSession.Builder object>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af44921-c7eb-4fd1-afc0-a96c55145982",
   "metadata": {},
   "source": [
    "## Displaying Version of Spark:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20750a33-687e-4282-88bd-b1b85a939053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3979534-7fe6-4be3-9b59-79eafe0f1a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1ba54-02da-4a3f-a2c5-1224b6b63bda",
   "metadata": {},
   "source": [
    "## Creating dataframe using <b><i>spark.range()</i></b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83cd9925-9b92-4ae5-ab19-f53143b5258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7477e86a-1b39-42f8-998e-98285fb4240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5,10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "031e1cf2-90a8-48ae-b8dd-1ab535563e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  4|\n",
      "|  7|\n",
      "| 10|\n",
      "| 13|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1,16,3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7c92df-87b0-40c0-bcab-cb107b813ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(100).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a104dba-ec05-42d7-9c2f-e0d0365ac777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(100, numPartitions=5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ece5c-1ae0-4f8b-9623-903494745590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
