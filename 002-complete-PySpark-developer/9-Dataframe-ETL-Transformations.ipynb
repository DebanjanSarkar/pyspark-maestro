{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d74a6c-80fe-4fcd-87cd-302f6580ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c857cb5-989b-449a-9f2e-1f10c4ae4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrame ETL - Transformations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a96971b-84ea-425c-8227-7c202b5d0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddbb970-854c-4cab-a7f0-c18292b3ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DEBANJAN:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame ETL - Transformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21ef7a5f460>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e5d9e2-ce9e-44d0-aa48-ee2ecb13433b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "993e8fa3-b643-4b27-8865-de92ef122d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec91040-b9e0-48e0-9def-6a4adbfbad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"park.sql.shuffle.partitions\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b9d8c4-d8a8-4eac-9f50-dcfe93802694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.codegen.wholeStage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd8343d-b841-4488-a567-0ebe53caa226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'client'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.submit.deployMode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6e4d8-1c72-433d-9a01-02ada2ec3263",
   "metadata": {},
   "source": [
    "## Preparing Data that will be used in the examples:-\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf70fec1-1c90-437d-96cf-e1ac1fb346cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad248a8-6ceb-4fae-8e5b-9253ab53effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, TimestampType\n",
    "\n",
    "ord_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", TimestampType(), False),\n",
    "    StructField(\"customer_id\", LongType(), False),\n",
    "    StructField(\"order_status\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1487fa1-c183-4692-8f97-6a4a5d6c1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_df = spark.read.format(\"csv\").schema( ord_schema ).load(\"data/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4060fada-e77d-41cb-a19b-d46120eea153",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "        ('Robert', 35, 40, 40), ('Robert', 35, 40, 40),\n",
    "        ('Ram', 31, 33, 29), ('Ram', 31, 33, 91)\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame( data = emp_data, schema = [\"name\", \"score1\", \"score2\", \"score3\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f56fd534-f2b1-47ac-8824-e0fc444d415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 15:37:49|      11599|         CLOSED|\n",
      "|       2|2013-07-25 21:50:25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54e26c7-091e-47c5-b19b-412073fbfd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c7dd06-3e60-4c2e-8ac8-ec9580baf275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('customer_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e10e1a-01f9-4ca7-a03c-6fc66f66cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|  name|score1|score2|score3|\n",
      "+------+------+------+------+\n",
      "|Robert|    35|    40|    40|\n",
      "|Robert|    35|    40|    40|\n",
      "|   Ram|    31|    33|    29|\n",
      "|   Ram|    31|    33|    91|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69d68ec7-6d66-45c6-8471-05330e660bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- score1: long (nullable = true)\n",
      " |-- score2: long (nullable = true)\n",
      " |-- score3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070b888-6255-4355-91ba-67c2e7b691d7",
   "metadata": {},
   "source": [
    "# 1. Selection / Projection:\n",
    "-----------------------------\n",
    "- ***select( \\*cols )*** : Takes single or multiple column datatype objects as parameter.\n",
    "- ***selectExpr( \\*exprs )*** : Take SQL style expresssions for selection of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd5a31b-93cd-40d8-b0ef-4a576d2e0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|order_id|         order_date|\n",
      "+--------+-------------------+\n",
      "|       1|2013-07-25 00:00:00|\n",
      "|       2|2013-07-25 00:00:00|\n",
      "|       3|2013-07-25 00:00:00|\n",
      "|       4|2013-07-25 00:00:00|\n",
      "|       5|2013-07-25 00:00:00|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_id, ord_df.order_date ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4456d42e-dd10-4944-8c18-a4b5ce58ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "|   order_status|order_status_lowercased|\n",
      "+---------------+-----------------------+\n",
      "|         CLOSED|                 closed|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "|       COMPLETE|               complete|\n",
      "|         CLOSED|                 closed|\n",
      "|       COMPLETE|               complete|\n",
      "|       COMPLETE|               complete|\n",
      "|       COMPLETE|               complete|\n",
      "|     PROCESSING|             processing|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "| PAYMENT_REVIEW|         payment_review|\n",
      "|         CLOSED|                 closed|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "|     PROCESSING|             processing|\n",
      "|       COMPLETE|               complete|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "|       COMPLETE|               complete|\n",
      "|         CLOSED|                 closed|\n",
      "|PENDING_PAYMENT|        pending_payment|\n",
      "|     PROCESSING|             processing|\n",
      "+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( ord_df.order_status, F.lower(ord_df.order_status).alias(\"order_status_lowercased\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac10c94-c0fd-4121-a6dd-19034baf6562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------+\n",
      "|         order_date|date_of_order|time_of_order|\n",
      "+-------------------+-------------+-------------+\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "|2013-07-25 00:00:00|   2013-07-25|        00:00|\n",
      "+-------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.selectExpr( \"order_date\", \"date(order_date) AS date_of_order\", \"substring(order_date, 12, 5) AS time_of_order\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1783ed-016b-4ad3-8dbf-3dea40bb853b",
   "metadata": {},
   "source": [
    "## Unpivoting:-\n",
    "------------------\n",
    "- ***stack(n, \\*values)*** SQL function, to create a dataframe with ***n*** records, and fitting the whole data in excess of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccd16513-465d-47a5-b7b3-1425512ea457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_id = spark.range(1)\n",
    "df_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5641a12-7797-4903-b1f0-f7f52eae93e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|col0|col1|col2|col3|col4|\n",
      "+----+----+----+----+----+\n",
      "|   1|   2|   3|   4|   5|\n",
      "|   6|   7|   8|   9|  10|\n",
      "|  11|  12|  13|  14|null|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_id.selectExpr( \" stack(3, 1,2,3,4,5,6,7,8, 9, 10, 11, 12, 13, 14) \" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13836dc0-514a-4eaf-894a-1735f76bebad",
   "metadata": {},
   "source": [
    "# 2. Creating a new column / Transforming an existing column:-\n",
    "----------------------------------------------------------------\n",
    "***withColumn(column_name, \\<transformation> )*** - If ***column_name*** matches an existing column name, then the transformation will be applied to that row, else a new column wwill be created with the new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840242a6-8dbd-4bd9-be31-4a6b51b1c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9481357d-3336-429e-9897-919d34e481c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+----------+\n",
      "|order_id|         order_date|customer_id|   order_status|order_year|\n",
      "+--------+-------------------+-----------+---------------+----------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|      2013|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|      2013|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|      2013|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|      2013|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|      2013|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|      2013|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|      2013|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|      2013|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|      2013|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|      2013|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|      2013|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|      2013|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|      2013|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|      2013|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|      2013|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|      2013|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|      2013|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|      2013|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|      2013|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|      2013|\n",
      "+--------+-------------------+-----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.withColumn(\"order_year\", F.year(ord_df.order_date) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d024e8-cf62-413b-bb6a-d296a0123bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|      2013|      11599|         CLOSED|\n",
      "|       2|      2013|        256|PENDING_PAYMENT|\n",
      "|       3|      2013|      12111|       COMPLETE|\n",
      "|       4|      2013|       8827|         CLOSED|\n",
      "|       5|      2013|      11318|       COMPLETE|\n",
      "|       6|      2013|       7130|       COMPLETE|\n",
      "|       7|      2013|       4530|       COMPLETE|\n",
      "|       8|      2013|       2911|     PROCESSING|\n",
      "|       9|      2013|       5657|PENDING_PAYMENT|\n",
      "|      10|      2013|       5648|PENDING_PAYMENT|\n",
      "|      11|      2013|        918| PAYMENT_REVIEW|\n",
      "|      12|      2013|       1837|         CLOSED|\n",
      "|      13|      2013|       9149|PENDING_PAYMENT|\n",
      "|      14|      2013|       9842|     PROCESSING|\n",
      "|      15|      2013|       2568|       COMPLETE|\n",
      "|      16|      2013|       7276|PENDING_PAYMENT|\n",
      "|      17|      2013|       2667|       COMPLETE|\n",
      "|      18|      2013|       1205|         CLOSED|\n",
      "|      19|      2013|       9488|PENDING_PAYMENT|\n",
      "|      20|      2013|       9198|     PROCESSING|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Giving an alias same as an existing column name, to see that the existing column in the dataframe gets changed.\n",
    "\n",
    "ord_df.withColumn(\"order_date\", F.year(ord_df.order_date)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8354ab31-2e02-432b-a7aa-a0b61cff4bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+----------+-----------+---------+----------+------------+------------+\n",
      "|order_id|         order_date|customer_id|   order_status|order_year|order_month|order_day|order_hour|order_minute|order_second|\n",
      "+--------+-------------------+-----------+---------------+----------+-----------+---------+----------+------------+------------+\n",
      "|       1|2013-07-25 15:37:49|      11599|         CLOSED|      2013|          7|       25|        15|          37|          49|\n",
      "|       2|2013-07-25 21:50:25|        256|PENDING_PAYMENT|      2013|          7|       25|        21|          50|          25|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|      2013|          7|       25|         0|           0|           0|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|       6|2013-07-25 00:00:00|       7130|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|       7|2013-07-25 00:00:00|       4530|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|       8|2013-07-25 00:00:00|       2911|     PROCESSING|      2013|          7|       25|         0|           0|           0|\n",
      "|       9|2013-07-25 00:00:00|       5657|PENDING_PAYMENT|      2013|          7|       25|         0|           0|           0|\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|      2013|          7|       25|         0|           0|           0|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|      2013|          7|       25|         0|           0|           0|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|      2013|          7|       25|         0|           0|           0|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|      2013|          7|       25|         0|           0|           0|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|      2013|          7|       25|         0|           0|           0|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|      2013|          7|       25|         0|           0|           0|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|      2013|          7|       25|         0|           0|           0|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|      2013|          7|       25|         0|           0|           0|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|      2013|          7|       25|         0|           0|           0|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|      2013|          7|       25|         0|           0|           0|\n",
      "+--------+-------------------+-----------+---------------+----------+-----------+---------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select( \n",
    "    \"*\",\n",
    "    F.year(ord_df.order_date).alias(\"order_year\"),\n",
    "    F.month(ord_df.order_date).alias(\"order_month\"),\n",
    "    F.dayofmonth(ord_df.order_date).alias(\"order_day\"),\n",
    "    F.hour(ord_df.order_date).alias(\"order_hour\"),\n",
    "    F.minute(ord_df.order_date).alias(\"order_minute\"),\n",
    "    F.second(ord_df.order_date).alias(\"order_second\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a023316b-579a-44ee-b5bc-da1e3faab4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 15:37:49|      11599|         CLOSED|\n",
      "|       2|2013-07-25 21:50:25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|      11318|       COMPLETE|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f67f41-c6ab-423e-9733-3578675ebe89",
   "metadata": {},
   "source": [
    "### Dropping columns:-\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1196af8-5674-4444-9ff8-cf65c5345a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|order_id|customer_id|\n",
      "+--------+-----------+\n",
      "|       1|      11599|\n",
      "|       2|        256|\n",
      "|       3|      12111|\n",
      "|       4|       8827|\n",
      "|       5|      11318|\n",
      "|       6|       7130|\n",
      "|       7|       4530|\n",
      "|       8|       2911|\n",
      "|       9|       5657|\n",
      "|      10|       5648|\n",
      "|      11|        918|\n",
      "|      12|       1837|\n",
      "|      13|       9149|\n",
      "|      14|       9842|\n",
      "|      15|       2568|\n",
      "|      16|       7276|\n",
      "|      17|       2667|\n",
      "|      18|       1205|\n",
      "|      19|       9488|\n",
      "|      20|       9198|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.drop( \"order_date\", \"order_status\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045545fd-3b49-4ee2-9449-e178432ab9a1",
   "metadata": {},
   "source": [
    "### Dropping Duplicate Rows:-\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8dff7a0-1935-4e4c-adf7-0560a103b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|  name|score1|score2|score3|\n",
      "+------+------+------+------+\n",
      "|Robert|    35|    40|    40|\n",
      "|Robert|    35|    40|    40|\n",
      "|   Ram|    31|    33|    29|\n",
      "|   Ram|    31|    33|    91|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c59b32c-4675-4b08-a2c8-a8f049a8c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|  name|score1|score2|score3|\n",
      "+------+------+------+------+\n",
      "|Robert|    35|    40|    40|\n",
      "|   Ram|    31|    33|    29|\n",
      "|   Ram|    31|    33|    91|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc1a8f54-5fae-4aad-a682-ed274a5f69aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|  name|score1|score2|score3|\n",
      "+------+------+------+------+\n",
      "|Robert|    35|    40|    40|\n",
      "|   Ram|    31|    33|    29|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.dropDuplicates([\"name\", \"score1\", \"score2\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc7272-235d-40a7-b7e8-069f48649df0",
   "metadata": {},
   "source": [
    "# 3. Filtering Data:-\n",
    "----------------------\n",
    "1. Below methods/APIs are used to filter data, based on the condition provided. Both the methods works exactly same:\n",
    "- ***filter()***\n",
    "- ***where()***\n",
    "\n",
    "For defining multiple conditions, each condition should be inside () seperately, and they are to be combined using ***&*** , **|** symbols.\n",
    "\n",
    "2. Also, we can give any SQL type syntax to the above methods, where the whole SQL condition must be inside quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "802c521a-b225-42a7-85b0-51bf0be65530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting orders with ids between 10 and 20\n",
    "\n",
    "ord_df.filter( (ord_df.order_id>=10) & (ord_df.order_id<=20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8228d747-3bd3-4ac0-ae9c-5473f9ca88d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.where( (ord_df.order_id>=10) & (ord_df.order_id<=20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5108b106-f6f2-411a-8a40-5a1f1c25b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.where( ord_df.order_id.between(10,20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "653a9b83-23a4-4bbb-a955-10befdb3c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|      10|2013-07-25 00:00:00|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|       9198|     PROCESSING|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.where( ord_df.order_id.isin(10,11,12,13,14,15,16,17,18,19,20) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d76b852-d454-47bb-93a6-46e6fb1d9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|((order_id >= 10) AND (order_id <= 20))|\n",
      "+---------------------------------------+\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                  false|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "|                                   true|\n",
      "+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting will give boolean valued column only\n",
    "\n",
    "ord_df.select( (ord_df.order_id>=10) & (ord_df.order_id<=20) ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0c96-d816-4928-8973-d1d2fab00ad9",
   "metadata": {},
   "source": [
    "### Using SQL syntax:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6eff64e1-f5ba-492b-b94d-f46d5eb0f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+------------+\n",
      "|order_id|         order_date|customer_id|order_status|\n",
      "+--------+-------------------+-----------+------------+\n",
      "|       1|2013-07-25 15:37:49|      11599|      CLOSED|\n",
      "|       4|2013-07-25 00:00:00|       8827|      CLOSED|\n",
      "|      12|2013-07-25 00:00:00|       1837|      CLOSED|\n",
      "|      18|2013-07-25 00:00:00|       1205|      CLOSED|\n",
      "|      21|2013-07-25 00:00:00|       2711|     PENDING|\n",
      "|      24|2013-07-25 00:00:00|      11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:00|       9503|      CLOSED|\n",
      "|      36|2013-07-25 00:00:00|       5649|     PENDING|\n",
      "|      37|2013-07-25 00:00:00|       5863|      CLOSED|\n",
      "|      39|2013-07-25 00:00:00|       8214|     PENDING|\n",
      "|      42|2013-07-25 00:00:00|       9776|     PENDING|\n",
      "|      44|2013-07-25 00:00:00|      10500|     PENDING|\n",
      "|      49|2013-07-25 00:00:00|       1871|     PENDING|\n",
      "|      51|2013-07-25 00:00:00|      12271|      CLOSED|\n",
      "|      55|2013-07-25 00:00:00|       2052|     PENDING|\n",
      "|      57|2013-07-25 00:00:00|       7073|      CLOSED|\n",
      "|      61|2013-07-25 00:00:00|       4791|      CLOSED|\n",
      "|      62|2013-07-25 00:00:00|       9111|      CLOSED|\n",
      "|      68|2013-07-25 00:00:00|       4320|     PENDING|\n",
      "|      85|2013-07-25 00:00:00|       1485|     PENDING|\n",
      "+--------+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.where( \" order_status IN ('CLOSED', 'PENDING') \" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706954d-cbc3-4dcd-9bd3-52c660fa4440",
   "metadata": {},
   "source": [
    "# 4. Sorting records:-\n",
    "-----------------------\n",
    "- ***orderBy( col.asc() OR col.desc() OR col.asc_nulls_first() OR ... )***\n",
    "- ***sort()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9da25be7-a1da-4ee8-9676-1c52052f9e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|SUSPECTED_FRAUD|\n",
      "|     PROCESSING|\n",
      "|PENDING_PAYMENT|\n",
      "|        PENDING|\n",
      "| PAYMENT_REVIEW|\n",
      "|        ON_HOLD|\n",
      "|       COMPLETE|\n",
      "|         CLOSED|\n",
      "|       CANCELED|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select('order_status').distinct().orderBy( ord_df.order_status.desc() ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0c0a7d8-69a9-4cdd-b82e-5e22ac02f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   order_status|\n",
      "+---------------+\n",
      "|SUSPECTED_FRAUD|\n",
      "|     PROCESSING|\n",
      "|PENDING_PAYMENT|\n",
      "|        PENDING|\n",
      "| PAYMENT_REVIEW|\n",
      "|        ON_HOLD|\n",
      "|       COMPLETE|\n",
      "|         CLOSED|\n",
      "|       CANCELED|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.select('order_status').distinct().sort( ord_df.order_status.desc() ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0efa-2613-4850-87d8-b209cc8f8125",
   "metadata": {},
   "source": [
    "Sorting based on multiple columns, progressively:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0db5901e-16f0-4626-b8ff-8e7de1ed35b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|      69|2013-07-25 00:00:00|       2821|SUSPECTED_FRAUD|\n",
      "|     117|2013-07-26 00:00:00|         58|SUSPECTED_FRAUD|\n",
      "|     246|2013-07-26 00:00:00|       9616|SUSPECTED_FRAUD|\n",
      "|     320|2013-07-26 00:00:00|      10698|SUSPECTED_FRAUD|\n",
      "|     329|2013-07-26 00:00:00|       1944|SUSPECTED_FRAUD|\n",
      "|     411|2013-07-27 00:00:00|      11164|SUSPECTED_FRAUD|\n",
      "|     423|2013-07-27 00:00:00|       9632|SUSPECTED_FRAUD|\n",
      "|     453|2013-07-27 00:00:00|        381|SUSPECTED_FRAUD|\n",
      "|     548|2013-07-28 00:00:00|       6889|SUSPECTED_FRAUD|\n",
      "|     580|2013-07-28 00:00:00|       8677|SUSPECTED_FRAUD|\n",
      "|     587|2013-07-28 00:00:00|       6008|SUSPECTED_FRAUD|\n",
      "|     818|2013-07-29 00:00:00|       8459|SUSPECTED_FRAUD|\n",
      "|     830|2013-07-29 00:00:00|       9103|SUSPECTED_FRAUD|\n",
      "|     909|2013-07-30 00:00:00|       8670|SUSPECTED_FRAUD|\n",
      "|     932|2013-07-30 00:00:00|       7231|SUSPECTED_FRAUD|\n",
      "|     940|2013-07-30 00:00:00|       5098|SUSPECTED_FRAUD|\n",
      "|     963|2013-07-30 00:00:00|      10558|SUSPECTED_FRAUD|\n",
      "|     982|2013-07-30 00:00:00|       2873|SUSPECTED_FRAUD|\n",
      "|     993|2013-07-30 00:00:00|       5122|SUSPECTED_FRAUD|\n",
      "|    1002|2013-07-30 00:00:00|       5170|SUSPECTED_FRAUD|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.orderBy( ord_df.order_status.desc(), ord_df.order_id.asc() ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48153b51-1410-4582-934c-f745cba4371c",
   "metadata": {},
   "source": [
    "### Sorting the records within its own partition:-\n",
    "--------------------------------------------------------\n",
    "- ***sortWithinPartitions()*** - Sorts the records in each partition, and there is no shuffling accross partitions. Extracting the data sorted by this method creates a separate file for each partition, and each file contains sorted records locally (not globally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5715871e-727e-4d26-9b5c-76161ad5aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    (1,'cap'), (2,'mall'), (3,'jupyter'), (4,'god'), (5, 'ball'), (6, 'zebra'), (7, 'wood'), (8, 'dungeon')\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame( data = test_data, schema = [\"id\", \"word\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a3c802-c181-484a-94d5-e6734de22600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   word|\n",
      "+---+-------+\n",
      "|  1|    cap|\n",
      "|  2|   mall|\n",
      "|  3|jupyter|\n",
      "|  4|    god|\n",
      "|  5|   ball|\n",
      "|  6|  zebra|\n",
      "|  7|   wood|\n",
      "|  8|dungeon|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c98a8a66-477f-46e3-8a74-02045e964fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67baec5a-2b54-4075-b922-15f2ac25476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d7858d8-a721-4bd5-aa25-4c12dba97d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeab07a3-5038-4c89-8c1f-ca1c47c252bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9047c84e-6826-4ba8-a3a2-01bd2e2103f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=3, word='jupyter'),\n",
       "  Row(id=4, word='god'),\n",
       "  Row(id=5, word='ball'),\n",
       "  Row(id=6, word='zebra'),\n",
       "  Row(id=7, word='wood')],\n",
       " [Row(id=1, word='cap'), Row(id=2, word='mall'), Row(id=8, word='dungeon')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23580671-2f7f-4df7-ad13-bed727462b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sortWithinPartitions( test_df.word.asc() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee617354-a155-4cdb-9ffb-3db2618a0815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=5, word='ball'),\n",
       "  Row(id=4, word='god'),\n",
       "  Row(id=3, word='jupyter'),\n",
       "  Row(id=7, word='wood'),\n",
       "  Row(id=6, word='zebra')],\n",
       " [Row(id=1, word='cap'), Row(id=8, word='dungeon'), Row(id=2, word='mall')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e381c1-12a2-4ca0-909d-dbaa8ff002c8",
   "metadata": {},
   "source": [
    "# 5. Set Opeartions:-\n",
    "-------------------------\n",
    "- ***union()*** - Keeps duplicates from spark 2.4+\n",
    "- ***unionAll()*** - Keeps duplicates\n",
    "- ***unionByName()*** - Keeps duplicates\n",
    "- ***intersect()*** - Removes duplicates\n",
    "- ***intersectAll()*** - Keeps duplicates\n",
    "- ***exceptAll()*** - Keeps duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2ebe202-d53d-4cf0-9a39-cb797d4012ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data1 = [(1,'a'), (1,'a'), (2,'b'), (2,'b'), (3,'c')]\n",
    "set_data2 = [(1,'a'), (1,'a'), (4,'d'), (5,'e')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ee5ceec-da9f-45bb-ae87-db9f09affd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_df1 = spark.createDataFrame( data = set_data1, schema = [\"id\", \"letter\"] )\n",
    "set_df2 = spark.createDataFrame( data = set_data2, schema = [\"id\", \"letter\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "801878c8-e9ee-4bd7-b451-832956af29f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "795213ec-ee4d-4adb-b637-7fd9cc989ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  4|     d|\n",
      "|  5|     e|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35a90c0e-c5b8-4bcb-a75e-deb374afd795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  4|     d|\n",
      "|  5|     e|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.union( set_df2 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bb3106f-7d67-46ab-8c07-2e234d456e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "|  4|     d|\n",
      "|  5|     e|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.unionAll(set_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "daad63b9-7eb6-485a-b229-7499aca348e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|letter| id|\n",
      "+------+---+\n",
      "|     a|  1|\n",
      "|     a|  1|\n",
      "|     b|  2|\n",
      "|     c|  3|\n",
      "|     1|  a|\n",
      "|     1|  a|\n",
      "|     4|  d|\n",
      "|     5|  e|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.select(\"letter\", \"id\").union(set_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5cf9168-90dd-44ee-b326-cfae6b2eaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|letter| id|\n",
      "+------+---+\n",
      "|     a|  1|\n",
      "|     a|  1|\n",
      "|     b|  2|\n",
      "|     c|  3|\n",
      "|     a|  1|\n",
      "|     a|  1|\n",
      "|     d|  4|\n",
      "|     e|  5|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.select(\"letter\", \"id\").unionByName(set_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fa559d7-635e-41f2-b99f-b8b16d27612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.intersect(set_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7254d7c-d93c-48ee-8b79-80f028042473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  1|     a|\n",
      "|  1|     a|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.intersectAll(set_df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3629ce8f-857c-4f56-9b81-990595db954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|letter|\n",
      "+---+------+\n",
      "|  2|     b|\n",
      "|  2|     b|\n",
      "|  3|     c|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_df1.exceptAll(set_df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5a08c-513f-40e0-a26c-2c55f738886e",
   "metadata": {},
   "source": [
    "# 6. Joins:\n",
    "-------------\n",
    "- ***df1.join(df2, df1.col == df2.col, how = inner/outer/left/right/....)***\n",
    "- ***df1.crossJoin(df2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e09de198-8979-4209-9228-52a3993179e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    (1,'Robert'), (2,'Ria'), (3, 'James')\n",
    "]\n",
    "\n",
    "country_data = [(2,'USA'), (4,'India')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f15f71c-764c-47e9-a2cf-6c5dd51b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame( data = emp_data, schema = [\"empid\", \"empname\"] )\n",
    "country_df = spark.createDataFrame( data = country_data, schema = [\"empid\", \"country\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c34b096-e4e0-4b2b-bc96-0cb054eb8661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empid|empname|\n",
      "+-----+-------+\n",
      "|    1| Robert|\n",
      "|    2|    Ria|\n",
      "|    3|  James|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ba68be2-61c0-4e0d-9a72-f8ede874f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empid|country|\n",
      "+-----+-------+\n",
      "|    2|    USA|\n",
      "|    4|  India|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228797f5-1813-402e-bc61-c89a0d171ba2",
   "metadata": {},
   "source": [
    "### a. Inner Join:-\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e685b4d4-53e9-49dd-9659-5ac2ad17f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    2|    Ria|    2|    USA|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'inner' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63d529-a393-4cde-95ec-bf07de1f99d6",
   "metadata": {},
   "source": [
    "### b. Left Outer Join / Left Join:-\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30bdc7cb-b467-47ad-87e1-37cbbce1c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert| null|   null|\n",
      "|    2|    Ria|    2|    USA|\n",
      "|    3|  James| null|   null|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'left' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c37eb2-091e-4f47-8b6e-80677c4bd70a",
   "metadata": {},
   "source": [
    "### c. Right Outer Join / Right Join:-\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f32061b-1744-44c9-85b4-aa5c9179b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    2|    Ria|    2|    USA|\n",
      "| null|   null|    4|  India|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'right' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c769d-0ce4-4fa5-9a05-0fb821e8bda2",
   "metadata": {},
   "source": [
    "### d. Full Outer Join :-\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef82d9d-88c9-476c-b04b-41f2488e9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert| null|   null|\n",
      "|    2|    Ria|    2|    USA|\n",
      "|    3|  James| null|   null|\n",
      "| null|   null|    4|  India|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'outer' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073ee4e-bf92-4949-bf5d-627e864d3c81",
   "metadata": {},
   "source": [
    "### e. Cross Join:-\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52458bfc-43b9-4b13-b222-e911254d72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert|    2|    USA|\n",
      "|    1| Robert|    4|  India|\n",
      "|    2|    Ria|    2|    USA|\n",
      "|    2|    Ria|    4|  India|\n",
      "|    3|  James|    2|    USA|\n",
      "|    3|  James|    4|  India|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == emp_df.empid, 'cross' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d11ed9-d095-49bc-af21-cc80a1f367a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+\n",
      "|empid|empname|empid|country|\n",
      "+-----+-------+-----+-------+\n",
      "|    1| Robert|    2|    USA|\n",
      "|    1| Robert|    4|  India|\n",
      "|    2|    Ria|    2|    USA|\n",
      "|    2|    Ria|    4|  India|\n",
      "|    3|  James|    2|    USA|\n",
      "|    3|  James|    4|  India|\n",
      "+-----+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.crossJoin(country_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037d8f3-0eca-4afc-88fc-110a42700be6",
   "metadata": {},
   "source": [
    "### f. Left Anti Join:-\n",
    "-------------------------\n",
    "Gives the records that are present in the left table, but not in the right table, based on the matching condition.\n",
    "\n",
    "Basically,\n",
    "## (Left Anti Join Records) = (Left Join Records) - (Inner Join Records)\n",
    "\n",
    "Only the columns present in the left table will be shown, as all the right table columns will contan NULL, thus of no use even if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ef25799-a9c8-48c1-af0a-46ffa7ae9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empid|empname|\n",
      "+-----+-------+\n",
      "|    1| Robert|\n",
      "|    3|  James|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'left_anti' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646da7d5-858c-4cf1-8aa5-e446d767e65a",
   "metadata": {},
   "source": [
    "### g. Left Semi Join:-\n",
    "-------------------------\n",
    "\n",
    "Join type followed is basicaly the inner join. After joining, only the columns of the left table will be in the result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24cb3123-9808-47c5-beaf-e4f28f017ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|empid|empname|\n",
      "+-----+-------+\n",
      "|    2|    Ria|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join( country_df, emp_df.empid == country_df.empid, 'left_semi' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74d99d-ac82-4f07-80c7-81e20a7a44da",
   "metadata": {},
   "source": [
    "### h. Self Join :-\n",
    "--------------------\n",
    "There is no specific function or API for this join. This a conceptual join, that is accomplished using alias for same dataframe, and then joining them using any of the join types seen above.\n",
    "\n",
    "- In selft Join, as we use same dataframe as both left and right table, we must set alias for the tables, such that in the join condition, columns of both the dataframes ca be diistinguished.\n",
    "\n",
    "- As the column names will be accessed by the name of the alias, and not directly by the dataframe name, the column names must be placed as string inside the ***pyspark.sql.functions.col()*** function, else the columns cannot be detected and will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc719849-49b8-4908-a6bc-35c49d06a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data2 = [\n",
    "    (1,'Robert', 2), (2,'Ria', 3), (3, 'James', 5)\n",
    "]\n",
    "\n",
    "emp_df2 = spark.createDataFrame( data = emp_data2, schema = [\"empid\", \"empname\", \"bossid\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a107fe3f-9379-45df-9e13-1dd0f53e4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "|empid|empname|bossid|\n",
      "+-----+-------+------+\n",
      "|    1| Robert|     2|\n",
      "|    2|    Ria|     3|\n",
      "|    3|  James|     5|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb1123-089f-42d8-ac64-cad4cea5b5b6",
   "metadata": {},
   "source": [
    "#### Inner Self Join:-\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b4226ab-a195-4ce5-bcd3-cc06db7949eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+-----+-------+------+\n",
      "|empid|empname|bossid|empid|empname|bossid|\n",
      "+-----+-------+------+-----+-------+------+\n",
      "|    1| Robert|     2|    2|    Ria|     3|\n",
      "|    2|    Ria|     3|    3|  James|     5|\n",
      "+-----+-------+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.alias(\"t1\").join( emp_df2.alias(\"t2\"), F.col(\"t1.bossid\") == F.col(\"t2.empid\"), 'inner' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9812f-55a2-4ea2-a47f-1a08a53d7ff8",
   "metadata": {},
   "source": [
    "#### Inner Left Join:-\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca4063b5-8eb0-4bd9-820c-7205e0bb8614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+-----+-------+------+\n",
      "|empid|empname|bossid|empid|empname|bossid|\n",
      "+-----+-------+------+-----+-------+------+\n",
      "|    1| Robert|     2|    2|    Ria|     3|\n",
      "|    2|    Ria|     3|    3|  James|     5|\n",
      "|    3|  James|     5| null|   null|  null|\n",
      "+-----+-------+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.alias(\"t1\").join( emp_df2.alias(\"t2\"), F.col(\"t1.bossid\") == F.col(\"t2.empid\"), \"left\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c256a8c2-92c4-48f9-9e4a-ba0180971fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------------+\n",
      "|empid|empname|bossid|manager_name|\n",
      "+-----+-------+------+------------+\n",
      "|    1| Robert|     2|         Ria|\n",
      "|    2|    Ria|     3|       James|\n",
      "|    3|  James|     5|        null|\n",
      "+-----+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df2.alias(\"t1\").join( emp_df2.alias(\"t2\"), F.col(\"t1.bossid\") == F.col(\"t2.empid\"), \"left\" ).select( \n",
    "    F.col(\"t1.empid\"), \n",
    "    F.col(\"t1.empname\"),\n",
    "    F.col(\"t1.bossid\"),\n",
    "    F.col(\"t2.empname\").alias(\"manager_name\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60b73c-d51d-487b-b84a-aa6f0adf0b66",
   "metadata": {},
   "source": [
    "### i. Multi Column Join :-\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbce7ae0-8b69-40ab-84e0-37da0586670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stud_data = [\n",
    "    (1,'Debanjan', 95), (2,'Projna', 89), (3,'Proman', 77), (4, 'Shaman', 65)\n",
    "]\n",
    "\n",
    "marks_data = [\n",
    "    (100,91,'A+'), (90,81,'A'), (80,71,'B'), (70,61,'C')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5775b3ab-0096-4902-978b-ad0e332477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stud_df = spark.createDataFrame( data = stud_data, schema = [\"id\", \"name\", \"marks\"] )\n",
    "\n",
    "marks_df = spark.createDataFrame( data = marks_data, schema = [\"min_marks\", \"max_marks\", \"grade\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05e4a7b-4435-444b-84b1-e6d0e7bf95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    name|marks|\n",
      "+---+--------+-----+\n",
      "|  1|Debanjan|   95|\n",
      "|  2|  Projna|   89|\n",
      "|  3|  Proman|   77|\n",
      "|  4|  Shaman|   65|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95471e43-ed49-4a32-a649-bdaf8925c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n",
      "|min_marks|max_marks|grade|\n",
      "+---------+---------+-----+\n",
      "|      100|       91|   A+|\n",
      "|       90|       81|    A|\n",
      "|       80|       71|    B|\n",
      "|       70|       61|    C|\n",
      "+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8120eb1d-dbc4-429d-9278-74fa238ad267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+---------+-----+\n",
      "| id|    name|marks|min_marks|max_marks|grade|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "|  1|Debanjan|   95|     null|     null| null|\n",
      "|  2|  Projna|   89|     null|     null| null|\n",
      "|  3|  Proman|   77|     null|     null| null|\n",
      "|  4|  Shaman|   65|     null|     null| null|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stud_df.join( marks_df, ( ( stud_df.marks >= marks_df.min_marks  ) & ( stud_df.marks <= marks_df.max_marks ) ) , 'left' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28756e0c-5198-4674-8af1-db3453e167d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+---------+-----+\n",
      "| id|    name|marks|min_marks|max_marks|grade|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "|  1|Debanjan|   95|     null|     null| null|\n",
      "|  2|  Projna|   89|     null|     null| null|\n",
      "|  3|  Proman|   77|     null|     null| null|\n",
      "|  4|  Shaman|   65|     null|     null| null|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stud_df.join( marks_df, stud_df.marks.between( marks_df.min_marks, marks_df.max_marks ) , 'left' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c8a67ef-c909-4594-800c-d2aad5fd07f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5c33233-4d2e-47c7-9916-01792e3d5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- min_marks: long (nullable = true)\n",
      " |-- max_marks: long (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d90ab64-51c5-4a7f-8c46-a0f5785272ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---------+---------+-----+\n",
      "| id|    name|marks|min_marks|max_marks|grade|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "|  1|Debanjan|   95|     null|     null| null|\n",
      "|  2|  Projna|   89|     null|     null| null|\n",
      "|  3|  Proman|   77|     null|     null| null|\n",
      "|  4|  Shaman|   65|     null|     null| null|\n",
      "+---+--------+-----+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stud_df.join(\n",
    "    marks_df,\n",
    "    (stud_df[\"marks\"] >= marks_df[\"min_marks\"]) & (stud_df[\"marks\"] <= marks_df[\"max_marks\"]),\n",
    "    \"left\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075c437-218b-40f8-83b1-3cdfc3bd2bf4",
   "metadata": {},
   "source": [
    "# 7. Aggregation:-\n",
    "-------------------\n",
    "- ***summary()*** - Returns a dataframe containing data like count, mean, min, max, standard deviation, percentages, for the dataframe on which this method is applied.\n",
    "- The rest aggregation functions like ***min(), max(), avg()*** are present in ***pyspark.sql.functions*** module, and they can only be applied inside ***select()*** or ***agg()*** dataframe methods. They take the column name as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77d2966b-ae7d-4cbb-ae1b-d490ee19144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType\n",
    "\n",
    "ordItems_schema = StructType([\n",
    "    StructField(\"item_id\", LongType(), False),\n",
    "    StructField(\"order_id\", LongType(), False),\n",
    "    StructField(\"product_id\", LongType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"subtotal\", DoubleType(), False),\n",
    "    StructField(\"price\", DoubleType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9cc0f4-b85a-47e0-a988-1a70f382c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordItems = spark.read.csv(\"./data/orderItems.csv\", schema = ordItems_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6e38859-f7f2-46c3-946d-ca41d2c83d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------+--------+------+\n",
      "|item_id|order_id|product_id|quantity|subtotal| price|\n",
      "+-------+--------+----------+--------+--------+------+\n",
      "|      1|       1|       957|       1|  299.98|299.98|\n",
      "|      2|       2|      1073|       1|  199.99|199.99|\n",
      "|      3|       2|       502|       5|   250.0|  50.0|\n",
      "|      4|       2|       403|       1|  129.99|129.99|\n",
      "|      5|       4|       897|       2|   49.98| 24.99|\n",
      "|      6|       4|       365|       5|  299.95| 59.99|\n",
      "|      7|       4|       502|       3|   150.0|  50.0|\n",
      "|      8|       4|      1014|       4|  199.92| 49.98|\n",
      "|      9|       5|       957|       1|  299.98|299.98|\n",
      "|     10|       5|       365|       5|  299.95| 59.99|\n",
      "|     11|       5|      1014|       2|   99.96| 49.98|\n",
      "|     12|       5|       957|       1|  299.98|299.98|\n",
      "|     13|       5|       403|       1|  129.99|129.99|\n",
      "|     14|       7|      1073|       1|  199.99|199.99|\n",
      "|     15|       7|       957|       1|  299.98|299.98|\n",
      "|     16|       7|       926|       5|   79.95| 15.99|\n",
      "|     17|       8|       365|       3|  179.97| 59.99|\n",
      "|     18|       8|       365|       5|  299.95| 59.99|\n",
      "|     19|       8|      1014|       4|  199.92| 49.98|\n",
      "|     20|       8|       502|       1|    50.0|  50.0|\n",
      "+-------+--------+----------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordItems.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01a3064d-2b34-4ed0-8fd9-9167af6b0643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordItems.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3abd6288-e696-4c9e-97cc-546e7df909a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- subtotal: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordItems.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe88e329-f357-4538-8d13-1756709192da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|          item_id|          order_id|       product_id|          quantity|          subtotal|            price|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|           172198|            172198|           172198|            172198|            172198|           172198|\n",
      "|   mean|          86099.5| 34442.56682423721|660.4877176273824|2.1821275508426345|199.32066533874877|133.7590662494616|\n",
      "| stddev|49709.42516431533|19883.325171992343| 310.514472790008|1.4663523175387134|112.74303721400686|118.5589325726674|\n",
      "|    min|                1|                 1|               19|                 1|              9.99|             9.99|\n",
      "|    25%|            43033|             17204|              403|                 1|            119.98|             50.0|\n",
      "|    50%|            86085|             34464|              627|                 1|            199.92|            59.99|\n",
      "|    75%|           129134|             51682|             1004|                 3|            299.95|           199.99|\n",
      "|    max|           172198|             68883|             1073|                 5|           1999.99|          1999.99|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordItems.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49de9a37-a4c3-4f60-8192-0fc4f09587e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|          quantity|            price|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|            172198|           172198|\n",
      "|   mean|2.1821275508426345|133.7590662494616|\n",
      "| stddev|1.4663523175387134|118.5589325726674|\n",
      "|    min|                 1|             9.99|\n",
      "|    25%|                 1|             50.0|\n",
      "|    50%|                 1|            59.99|\n",
      "|    75%|                 3|           199.99|\n",
      "|    max|                 5|          1999.99|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying summary() method on a subset of columns\n",
    "\n",
    "ordItems.select( 'quantity', 'price' ).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b276950d-f495-46f1-908a-a7b50e489090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------------------+-------------+-------------+\n",
      "|order_id|        avg(price)|round(avg(subtotal), 2)|min(subtotal)|max(subtotal)|\n",
      "+--------+------------------+-----------------------+-------------+-------------+\n",
      "|       1|            299.98|                 299.98|       299.98|       299.98|\n",
      "|       2|126.66000000000001|                 193.33|       129.99|        250.0|\n",
      "|       4|             46.24|                 174.96|        49.98|       299.95|\n",
      "|       5|           167.984|                 225.97|        99.96|       299.98|\n",
      "|       7|171.98666666666668|                 193.31|        79.95|       299.98|\n",
      "|       8|             54.99|                 182.46|         50.0|       299.95|\n",
      "|       9|166.65666666666667|                 199.99|       199.98|       199.99|\n",
      "|      10|           120.388|                 130.38|        21.99|       199.99|\n",
      "|      11|59.986000000000004|                 183.96|        49.98|       399.96|\n",
      "|      12|104.99000000000001|                 259.97|        100.0|       499.95|\n",
      "|      13|             31.99|                 127.96|       127.96|       127.96|\n",
      "|      14|166.65333333333334|                 183.31|         50.0|       399.98|\n",
      "|      15|148.39000000000001|                 185.18|         50.0|       399.98|\n",
      "|      16|             59.99|                 209.97|       119.98|       299.95|\n",
      "|      17|            70.988|                 138.97|        74.97|       239.96|\n",
      "|      18|            129.99|                 149.99|       119.98|       199.99|\n",
      "|      19|            349.98|                 349.98|       299.98|       399.98|\n",
      "|      20|             72.49|                 219.96|       129.99|       299.95|\n",
      "|      21|           35.4925|                  93.23|        69.98|       127.96|\n",
      "|      23|            299.98|                 299.98|       299.98|       299.98|\n",
      "+--------+------------------+-----------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find average price and average subtotal of all the orders:-\n",
    "\n",
    "ordItems.groupBy(\"order_id\").agg( \n",
    "    F.avg(\"price\"), \n",
    "    F.round(F.avg(\"subtotal\"), 2), \n",
    "    F.min(\"subtotal\"), \n",
    "    F.max(\"subtotal\") \n",
    ").orderBy( ordItems.order_id.asc() ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db412a59-8184-4550-82c0-87de0b4b0ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+----------+\n",
      "|min(subtotal)|max(subtotal)|min(price)|max(price)|\n",
      "+-------------+-------------+----------+----------+\n",
      "|         9.99|      1999.99|      9.99|   1999.99|\n",
      "+-------------+-------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordItems.agg( F.min(\"subtotal\"), F.max(\"subtotal\"), F.min(\"price\"), F.max(\"price\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a96297c4-7a31-4d2c-9e2d-8aa05f5d5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|round(avg(price), 2)|min(price)|max(price)|\n",
      "+--------------------+----------+----------+\n",
      "|              133.76|      9.99|   1999.99|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordItems.select( F.round(F.avg(\"price\"), 2), F.min(\"price\"), F.max(\"price\")  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b428752-8468-41af-bf23-4d6db929fd56",
   "metadata": {},
   "source": [
    "- ***sum()***\n",
    "- ***sumDistinct()*** - deprecated, and the newer version of this function is ***sum_distinct()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3657f5c-64d6-4aac-ada6-a22f434acd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.createDataFrame( data = [(1,),(2,),(3,),(3,),(4,),(5,)], schema = [\"value\"] )\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a29d0790-ac1b-4265-990a-bde20768c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\functions.py:316: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|sum(value)|sum(DISTINCT value)|\n",
      "+----------+-------------------+\n",
      "|        18|                 15|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.sum(\"value\"), F.sumDistinct(\"value\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b4d78d-4ca9-4522-ae44-16bd41ab4f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|sum(value)|sum(DISTINCT value)|\n",
      "+----------+-------------------+\n",
      "|        18|                 15|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.sum(\"value\"), F.sum_distinct(\"value\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537676d5-359e-43cc-9acd-1f12c529ec2b",
   "metadata": {},
   "source": [
    "- ***count()***\n",
    "- ***countDistinct()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "587cd336-d9e4-43bc-958a-26b49a9c331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+\n",
      "|count(value)|count(DISTINCT value)|\n",
      "+------------+---------------------+\n",
      "|           6|                    5|\n",
      "+------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.count(\"value\"), F.countDistinct(\"value\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d23f5c1-13d2-4c37-b31c-17058cac8cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------+\n",
      "|count|count_distinct|countDistinct|\n",
      "+-----+--------------+-------------+\n",
      "|    6|             5|            5|\n",
      "+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.agg( \n",
    "    F.count(\"value\").alias(\"count\"), \n",
    "    F.count_distinct(\"value\").alias(\"count_distinct\"), \n",
    "    F.countDistinct(\"value\").alias(\"countDistinct\") \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0aff59-24d5-4d57-8b29-dcb4730ee8f6",
   "metadata": {},
   "source": [
    "### Aggregating all values into a collection:-\n",
    "------------------------------------------------\n",
    "- ***collect_set()*** - Aggregates all the values in a group(if ***groupBy()*** is applied), or all values in a column, into a Python set (thus, does not contains duplicates).\n",
    "- ***collect_list()*** - Aggregates all the values in a group(if ***groupBy()*** is applied), or all values in a column, into a Python list (contains duplicates).\n",
    "\n",
    "As in the result set the attributes actually contaiin Python collections, thus they can be indexed also. But, Slicing dicing operation is ***not applicable***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30998750-d0ee-44c8-bd4c-984a0ec212c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|collect_set(value)|collect_list(value)|\n",
      "+------------------+-------------------+\n",
      "|   [1, 5, 2, 3, 4]| [1, 2, 3, 3, 4, 5]|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.collect_set(\"value\"), F.collect_list(\"value\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1db1b44c-6556-42cc-a8ce-0e11e846d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(value)[0]|collect_list(value)[0]|\n",
      "+---------------------+----------------------+\n",
      "|                    1|                     1|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.collect_set(\"value\")[0], F.collect_list(\"value\")[0] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52cfe769-7731-4297-a8a7-29da5cc50dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'substring(collect_set(value), 0, 3)' due to data type mismatch: argument 1 requires (string or binary) type, however, 'collect_set(value)' is of array<bigint> type.;\n'Aggregate [unresolvedalias(substring(collect_set(value#1608L, 0, 0), 0, 3), Some(org.apache.spark.sql.Column$$Lambda$3915/0x0000000101651040@60fa347e)), unresolvedalias(substring(collect_list(value#1608L, 0, 0), 0, 3), Some(org.apache.spark.sql.Column$$Lambda$3915/0x0000000101651040@60fa347e))]\n+- LogicalRDD [value#1608L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEBANJ~1\\AppData\\Local\\Temp/ipykernel_6116/1614692324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Apache_Spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'substring(collect_set(value), 0, 3)' due to data type mismatch: argument 1 requires (string or binary) type, however, 'collect_set(value)' is of array<bigint> type.;\n'Aggregate [unresolvedalias(substring(collect_set(value#1608L, 0, 0), 0, 3), Some(org.apache.spark.sql.Column$$Lambda$3915/0x0000000101651040@60fa347e)), unresolvedalias(substring(collect_list(value#1608L, 0, 0), 0, 3), Some(org.apache.spark.sql.Column$$Lambda$3915/0x0000000101651040@60fa347e))]\n+- LogicalRDD [value#1608L], false\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.collect_set(\"value\")[0:3], F.collect_list(\"value\")[0:3] ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bc20e-4a37-4644-9649-38bcbf9488f2",
   "metadata": {},
   "source": [
    "### Functions related to statistical analysis:\n",
    "------------------------------------------------\n",
    "- ***skewness()***\n",
    "- ***variance()***\n",
    "- ***stddev()*** - Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7d508e2-e6ca-4032-a6ee-d45d4f437b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+\n",
      "|skewness(value)|   var_samp(value)|stddev_samp(value)|\n",
      "+---------------+------------------+------------------+\n",
      "|            0.0|1.9999999999999996| 1.414213562373095|\n",
      "+---------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select( F.skewness(\"value\"), F.variance(\"value\"), F.stddev(\"value\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24b4ad-82c5-4f2c-844b-1047a35e23e8",
   "metadata": {},
   "source": [
    "# 8. Grouping and Pivot:-\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c0ae34b-926b-4c7e-896f-6d0fc9591135",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    (\"James\", \"Sales\", \"NY\", 9000,34), \n",
    "    (\"Alicia\", \"Sales\", \"NY\", 8600,56), \n",
    "    (\"Robert\", \"Sales\", \"CA\", 8100, 30), \n",
    "    (\"Lisa\", \"Finance\", \"CA\", 9000, 24), \n",
    "    (\"Deja\", \"Finance\", \"CA\", 9900, 40), \n",
    "    (\"Sugie\", \"Finance\", \"NY\", 8300, 36), \n",
    "    (\"Ram\", \"Finance\", \"NY\", 7900,53), \n",
    "    (\"Kyle\", \"Marketing\", \"CA\", 8000, 25), \n",
    "    (\"Reid\", \"Marketing\", \"NY\", 9100, 50)\n",
    "]\n",
    "\n",
    "emp_schema = [\"empname\", \"dept\", \"state\", \"salary\", \"age\"]\n",
    "\n",
    "emp_df = spark.createDataFrame( data = emp_data, schema = emp_schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "622ff418-2962-4dc7-b8b4-48c688876860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+------+---+\n",
      "|empname|     dept|state|salary|age|\n",
      "+-------+---------+-----+------+---+\n",
      "|  James|    Sales|   NY|  9000| 34|\n",
      "| Alicia|    Sales|   NY|  8600| 56|\n",
      "| Robert|    Sales|   CA|  8100| 30|\n",
      "|   Lisa|  Finance|   CA|  9000| 24|\n",
      "|   Deja|  Finance|   CA|  9900| 40|\n",
      "|  Sugie|  Finance|   NY|  8300| 36|\n",
      "|    Ram|  Finance|   NY|  7900| 53|\n",
      "|   Kyle|Marketing|   CA|  8000| 25|\n",
      "|   Reid|Marketing|   NY|  9100| 50|\n",
      "+-------+---------+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3515a27d-b9e1-419b-a649-f423b58931d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GroupedData in module pyspark.sql.group object:\n",
      "\n",
      "class GroupedData(pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin)\n",
      " |  GroupedData(jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      " |  \n",
      " |  A set of methods for aggregations on a :class:`DataFrame`,\n",
      " |  created by :func:`DataFrame.groupBy`.\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GroupedData\n",
      " |      pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jgd: py4j.java_gateway.JavaObject, df: pyspark.sql.dataframe.DataFrame)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Compute aggregates and returns the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      The available aggregate functions can be:\n",
      " |      \n",
      " |      1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`\n",
      " |      \n",
      " |      2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |         .. note:: There is no partial aggregation with group aggregate UDFs, i.e.,\n",
      " |             a full shuffle is required. Also, all the data of a group will be loaded into\n",
      " |             memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      " |             and certain groups are too large to fit in memory.\n",
      " |      \n",
      " |         .. seealso:: :func:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |      If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\n",
      " |      is the column to perform aggregation on, and the value is the aggregate function.\n",
      " |      \n",
      " |      Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      exprs : dict\n",
      " |          a dict mapping from column name (string) to aggregate functions (string),\n",
      " |          or a list of :class:`Column`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed\n",
      " |      in a single call to this function.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> gdf = df.groupBy(df.name)\n",
      " |      >>> sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
      " |      [Row(name='Alice', count(1)=1), Row(name='Bob', count(1)=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> sorted(gdf.agg(F.min(df.age)).collect())\n",
      " |      [Row(name='Alice', min(age)=2), Row(name='Bob', min(age)=5)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      " |      >>> @pandas_udf('int', PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
      " |      ... def min_udf(v):\n",
      " |      ...     return v.min()\n",
      " |      >>> sorted(gdf.agg(min_udf(df.age)).collect())  # doctest: +SKIP\n",
      " |      [Row(name='Alice', min_udf(age)=2), Row(name='Bob', min_udf(age)=5)]\n",
      " |  \n",
      " |  avg(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Computes average values for each numeric columns for each group.\n",
      " |      \n",
      " |      :func:`mean` is an alias for :func:`avg`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          column names. Non-numeric columns are ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().avg('age').collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> df3.groupBy().avg('age', 'height').collect()\n",
      " |      [Row(avg(age)=3.5, avg(height)=82.5)]\n",
      " |  \n",
      " |  count(self: 'GroupedData') -> pyspark.sql.dataframe.DataFrame\n",
      " |      Counts the number of records for each group.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(df.groupBy(df.age).count().collect())\n",
      " |      [Row(age=2, count=1), Row(age=5, count=1)]\n",
      " |  \n",
      " |  max(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Computes the max value for each numeric columns for each group.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().max('age').collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> df3.groupBy().max('age', 'height').collect()\n",
      " |      [Row(max(age)=5, max(height)=85)]\n",
      " |  \n",
      " |  mean(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Computes average values for each numeric columns for each group.\n",
      " |      \n",
      " |      :func:`mean` is an alias for :func:`avg`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          column names. Non-numeric columns are ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().mean('age').collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> df3.groupBy().mean('age', 'height').collect()\n",
      " |      [Row(avg(age)=3.5, avg(height)=82.5)]\n",
      " |  \n",
      " |  min(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Computes the min value for each numeric column for each group.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          column names. Non-numeric columns are ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().min('age').collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      >>> df3.groupBy().min('age', 'height').collect()\n",
      " |      [Row(min(age)=2, min(height)=80)]\n",
      " |  \n",
      " |  pivot(self, pivot_col: str, values: Union[List[ForwardRef('LiteralType')], NoneType] = None) -> 'GroupedData'\n",
      " |      Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n",
      " |      There are two versions of pivot function: one that requires the caller to specify the list\n",
      " |      of distinct values to pivot on, and one that does not. The latter is more concise but less\n",
      " |      efficient, because Spark needs to first compute the list of distinct values internally.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pivot_col : str\n",
      " |          Name of the column to pivot.\n",
      " |      values : list, optional\n",
      " |          List of values that will be translated to columns in the output DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      # Compute the sum of earnings for each year by course with each course as a separate column\n",
      " |      \n",
      " |      >>> df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n",
      " |      [Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n",
      " |      \n",
      " |      # Or without specifying column values (less efficient)\n",
      " |      \n",
      " |      >>> df4.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").collect()\n",
      " |      [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n",
      " |      >>> df5.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").collect()\n",
      " |      [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n",
      " |  \n",
      " |  sum(self: 'GroupedData', *cols: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Computes the sum for each numeric columns for each group.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          column names. Non-numeric columns are ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().sum('age').collect()\n",
      " |      [Row(sum(age)=7)]\n",
      " |      >>> df3.groupBy().sum('age', 'height').collect()\n",
      " |      [Row(sum(age)=7, sum(height)=165)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      " |  \n",
      " |  apply(self, udf: 'GroupedMapPandasUserDefinedFunction') -> pyspark.sql.dataframe.DataFrame\n",
      " |      It is an alias of :meth:`pyspark.sql.GroupedData.applyInPandas`; however, it takes a\n",
      " |      :meth:`pyspark.sql.functions.pandas_udf` whereas\n",
      " |      :meth:`pyspark.sql.GroupedData.applyInPandas` takes a Python native function.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      udf : :func:`pyspark.sql.functions.pandas_udf`\n",
      " |          a grouped map user-defined function returned by\n",
      " |          :func:`pyspark.sql.functions.pandas_udf`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      It is preferred to use :meth:`pyspark.sql.GroupedData.applyInPandas` over this\n",
      " |      API. This API will be deprecated in the future releases.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      " |      ...     (\"id\", \"v\"))\n",
      " |      >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      " |      ... def normalize(pdf):\n",
      " |      ...     v = pdf.v\n",
      " |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      " |      >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      " |      +---+-------------------+\n",
      " |      | id|                  v|\n",
      " |      +---+-------------------+\n",
      " |      |  1|-0.7071067811865475|\n",
      " |      |  1| 0.7071067811865475|\n",
      " |      |  2|-0.8320502943378437|\n",
      " |      |  2|-0.2773500981126146|\n",
      " |      |  2| 1.1094003924504583|\n",
      " |      +---+-------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  applyInPandas(self, func: 'PandasGroupedMapFunction', schema: Union[pyspark.sql.types.StructType, str]) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\n",
      " |      as a `DataFrame`.\n",
      " |      \n",
      " |      The function should take a `pandas.DataFrame` and return another\n",
      " |      `pandas.DataFrame`. For each group, all columns are passed together as a `pandas.DataFrame`\n",
      " |      to the user-function and the returned `pandas.DataFrame` are combined as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The `schema` should be a :class:`StructType` describing the schema of the returned\n",
      " |      `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      " |      the field names in the defined schema if specified as strings, or match the\n",
      " |      field data types by position if not strings, e.g. integer indices.\n",
      " |      The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes a `pandas.DataFrame`, and outputs a\n",
      " |          `pandas.DataFrame`.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd  # doctest: +SKIP\n",
      " |      >>> from pyspark.sql.functions import pandas_udf, ceil\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      " |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      " |      >>> def normalize(pdf):\n",
      " |      ...     v = pdf.v\n",
      " |      ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      " |      >>> df.groupby(\"id\").applyInPandas(\n",
      " |      ...     normalize, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      " |      +---+-------------------+\n",
      " |      | id|                  v|\n",
      " |      +---+-------------------+\n",
      " |      |  1|-0.7071067811865475|\n",
      " |      |  1| 0.7071067811865475|\n",
      " |      |  2|-0.8320502943378437|\n",
      " |      |  2|-0.2773500981126146|\n",
      " |      |  2| 1.1094003924504583|\n",
      " |      +---+-------------------+\n",
      " |      \n",
      " |      Alternatively, the user can pass a function that takes two arguments.\n",
      " |      In this case, the grouping key(s) will be passed as the first argument and the data will\n",
      " |      be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\n",
      " |      data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\n",
      " |      as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\n",
      " |      This is useful when the user does not want to hardcode grouping key(s) in the function.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      " |      ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      " |      >>> def mean_func(key, pdf):\n",
      " |      ...     # key is a tuple of one numpy.int64, which is the value\n",
      " |      ...     # of 'id' for the current group\n",
      " |      ...     return pd.DataFrame([key + (pdf.v.mean(),)])\n",
      " |      >>> df.groupby('id').applyInPandas(\n",
      " |      ...     mean_func, schema=\"id long, v double\").show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|  v|\n",
      " |      +---+---+\n",
      " |      |  1|1.5|\n",
      " |      |  2|6.0|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      >>> def sum_func(key, pdf):\n",
      " |      ...     # key is a tuple of two numpy.int64s, which is the values\n",
      " |      ...     # of 'id' and 'ceil(df.v / 2)' for the current group\n",
      " |      ...     return pd.DataFrame([key + (pdf.v.sum(),)])\n",
      " |      >>> df.groupby(df.id, ceil(df.v / 2)).applyInPandas(\n",
      " |      ...     sum_func, schema=\"id long, `ceil(v / 2)` long, v double\").show()  # doctest: +SKIP\n",
      " |      +---+-----------+----+\n",
      " |      | id|ceil(v / 2)|   v|\n",
      " |      +---+-----------+----+\n",
      " |      |  2|          5|10.0|\n",
      " |      |  1|          1| 3.0|\n",
      " |      |  2|          3| 5.0|\n",
      " |      |  2|          2| 3.0|\n",
      " |      +---+-----------+----+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function requires a full shuffle. All the data of a group will be loaded\n",
      " |      into memory, so the user should be aware of the potential OOM risk if data is skewed\n",
      " |      and certain groups are too large to fit in memory.\n",
      " |      \n",
      " |      This API is experimental.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  cogroup(self, other: 'GroupedData') -> 'PandasCogroupedOps'\n",
      " |      Cogroups this group with another group so that we can run cogrouped operations.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      See :class:`PandasCogroupedOps` for the operations that can be run.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.group_ops.PandasGroupedOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( emp_df.groupBy(\"dept\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4bb79c1-db54-408f-9731-b8c55c298f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+-----------+-----------+-------------+\n",
      "|     dept|sum(salary)|      avg(salary)|min(salary)|max(salary)|count(salary)|\n",
      "+---------+-----------+-----------------+-----------+-----------+-------------+\n",
      "|    Sales|      25700|8566.666666666666|       8100|       9000|            3|\n",
      "|  Finance|      35100|           8775.0|       7900|       9900|            4|\n",
      "|Marketing|      17100|           8550.0|       8000|       9100|            2|\n",
      "+---------+-----------+-----------------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\").agg( \n",
    "    F.sum(\"salary\"),\n",
    "    F.avg(\"salary\"),\n",
    "    F.min(\"salary\"),\n",
    "    F.max(\"salary\"),\n",
    "    F.count(\"salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43e91d-5aea-43cf-a78e-285153933b22",
   "metadata": {},
   "source": [
    "### Grouping multiple columns:-\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f45a320-9a52-4c3f-b723-5f60575b3c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+--------+\n",
      "|     dept|state|min(salary)|min(age)|\n",
      "+---------+-----+-----------+--------+\n",
      "|    Sales|   NY|       8600|      34|\n",
      "|    Sales|   CA|       8100|      30|\n",
      "|  Finance|   CA|       9000|      24|\n",
      "|  Finance|   NY|       7900|      36|\n",
      "|Marketing|   NY|       9100|      50|\n",
      "|Marketing|   CA|       8000|      25|\n",
      "+---------+-----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy( emp_df.dept, emp_df.state ).min( \"salary\", \"age\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9e0f3-72e5-4c3b-83e8-8ae9c180db2b",
   "metadata": {},
   "source": [
    "### Applying filter before and after grouping:-\n",
    "-------------------------------------------------\n",
    "No separate clause for ***HAVING***. Instead, applying ***where()*** or ***filter()*** after grouping does the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef617f7d-0a6d-4f15-820a-aaca33b66ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     dept|sum(salary)|\n",
      "+---------+-----------+\n",
      "|    Sales|      17600|\n",
      "|  Finance|      16200|\n",
      "|Marketing|       9100|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.where( emp_df.state=='NY' ).groupBy('dept').agg( F.sum(\"salary\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8bba58d-ac2b-41e1-afbb-60f0d268e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   dept|total_salary|\n",
      "+-------+------------+\n",
      "|  Sales|       17600|\n",
      "|Finance|       16200|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.where( emp_df.state=='NY' ).groupBy('dept').agg( \n",
    "    F.sum(\"salary\").alias(\"total_salary\") \n",
    ").where( F.col('total_salary') > 10000 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7833ad5-22b1-46ab-a621-6bb96248c437",
   "metadata": {},
   "source": [
    "## Pivot:-\n",
    "-------------\n",
    "\n",
    "- ***df.groupBy(\\<group_column_name>).pivot(\\<pivot_column_name>).\\<aggregate_function>(\\<aggregate_column_name>)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71e4ef6e-1fc1-4673-bef8-4954016540fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|     dept|state|salary|\n",
      "+---------+-----+------+\n",
      "|    Sales|   NY|  9000|\n",
      "|    Sales|   NY|  8600|\n",
      "|    Sales|   CA|  8100|\n",
      "|  Finance|   CA|  9000|\n",
      "|  Finance|   CA|  9900|\n",
      "|  Finance|   NY|  8300|\n",
      "|  Finance|   NY|  7900|\n",
      "|Marketing|   CA|  8000|\n",
      "|Marketing|   NY|  9100|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df = emp_df.select(\"dept\", \"state\", \"salary\")\n",
    "sub_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92bfc536-629c-4d67-a941-f25f4c31118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+\n",
      "|     dept|state|sum(salary)|\n",
      "+---------+-----+-----------+\n",
      "|    Sales|   NY|      17600|\n",
      "|    Sales|   CA|       8100|\n",
      "|  Finance|   CA|      18900|\n",
      "|  Finance|   NY|      16200|\n",
      "|Marketing|   NY|       9100|\n",
      "|Marketing|   CA|       8000|\n",
      "+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.groupBy( \"dept\", \"state\" ).sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a5969-50fd-4f54-86b0-512762be84a3",
   "metadata": {},
   "source": [
    "- For pivot, of the two (or multiple) group columns, we just take out and make one column as the **Pivot Column**, whose values are to be converted as the column names, and the rest aggregation will be same.\n",
    "\n",
    "- Pivot column will be defined by passing the column name as the parameter of the ***pivot()*** method. This method is to be used **after grouping** and **before aggregation** in pyspark.\n",
    "\n",
    "- When using pivot, only one aggregation of a column is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "623f64c1-6f8b-4c5f-9b30-44a3223ff2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+\n",
      "|     dept|   CA|   NY|\n",
      "+---------+-----+-----+\n",
      "|    Sales| 8100|17600|\n",
      "|  Finance|18900|16200|\n",
      "|Marketing| 8000| 9100|\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.groupBy( sub_df.dept ).pivot( \"state\" ).sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "80fe61b6-9b44-40fb-80ef-c41d5b84cdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+-----+\n",
      "|state|Finance|Marketing|Sales|\n",
      "+-----+-------+---------+-----+\n",
      "|   CA|  18900|     8000| 8100|\n",
      "|   NY|  16200|     9100|17600|\n",
      "+-----+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.groupBy( sub_df.state ).pivot( \"dept\" ).sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fc639eb2-fd3c-46a8-a641-acaed5e77a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+\n",
      "|     dept|   CA|   NY|\n",
      "+---------+-----+-----+\n",
      "|    Sales| 8100|17600|\n",
      "|  Finance|18900|16200|\n",
      "|Marketing| 8000| 9100|\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.groupBy( sub_df.dept ).pivot( \"state\" ).agg( F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e8b60-ce1f-4688-bec5-dbea23547fa2",
   "metadata": {},
   "source": [
    "### Using Pivot with a single group column:-\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c6f0713-19a8-4004-9a39-e1ef83d14093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+-----+\n",
      "|     dept|Finance|Marketing|Sales|\n",
      "+---------+-------+---------+-----+\n",
      "|    Sales|   null|     null|25700|\n",
      "|  Finance|  35100|     null| null|\n",
      "|Marketing|   null|    17100| null|\n",
      "+---------+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\"dept\", \"salary\").groupBy(\"dept\").pivot(\"dept\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf42cb-4dcb-4b9e-a072-d766c4916462",
   "metadata": {},
   "source": [
    "# 9. Unpivot:-\n",
    "----------------\n",
    "No specific function exists for unpivoting. It is done using ***spark.sql()*** of ***df.selectExpr()***, as use of ***stack()*** function is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e1f3db0-603e-4ed2-b134-a35ddd31ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  a|\n",
      "|  b|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df = spark.createDataFrame( data = [('a',),('b',)], schema = [\"id\"] )\n",
    "dummy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90d02c43-1f54-42db-9e9e-59e152080b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+\n",
      "|col0|col1|col2|col3|col4|col5|\n",
      "+----+----+----+----+----+----+\n",
      "|   1|   2|   3|   4|   5|   6|\n",
      "|   7|   8|   9|   0|  10|  11|\n",
      "|  12|  13|  14|  15|  16|null|\n",
      "|   1|   2|   3|   4|   5|   6|\n",
      "|   7|   8|   9|   0|  10|  11|\n",
      "|  12|  13|  14|  15|  16|null|\n",
      "+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First parameter of stack() fuction defines the number of rows would be there.\n",
    "\n",
    "dummy_df.selectExpr( \"stack(3, 1,2,3,4,5,6,7,8,9,0,10,11,12,13,14,15,16)\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9a0dcca-8614-4f53-8dec-acca92ad8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+----+----+\n",
      "| id|col0|col1|col2|col3|col4|col5|\n",
      "+---+----+----+----+----+----+----+\n",
      "|  a|   1|   2|   3|   4|   5|   6|\n",
      "|  a|   7|   8|   9|   0|  10|  11|\n",
      "|  a|  12|  13|  14|  15|  16|null|\n",
      "|  b|   1|   2|   3|   4|   5|   6|\n",
      "|  b|   7|   8|   9|   0|  10|  11|\n",
      "|  b|  12|  13|  14|  15|  16|null|\n",
      "+---+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.selectExpr( \"id\", \"stack(3, 1,2,3,4,5,6,7,8,9,0,10,11,12,13,14,15,16)\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe778b56-4886-490a-a7b7-98072dae3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+\n",
      "|     dept|   CA|   NY|\n",
      "+---------+-----+-----+\n",
      "|    Sales| 8100|17600|\n",
      "|  Finance|18900|16200|\n",
      "|Marketing| 8000| 9100|\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = sub_df.groupBy(\"dept\").pivot(\"state\").sum(\"salary\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "190c3abe-cb06-4f8b-af7b-7ae56093365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|     dept|state|salary|\n",
      "+---------+-----+------+\n",
      "|    Sales|   CA|  8100|\n",
      "|    Sales|   NY| 17600|\n",
      "|  Finance|   CA| 18900|\n",
      "|  Finance|   NY| 16200|\n",
      "|Marketing|   CA|  8000|\n",
      "|Marketing|   NY|  9100|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df.selectExpr(\"dept\", \" stack(2, 'CA', CA, 'NY', NY) as (state,salary) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bfc88-ab2d-4fa4-b4a9-ab210ff924ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
